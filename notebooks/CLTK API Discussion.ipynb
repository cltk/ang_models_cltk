{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sample text we have multi-sentence fragment from [The Atlantic](https://www.theatlantic.com/technology/archive/2018/12/what-us-cities-can-learn-from-venices-floods/577255/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "The Venetian climate-adaptation repertoire is staged in four settings. \n",
    "The barrier islands that separate the Adriatic from the lagoon are the first line of defense against high water. \n",
    "The three inlets that cut through the islands where the tides surge and ebb constitute the second: \n",
    "Here, when completed, the mose barriers will shut when needed. Then comes the lagoon and, finally, the city. \n",
    "Large-scale engineering, which tends to get more attention, is confined to the mose barriers. \n",
    "Projects on the islands and within the lagoon focus on restoring damaged landforms and the habitats they offer to indigenous plants and animals. \n",
    "Meanwhile, urban adaptation campaigns are focused on dredging city canals and bolstering individual buildings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a spaCy pipeline and process the text.  I'm using the \"large\" model for English, documented here: https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-2.0.0\n",
    "\n",
    "The spaCy website is a strength of the project, incidentally.  The clarity, organization', and directness of the docs is appealing, though I find them a bit thin on details, e.g. of the APIs.\n",
    "\n",
    "So: this model comes with a POS tagger, NER, a dependency parser (a CNN trained on OntoNotes), GloVe vectors (300 dim).  Tokenization and lemmatization too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting class name!  English is a [Language](https://spacy.io/api/language), and Language has [Pipe](http://spacy.io/api/pipe) objects.  So unlike the CLTK, there's a single object that provides access to all tasks available for each language.\n",
    "\n",
    "Note that by default, when you load a model, all components of the pipeline are active.\n",
    "\n",
    "Like in the stanford CoreNLP, processing text returns a container object called a [Document](https://spacy.io/api/doc), which is a sequence of [Token](https://spacy.io/api/token) objects.  The English object is callable, so simply passing it the text launches the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.doc.Doc, spacy.tokens.token.Token)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "type(doc), type(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Token has many functions for interrogating its place in the dependency graph, and doesn't necessarily have the greatest interface, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Venetian, 17525330293902761285, 83)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[2]\n",
    "token, token.lemma, token.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're getting IDs for lemma and POS tag.  Docs say to append an underscore to the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('venetian', 'ADJ')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.lemma_ ,token.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get a complete set of tags, we might say:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', '\\n', 'SPACE'),\n",
       " ('The', 'the', 'DET'),\n",
       " ('Venetian', 'venetian', 'ADJ'),\n",
       " ('climate', 'climate', 'NOUN'),\n",
       " ('-', '-', 'PUNCT'),\n",
       " ('adaptation', 'adaptation', 'NOUN'),\n",
       " ('repertoire', 'repertoire', 'NOUN'),\n",
       " ('is', 'be', 'VERB'),\n",
       " ('staged', 'stag', 'VERB'),\n",
       " ('in', 'in', 'ADP'),\n",
       " ('four', 'four', 'NUM'),\n",
       " ('settings', 'setting', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('The', 'the', 'DET'),\n",
       " ('barrier', 'barrier', 'NOUN'),\n",
       " ('islands', 'island', 'NOUN'),\n",
       " ('that', 'that', 'ADJ'),\n",
       " ('separate', 'separate', 'VERB'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('Adriatic', 'adriatic', 'PROPN'),\n",
       " ('from', 'from', 'ADP'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('lagoon', 'lagoon', 'NOUN'),\n",
       " ('are', 'be', 'VERB'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('first', 'first', 'ADJ'),\n",
       " ('line', 'line', 'NOUN'),\n",
       " ('of', 'of', 'ADP'),\n",
       " ('defense', 'defense', 'NOUN'),\n",
       " ('against', 'against', 'ADP'),\n",
       " ('high', 'high', 'ADJ'),\n",
       " ('water', 'water', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('The', 'the', 'DET'),\n",
       " ('three', 'three', 'NUM'),\n",
       " ('inlets', 'inlet', 'NOUN'),\n",
       " ('that', 'that', 'ADJ'),\n",
       " ('cut', 'cut', 'VERB'),\n",
       " ('through', 'through', 'ADP'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('islands', 'island', 'NOUN'),\n",
       " ('where', 'where', 'ADV'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('tides', 'tide', 'NOUN'),\n",
       " ('surge', 'surge', 'VERB'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " ('ebb', 'ebb', 'NOUN'),\n",
       " ('constitute', 'constitute', 'VERB'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('second', 'second', 'ADJ'),\n",
       " (':', ':', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('Here', 'here', 'ADV'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('when', 'when', 'ADV'),\n",
       " ('completed', 'complete', 'VERB'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('mose', 'mose', 'NOUN'),\n",
       " ('barriers', 'barrier', 'NOUN'),\n",
       " ('will', 'will', 'VERB'),\n",
       " ('shut', 'shut', 'VERB'),\n",
       " ('when', 'when', 'ADV'),\n",
       " ('needed', 'need', 'VERB'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('Then', 'then', 'ADV'),\n",
       " ('comes', 'come', 'VERB'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('lagoon', 'lagoon', 'NOUN'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('finally', 'finally', 'ADV'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('city', 'city', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('Large', 'large', 'ADJ'),\n",
       " ('-', '-', 'PUNCT'),\n",
       " ('scale', 'scale', 'NOUN'),\n",
       " ('engineering', 'engineering', 'NOUN'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('which', 'which', 'ADJ'),\n",
       " ('tends', 'tend', 'VERB'),\n",
       " ('to', 'to', 'PART'),\n",
       " ('get', 'get', 'VERB'),\n",
       " ('more', 'more', 'ADJ'),\n",
       " ('attention', 'attention', 'NOUN'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('is', 'be', 'VERB'),\n",
       " ('confined', 'confine', 'VERB'),\n",
       " ('to', 'to', 'ADP'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('mose', 'mose', 'NOUN'),\n",
       " ('barriers', 'barrier', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('Projects', 'projects', 'PROPN'),\n",
       " ('on', 'on', 'ADP'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('islands', 'island', 'NOUN'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " ('within', 'within', 'ADP'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('lagoon', 'lagoon', 'NOUN'),\n",
       " ('focus', 'focus', 'NOUN'),\n",
       " ('on', 'on', 'ADP'),\n",
       " ('restoring', 'restore', 'VERB'),\n",
       " ('damaged', 'damage', 'VERB'),\n",
       " ('landforms', 'landform', 'NOUN'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " ('the', 'the', 'DET'),\n",
       " ('habitats', 'habitat', 'NOUN'),\n",
       " ('they', '-PRON-', 'PRON'),\n",
       " ('offer', 'offer', 'VERB'),\n",
       " ('to', 'to', 'ADP'),\n",
       " ('indigenous', 'indigenous', 'ADJ'),\n",
       " ('plants', 'plant', 'NOUN'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " ('animals', 'animal', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE'),\n",
       " ('Meanwhile', 'meanwhile', 'ADV'),\n",
       " (',', ',', 'PUNCT'),\n",
       " ('urban', 'urban', 'ADJ'),\n",
       " ('adaptation', 'adaptation', 'NOUN'),\n",
       " ('campaigns', 'campaign', 'NOUN'),\n",
       " ('are', 'be', 'VERB'),\n",
       " ('focused', 'focus', 'VERB'),\n",
       " ('on', 'on', 'ADP'),\n",
       " ('dredging', 'dredge', 'VERB'),\n",
       " ('city', 'city', 'NOUN'),\n",
       " ('canals', 'canal', 'NOUN'),\n",
       " ('and', 'and', 'CCONJ'),\n",
       " ('bolstering', 'bolster', 'VERB'),\n",
       " ('individual', 'individual', 'ADJ'),\n",
       " ('buildings', 'building', 'NOUN'),\n",
       " ('.', '.', 'PUNCT'),\n",
       " ('\\n', '\\n', 'SPACE')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.lemma_, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "lemmas_tags = [(token.text, token.lemma_, token.pos_) for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface is tight and seemingly economical, though a beginner may not immediately realize that all tasks defined for the language are run on invoking the `nlp` object.  In particular, the syntactic parser slows things down by an order of magnitude.  \n",
    "\n",
    "The `disable` keywoard argument specifies which pipeline components to not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full pipeline: 2.6630516052246094 seconds\n",
      "Tokenizer and tagger only: 0.16348052024841309 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "nlp(text*100)\n",
    "print(\"Full pipeline: {0} seconds\".format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "nlp(text*100, disable=['parser', 'ner'])\n",
    "print(\"Tokenizer and tagger only: {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I would prefer to explicitly name the pipeline components I want when creating the pipeline.  This is how the Stanford CoreNLP system works.  Here's an example from a production system at work (no IP here ;).  We use Clojure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```clojure\n",
    "(defn make-pipeline\n",
    "  \"Creates and returns Stanford CoreNLP (pipeline) object with specified annotators\"\n",
    "  [annotators]\n",
    "  (StanfordCoreNLP. (doto \n",
    "                      (Properties.) (.put \"annotators\" annotators)\n",
    "                      (.setProperty \"tokenize.options\" \"unicodeEllipsis=true\"))))\n",
    "\n",
    "(defonce master-pipe (make-pipeline \"tokenize, ssplit, pos, lemma, ner, parse\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Latin, on the assumption that Latin tools are among the most popular of the CLTK.  Note that I've never used the CLTK Latin tools, so I'm approaching this like the newbie I am.\n",
    "\n",
    "First we make sure we have the corpora installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latin_text_perseus',\n",
       " 'latin_treebank_perseus',\n",
       " 'latin_text_latin_library',\n",
       " 'phi5',\n",
       " 'phi7',\n",
       " 'latin_proper_names_cltk',\n",
       " 'latin_models_cltk',\n",
       " 'latin_pos_lemmata_cltk',\n",
       " 'latin_treebank_index_thomisticus',\n",
       " 'latin_lexica_perseus',\n",
       " 'latin_training_set_sentence_cltk',\n",
       " 'latin_word2vec_cltk',\n",
       " 'latin_text_antique_digiliblt',\n",
       " 'latin_text_corpus_grammaticorum_latinorum',\n",
       " 'latin_text_poeti_ditalia']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.list_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a text, let's try with a bit from the greatest autobiography ever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_text = '''\n",
    "per idem tempus annorum novem, ab undevicensimo anno aetatis meae usque ad duodetricensimum, \n",
    "seducebamur et seducebamus, falsi atque fallentes in variis cupiditatibus, \n",
    "et palam per doctrinas quas liberales vocant, occulte autem falso nomine religionis, \n",
    "hic superbi, ibi superstitiosi, ubique vani, hac popularis gloriae sectantes inanitatem,\n",
    "usque ad theatricos plausus et contentiosa carmina et agonem coronarum faenearum \n",
    "et spectaculorum nugas et intemperantiam libidinum, illac autem purgari nos ab istis sordibus expetentes, \n",
    "cum eis qui appellarentur electi et sancti afferremus escas de quibus nobis in officina aqualiculi \n",
    "sui fabricarent angelos et deos per quos liberaremur.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, lemmas and POS tags must be requested separately.  Starting with the former, one is immediately faced with, it seems, several options, according to http://docs.cltk.org/en/latest/latin.html  There is a dictionary-based lemmatizer, and then an extended discussion about n-gram backoff lemmatizers.  Which one works best?  We aren't told, but one might reason that since the dictionary-based implementation amounts to a unigram model, perhaps a backoff mechanism will better handle syncretism (ambiguities in the paradigms).\n",
    "\n",
    "However the documentation for the backoff lemmatizers seems to be incomplete (?) So we'll stick to the simpler model.\n",
    "\n",
    "The text doesn't need the J/I and V/U replacer.  The lemmatizer itself has the interesting name `LemmaReplacer`, as if it's in a class hierarchy with the `JVReplacer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['per',\n",
       " 'idem',\n",
       " 'tempus',\n",
       " 'annus',\n",
       " 'novo',\n",
       " ',',\n",
       " 'ab',\n",
       " 'undevicensimo',\n",
       " 'adnato',\n",
       " 'aetas',\n",
       " 'meus',\n",
       " 'usque',\n",
       " 'ad',\n",
       " 'duodetricensimum',\n",
       " ',',\n",
       " 'seducebamur',\n",
       " 'et',\n",
       " 'seducebamus',\n",
       " ',',\n",
       " 'fallo',\n",
       " 'atque',\n",
       " 'fallo',\n",
       " 'in',\n",
       " 'varius1',\n",
       " 'cupiditas',\n",
       " ',',\n",
       " 'et',\n",
       " 'pala',\n",
       " 'per',\n",
       " 'doctrina',\n",
       " 'qui1',\n",
       " 'liberalis1',\n",
       " 'voco',\n",
       " ',',\n",
       " 'occulo',\n",
       " 'autem',\n",
       " 'fallo',\n",
       " 'nomen',\n",
       " 'religio',\n",
       " ',',\n",
       " 'hic',\n",
       " 'superbus',\n",
       " ',',\n",
       " 'ibi',\n",
       " 'superstitiosus',\n",
       " ',',\n",
       " 'ubique',\n",
       " 'vanus',\n",
       " ',',\n",
       " 'hic',\n",
       " 'populo',\n",
       " 'gloria',\n",
       " 'sector2',\n",
       " 'inanitas',\n",
       " ',',\n",
       " 'usque',\n",
       " 'ad',\n",
       " 'theatricos',\n",
       " 'plaudo',\n",
       " 'et',\n",
       " 'contentiosus',\n",
       " 'carmen1',\n",
       " 'et',\n",
       " 'agon',\n",
       " 'corona',\n",
       " 'faenearum',\n",
       " 'et',\n",
       " 'spectaculum',\n",
       " 'nugae',\n",
       " 'et',\n",
       " 'intemperantia',\n",
       " 'libido',\n",
       " ',',\n",
       " 'illic',\n",
       " 'autem',\n",
       " 'purgo',\n",
       " 'nos',\n",
       " 'ab',\n",
       " 'iste',\n",
       " 'sordes',\n",
       " 'expeto',\n",
       " ',',\n",
       " 'cum',\n",
       " 'is',\n",
       " 'qui1',\n",
       " 'appello',\n",
       " 'eligo',\n",
       " 'et',\n",
       " 'sancio',\n",
       " 'afferremus',\n",
       " 'esca',\n",
       " 'de',\n",
       " 'qui1',\n",
       " 'nos',\n",
       " 'in',\n",
       " 'officina',\n",
       " 'aqualiculi',\n",
       " 'suo',\n",
       " 'fabricarent',\n",
       " 'angelus',\n",
       " 'et',\n",
       " 'deus',\n",
       " 'per',\n",
       " 'qui1',\n",
       " 'libero.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "latin_text = latin_text.lower()\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "lemmatizer.lemmatize(latin_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My extremely weak knowledge of Latin suggests that there are a few errors: `anno` -> `adnato`, and `seducebamur` is not reduced, among the others.  Effects of a dictionary-based implementation, I'm guessing.\n",
    "\n",
    "Also it would be nice to have the inflected forms returned with the lemmas, in effect getting tokenization from the lemmatizer.  No need though, since the POS tagger also returns individual tokens.\n",
    "\n",
    "Now for the POS tags.  Again there are several options.  Not knowing the facts for Latin, I happen to believe the CRF tagger works better than TnT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('per', 'R--------'),\n",
       " ('idem', 'P-S---NA-'),\n",
       " ('tempus', 'N-S---NA-'),\n",
       " ('annorum', 'A-S---MA-'),\n",
       " ('novem', 'N-S---MA-'),\n",
       " (',', 'U--------'),\n",
       " ('ab', 'R--------'),\n",
       " ('undevicensimo', 'A-S---MB-'),\n",
       " ('anno', 'N-S---MB-'),\n",
       " ('aetatis', 'N-S---FG-'),\n",
       " ('meae', 'A-S---FG-'),\n",
       " ('usque', 'D--------'),\n",
       " ('ad', 'R--------'),\n",
       " ('duodetricensimum', 'N-S---MA-'),\n",
       " (',', 'U--------'),\n",
       " ('seducebamur', 'V1PPIP---'),\n",
       " ('et', 'C--------'),\n",
       " ('seducebamus', 'N-S---MN-'),\n",
       " (',', 'U--------'),\n",
       " ('falsi', 'T-PRPPMN-'),\n",
       " ('atque', 'C--------'),\n",
       " ('fallentes', 'N-P---MN-'),\n",
       " ('in', 'R--------'),\n",
       " ('variis', 'A-P---FB-'),\n",
       " ('cupiditatibus', 'N-P---FB-'),\n",
       " (',', 'U--------'),\n",
       " ('et', 'C--------'),\n",
       " ('palam', 'N-S---FA-'),\n",
       " ('per', 'R--------'),\n",
       " ('doctrinas', 'N-P---FA-'),\n",
       " ('quas', 'A-P---FA-'),\n",
       " ('liberales', 'A-P---MN-'),\n",
       " ('vocant', 'V3PPIA---'),\n",
       " (',', 'U--------'),\n",
       " ('occulte', 'D--------'),\n",
       " ('autem', 'C--------'),\n",
       " ('falso', 'A-S---NB-'),\n",
       " ('nomine', 'N-S---NB-'),\n",
       " ('religionis', 'N-S---MG-'),\n",
       " (',', 'U--------'),\n",
       " ('hic', 'P-S---MN-'),\n",
       " ('superbi', 'D--------'),\n",
       " (',', 'U--------'),\n",
       " ('ibi', 'D--------'),\n",
       " ('superstitiosi', 'A-P---MN-'),\n",
       " (',', 'U--------'),\n",
       " ('ubique', 'D--------'),\n",
       " ('vani', 'A-P---MN-'),\n",
       " (',', 'U--------'),\n",
       " ('hac', 'D--------'),\n",
       " ('popularis', 'V2SRSA---'),\n",
       " ('gloriae', 'N-S---FG-'),\n",
       " ('sectantes', 'T-PPPAMN-'),\n",
       " ('inanitatem', 'N-S---FA-'),\n",
       " (',', 'U--------'),\n",
       " ('usque', 'D--------'),\n",
       " ('ad', 'R--------'),\n",
       " ('theatricos', 'N-P---MA-'),\n",
       " ('plausus', 'T-SRPPMN-'),\n",
       " ('et', 'C--------'),\n",
       " ('contentiosa', 'A-P---NA-'),\n",
       " ('carmina', 'N-P---NA-'),\n",
       " ('et', 'C--------'),\n",
       " ('agonem', 'N-S---MA-'),\n",
       " ('coronarum', 'A-S---MA-'),\n",
       " ('faenearum', 'N-S---MA-'),\n",
       " ('et', 'C--------'),\n",
       " ('spectaculorum', 'N-P---MG-'),\n",
       " ('nugas', 'N-P---FA-'),\n",
       " ('et', 'C--------'),\n",
       " ('intemperantiam', 'N-S---FA-'),\n",
       " ('libidinum', 'N-P---FG-'),\n",
       " (',', 'U--------'),\n",
       " ('illac', 'D--------'),\n",
       " ('autem', 'C--------'),\n",
       " ('purgari', 'V--PNP---'),\n",
       " ('nos', 'P-P---MA-'),\n",
       " ('ab', 'R--------'),\n",
       " ('istis', 'A-P---MB-'),\n",
       " ('sordibus', 'N-P---MB-'),\n",
       " ('expetentes', 'N-P---FA-'),\n",
       " (',', 'U--------'),\n",
       " ('cum', 'R--------'),\n",
       " ('eis', 'P-P---MB-'),\n",
       " ('qui', 'P-P---MN-'),\n",
       " ('appellarentur', 'V3PPIP---'),\n",
       " ('electi', 'T-PRPPMN-'),\n",
       " ('et', 'C--------'),\n",
       " ('sancti', 'T-PRPPMN-'),\n",
       " ('afferremus', 'V1PPIA---'),\n",
       " ('escas', 'N-P---FA-'),\n",
       " ('de', 'R--------'),\n",
       " ('quibus', 'N-P---MB-'),\n",
       " ('nobis', 'P-P---MD-'),\n",
       " ('in', 'R--------'),\n",
       " ('officina', 'N-P---NA-'),\n",
       " ('aqualiculi', 'N-S---MG-'),\n",
       " ('sui', 'A-S---MG-'),\n",
       " ('fabricarent', 'V3PISA---'),\n",
       " ('angelos', 'N-P---MA-'),\n",
       " ('et', 'C--------'),\n",
       " ('deos', 'N-P---MA-'),\n",
       " ('per', 'R--------'),\n",
       " ('quos', 'P-P---MA-'),\n",
       " ('liberaremur', 'V1PPIP---'),\n",
       " ('.', 'U--------')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "\n",
    "tagger = POSTag('latin')\n",
    "tagger.tag_crf(latin_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, we don't only get the tags, but a whole feature bundle.  As an aside, the doc page doesn't tell us how to interpret the complex tag, and I suggest that we should evaluate the accuracy of these.  \n",
    "\n",
    "So now we just have to join the two output lists to obtain the equivalent to the spaCy output, praying that the number of tokens returned by the two components is the same (one hopes that the same tokenizer is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('per', 'per', 'R--------'),\n",
       " ('idem', 'idem', 'P-S---NA-'),\n",
       " ('tempus', 'tempus', 'N-S---NA-'),\n",
       " ('annorum', 'annus', 'A-S---MA-'),\n",
       " ('novem', 'novo', 'N-S---MA-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('ab', 'ab', 'R--------'),\n",
       " ('undevicensimo', 'undevicensimo', 'A-S---MB-'),\n",
       " ('anno', 'adnato', 'N-S---MB-'),\n",
       " ('aetatis', 'aetas', 'N-S---FG-'),\n",
       " ('meae', 'meus', 'A-S---FG-'),\n",
       " ('usque', 'usque', 'D--------'),\n",
       " ('ad', 'ad', 'R--------'),\n",
       " ('duodetricensimum', 'duodetricensimum', 'N-S---MA-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('seducebamur', 'seducebamur', 'V1PPIP---'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('seducebamus', 'seducebamus', 'N-S---MN-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('falsi', 'fallo', 'T-PRPPMN-'),\n",
       " ('atque', 'atque', 'C--------'),\n",
       " ('fallentes', 'fallo', 'N-P---MN-'),\n",
       " ('in', 'in', 'R--------'),\n",
       " ('variis', 'varius1', 'A-P---FB-'),\n",
       " ('cupiditatibus', 'cupiditas', 'N-P---FB-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('palam', 'pala', 'N-S---FA-'),\n",
       " ('per', 'per', 'R--------'),\n",
       " ('doctrinas', 'doctrina', 'N-P---FA-'),\n",
       " ('quas', 'qui1', 'A-P---FA-'),\n",
       " ('liberales', 'liberalis1', 'A-P---MN-'),\n",
       " ('vocant', 'voco', 'V3PPIA---'),\n",
       " (',', ',', 'U--------'),\n",
       " ('occulte', 'occulo', 'D--------'),\n",
       " ('autem', 'autem', 'C--------'),\n",
       " ('falso', 'fallo', 'A-S---NB-'),\n",
       " ('nomine', 'nomen', 'N-S---NB-'),\n",
       " ('religionis', 'religio', 'N-S---MG-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('hic', 'hic', 'P-S---MN-'),\n",
       " ('superbi', 'superbus', 'D--------'),\n",
       " (',', ',', 'U--------'),\n",
       " ('ibi', 'ibi', 'D--------'),\n",
       " ('superstitiosi', 'superstitiosus', 'A-P---MN-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('ubique', 'ubique', 'D--------'),\n",
       " ('vani', 'vanus', 'A-P---MN-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('hac', 'hic', 'D--------'),\n",
       " ('popularis', 'populo', 'V2SRSA---'),\n",
       " ('gloriae', 'gloria', 'N-S---FG-'),\n",
       " ('sectantes', 'sector2', 'T-PPPAMN-'),\n",
       " ('inanitatem', 'inanitas', 'N-S---FA-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('usque', 'usque', 'D--------'),\n",
       " ('ad', 'ad', 'R--------'),\n",
       " ('theatricos', 'theatricos', 'N-P---MA-'),\n",
       " ('plausus', 'plaudo', 'T-SRPPMN-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('contentiosa', 'contentiosus', 'A-P---NA-'),\n",
       " ('carmina', 'carmen1', 'N-P---NA-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('agonem', 'agon', 'N-S---MA-'),\n",
       " ('coronarum', 'corona', 'A-S---MA-'),\n",
       " ('faenearum', 'faenearum', 'N-S---MA-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('spectaculorum', 'spectaculum', 'N-P---MG-'),\n",
       " ('nugas', 'nugae', 'N-P---FA-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('intemperantiam', 'intemperantia', 'N-S---FA-'),\n",
       " ('libidinum', 'libido', 'N-P---FG-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('illac', 'illic', 'D--------'),\n",
       " ('autem', 'autem', 'C--------'),\n",
       " ('purgari', 'purgo', 'V--PNP---'),\n",
       " ('nos', 'nos', 'P-P---MA-'),\n",
       " ('ab', 'ab', 'R--------'),\n",
       " ('istis', 'iste', 'A-P---MB-'),\n",
       " ('sordibus', 'sordes', 'N-P---MB-'),\n",
       " ('expetentes', 'expeto', 'N-P---FA-'),\n",
       " (',', ',', 'U--------'),\n",
       " ('cum', 'cum', 'R--------'),\n",
       " ('eis', 'is', 'P-P---MB-'),\n",
       " ('qui', 'qui1', 'P-P---MN-'),\n",
       " ('appellarentur', 'appello', 'V3PPIP---'),\n",
       " ('electi', 'eligo', 'T-PRPPMN-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('sancti', 'sancio', 'T-PRPPMN-'),\n",
       " ('afferremus', 'afferremus', 'V1PPIA---'),\n",
       " ('escas', 'esca', 'N-P---FA-'),\n",
       " ('de', 'de', 'R--------'),\n",
       " ('quibus', 'qui1', 'N-P---MB-'),\n",
       " ('nobis', 'nos', 'P-P---MD-'),\n",
       " ('in', 'in', 'R--------'),\n",
       " ('officina', 'officina', 'N-P---NA-'),\n",
       " ('aqualiculi', 'aqualiculi', 'N-S---MG-'),\n",
       " ('sui', 'suo', 'A-S---MG-'),\n",
       " ('fabricarent', 'fabricarent', 'V3PISA---'),\n",
       " ('angelos', 'angelus', 'N-P---MA-'),\n",
       " ('et', 'et', 'C--------'),\n",
       " ('deos', 'deus', 'N-P---MA-'),\n",
       " ('per', 'per', 'R--------'),\n",
       " ('quos', 'qui1', 'P-P---MA-'),\n",
       " ('liberaremur', 'libero.', 'V1PPIP---')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = lemmatizer.lemmatize(latin_text)\n",
    "tags = tagger.tag_crf(latin_text)\n",
    "[(token, lemma, tag) for ((token, tag), lemma) in zip(tags, lemmas)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing again,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(text, disable=['parser', 'ner'])\n",
    "lemmas_tags_spacy = [(token.text, token.lemma_, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tag.pos import POSTag\n",
    "\n",
    "latin_text = latin_text.lower()\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "lemmas = lemmatizer.lemmatize(latin_text)\n",
    "\n",
    "tagger = POSTag('latin')\n",
    "tags = tagger.tag_crf(latin_text)\n",
    "\n",
    "lemmas_tags_cltk = [(token, lemma, tag) for ((token, tag), lemma) in zip(tags, lemmas)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
