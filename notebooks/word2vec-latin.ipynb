{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec for Latin using Keras\n",
    "\n",
    "This notebook is a quick tutorial on training word2vec vectors for Latin, using the simple and well-known skipgram method with negative sampling. \n",
    "\n",
    "Keras makes the process straightforward because the sampling procedure is implemented in Keras' preprocessing library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "First, the boring bits.  We use Latin text from [The Latin Library](http://www.thelatinlibrary.com/), which the CLTK thankfully makes available as a downloadable corpus.\n",
    "\n",
    "All tokens from the library have already been exported to a file, and exactly 260,000 of them, all words occuring at least twice, have been synthesized into a type list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_tokens = [line.rstrip() for line in open('ll_words.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_types = [line.rstrip() for line in open('ll_types.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a method of associating an index in the type list to a token: a mapping from token to integer index.\n",
    "Note that index zero is reserved for out-of-vocabulary (OOV) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "rev_index = {}\n",
    "for i, ll_type in enumerate(ll_types):\n",
    "    index[ll_type] = i + 1\n",
    "    rev_index[i + 1] = ll_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rewrite the complete sequence of the Latin Library into a 1D array of indices.  As noted, OOV terms map to index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = []\n",
    "for token in ll_tokens:\n",
    "    seq.append(index.get(token, 0))\n",
    "seq = np.asarray(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras will now create the dataset for training.  \n",
    "\n",
    "The dataset consists of pairs of indices selected from the sequence (one each for the target word and the context word), and a binary label, 0 or 1, indicating whether the distance between the context word is whithin some window of the target word.  \n",
    "\n",
    "The width of the window is set to 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
    "\n",
    "def create_dataset(types, window_size):\n",
    "    vocab_size = len(types) + 1\n",
    "    window_size = 4\n",
    "\n",
    "    sampling_table = make_sampling_table(vocab_size)\n",
    "    couples, labels = skipgrams(seq, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "\n",
    "    word_targets, word_contexts = zip(*couples)\n",
    "    word_targets = np.array(word_targets, dtype=\"int32\")\n",
    "    word_contexts = np.array(word_contexts, dtype=\"int32\")\n",
    "    \n",
    "    return word_targets, word_contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90504940,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_targets, word_contexts, labels = create_dataset(ll_types, 4)\n",
    "print(couples[:10], labels[:10])\n",
    "word_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for training is extremely simple.  The embedding matrix is initialized by Keras and is, of course, trainable.\n",
    "For each exemplar, i.e. a pair of indices into the embedding matrix, the corresponding word vectors are selected, and the cosine distance of the vectors is computed; this is then squashed through a sigmoid function.  The loss is the difference between the binary classification of the pair and the activation, and is propagated back to the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Input\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "def build_word2vec_model(vocab_size, vector_dim):\n",
    "    input_target = layers.Input((1,))\n",
    "    input_context = layers.Input((1,))\n",
    "\n",
    "    embed = layers.Embedding(vocab_size, vector_dim, input_length=1, trainable=True)\n",
    "    \n",
    "    target = embed(input_target)\n",
    "    target = layers.Reshape((vector_dim,))(target)\n",
    "    \n",
    "    context = embed(input_context)\n",
    "    context = layers.Reshape((vector_dim,))(context)\n",
    "    \n",
    "    dot = layers.dot([target, context], axes=1, normalize=True)\n",
    "    \n",
    "    out = layers.Dense(1, activation='sigmoid')(dot)\n",
    "    \n",
    "    model = Model([input_target, input_context], out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       78000300    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 300)          0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "no_dropout (Dense)              (None, 1)            2           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 78,000,302\n",
      "Trainable params: 78,000,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = build_word2vec_model(vocab_size, vector_dim)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a lexicon of 260,001 items (260,000 + OOV) and 300 dimensions, we have ~78 million parameters.\n",
    "\n",
    "We wrain with large batches (100,000), and a 10% validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81454446 samples, validate on 9050494 samples\n",
      "Epoch 1/50\n",
      "81454446/81454446 [==============================] - 75s 1us/step - loss: 0.5821 - acc: 0.7661 - val_loss: 0.5301 - val_acc: 0.7672\n",
      "Epoch 2/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.5061 - acc: 0.7782 - val_loss: 0.4915 - val_acc: 0.7843\n",
      "Epoch 3/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.4673 - acc: 0.8009 - val_loss: 0.4625 - val_acc: 0.8016\n",
      "Epoch 4/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.4346 - acc: 0.8216 - val_loss: 0.4386 - val_acc: 0.8172\n",
      "Epoch 5/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.4070 - acc: 0.8391 - val_loss: 0.4195 - val_acc: 0.8293\n",
      "Epoch 6/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.3831 - acc: 0.8530 - val_loss: 0.4038 - val_acc: 0.8381\n",
      "Epoch 7/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.3614 - acc: 0.8646 - val_loss: 0.3895 - val_acc: 0.8451\n",
      "Epoch 8/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.3407 - acc: 0.8749 - val_loss: 0.3757 - val_acc: 0.8513\n",
      "Epoch 9/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.3211 - acc: 0.8844 - val_loss: 0.3631 - val_acc: 0.8569\n",
      "Epoch 10/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.3035 - acc: 0.8928 - val_loss: 0.3531 - val_acc: 0.8612\n",
      "Epoch 11/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.2876 - acc: 0.9005 - val_loss: 0.3435 - val_acc: 0.8652\n",
      "Epoch 12/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2717 - acc: 0.9080 - val_loss: 0.3339 - val_acc: 0.8695\n",
      "Epoch 13/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2571 - acc: 0.9149 - val_loss: 0.3254 - val_acc: 0.8735\n",
      "Epoch 14/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2438 - acc: 0.9212 - val_loss: 0.3181 - val_acc: 0.8769\n",
      "Epoch 15/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2316 - acc: 0.9267 - val_loss: 0.3112 - val_acc: 0.8799\n",
      "Epoch 16/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2202 - acc: 0.9315 - val_loss: 0.3050 - val_acc: 0.8827\n",
      "Epoch 17/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.2096 - acc: 0.9359 - val_loss: 0.2995 - val_acc: 0.8849\n",
      "Epoch 18/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1993 - acc: 0.9400 - val_loss: 0.2940 - val_acc: 0.8873\n",
      "Epoch 19/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.1895 - acc: 0.9440 - val_loss: 0.2886 - val_acc: 0.8897\n",
      "Epoch 20/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1805 - acc: 0.9473 - val_loss: 0.2838 - val_acc: 0.8919\n",
      "Epoch 21/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1723 - acc: 0.9503 - val_loss: 0.2797 - val_acc: 0.8936\n",
      "Epoch 22/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1645 - acc: 0.9531 - val_loss: 0.2760 - val_acc: 0.8948\n",
      "Epoch 23/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1568 - acc: 0.9558 - val_loss: 0.2718 - val_acc: 0.8966\n",
      "Epoch 24/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1496 - acc: 0.9584 - val_loss: 0.2678 - val_acc: 0.8983\n",
      "Epoch 25/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.1429 - acc: 0.9606 - val_loss: 0.2644 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.1368 - acc: 0.9626 - val_loss: 0.2614 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1310 - acc: 0.9645 - val_loss: 0.2587 - val_acc: 0.9020\n",
      "Epoch 28/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1255 - acc: 0.9664 - val_loss: 0.2562 - val_acc: 0.9030\n",
      "Epoch 29/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1202 - acc: 0.9681 - val_loss: 0.2535 - val_acc: 0.9041\n",
      "Epoch 30/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1153 - acc: 0.9697 - val_loss: 0.2510 - val_acc: 0.9052\n",
      "Epoch 31/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1107 - acc: 0.9712 - val_loss: 0.2489 - val_acc: 0.9060\n",
      "Epoch 32/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.1065 - acc: 0.9725 - val_loss: 0.2470 - val_acc: 0.9068\n",
      "Epoch 33/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.1024 - acc: 0.9738 - val_loss: 0.2452 - val_acc: 0.9076\n",
      "Epoch 34/50\n",
      "81454446/81454446 [==============================] - 73s 1us/step - loss: 0.0986 - acc: 0.9750 - val_loss: 0.2437 - val_acc: 0.9083\n",
      "Epoch 35/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0950 - acc: 0.9761 - val_loss: 0.2422 - val_acc: 0.9090\n",
      "Epoch 36/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0916 - acc: 0.9772 - val_loss: 0.2407 - val_acc: 0.9098\n",
      "Epoch 37/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0884 - acc: 0.9781 - val_loss: 0.2394 - val_acc: 0.9104\n",
      "Epoch 38/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0855 - acc: 0.9791 - val_loss: 0.2379 - val_acc: 0.9111\n",
      "Epoch 39/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0827 - acc: 0.9799 - val_loss: 0.2367 - val_acc: 0.9118\n",
      "Epoch 40/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0801 - acc: 0.9807 - val_loss: 0.2354 - val_acc: 0.9125\n",
      "Epoch 41/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0777 - acc: 0.9814 - val_loss: 0.2342 - val_acc: 0.9131\n",
      "Epoch 42/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0756 - acc: 0.9820 - val_loss: 0.2333 - val_acc: 0.9136\n",
      "Epoch 43/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0736 - acc: 0.9826 - val_loss: 0.2325 - val_acc: 0.9142\n",
      "Epoch 44/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0718 - acc: 0.9831 - val_loss: 0.2318 - val_acc: 0.9146\n",
      "Epoch 45/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0701 - acc: 0.9836 - val_loss: 0.2313 - val_acc: 0.9150\n",
      "Epoch 46/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0685 - acc: 0.9840 - val_loss: 0.2308 - val_acc: 0.9154\n",
      "Epoch 47/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0671 - acc: 0.9844 - val_loss: 0.2305 - val_acc: 0.9157\n",
      "Epoch 48/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0658 - acc: 0.9848 - val_loss: 0.2304 - val_acc: 0.9160\n",
      "Epoch 49/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0646 - acc: 0.9851 - val_loss: 0.2302 - val_acc: 0.9162\n",
      "Epoch 50/50\n",
      "81454446/81454446 [==============================] - 74s 1us/step - loss: 0.0634 - acc: 0.9854 - val_loss: 0.2302 - val_acc: 0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f425c020e10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit([word_target, word_context], labels, epochs=50, batch_size=100000, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is quite good.  The validation accuracy suggests that, for in-vocabulary words, the embeddings can predict with 91.63% accuracy whether a pair of words tend to co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the vectors\n",
    "\n",
    "We'll need to write both the embedding vectors themselves, and the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = m.get_weights()[0]\n",
    "with open('latin_vectors.bin', 'wb') as outfile:\n",
    "    pickle.dump(w, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('latin_types.txt', 'w') as outfile:\n",
    "    for word, id in index.items():\n",
    "        outfile.write('{0}\\t{1}\\n'.format(id, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260001, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('latin_vectors.bin', 'rb') as infile:\n",
    "    latin_vectors = pickle.load(infile)\n",
    "latin_vectors.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
