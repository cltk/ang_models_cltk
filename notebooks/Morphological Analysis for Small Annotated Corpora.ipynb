{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Analysis for Small Annotated Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  A problem ... and an opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [2017 Replication Study of LSTM-based POS taggers (Horsmann and Zesch, 2017)](https://www.ltl.uni-due.de/wp-content/uploads/horsmannZesch_emnlp2017.pdf), which builds on a well-known 2016 investigation on [arXiv](https://arxiv.org/pdf/1604.05529.pdf), it is stated that there's a soft minimum of around 60k tokens for reasonable performance in the POS tagging task.\n",
    "\n",
    "The following chart shows the performance of various models, for a wide set of languages and language families.  Note that what is reported is 10-fold cross validation, which, depending on the experimental setup, may overestimate performance on a held out test set containing words not seen during training.\n",
    "\n",
    "<img src=\"pos_accuracies.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLTK project develops NLP tools for dead languages.  Annotated corpora for these, when available, are sometimes relaively small.  For [Old English](http://www.oldenglishaerobics.net/resources/magic_letter.pdf) (OE, a Germanic language), for example, the [ISWOC Treebank](http://iswoc.github.io/), contains only 28,300 tokens, so half or less of the recommended minimum.  The question therefore arises of whether the performance observed for Germanic languages, in the range of 90%-96+% accuracy, can be reproduced.\n",
    "\n",
    "Note the odd low outliter accuracy scores turned in by Icelandic.  It is relevant here because Icelandic is the modern language closest to OE, also because of its orthography: like OE, the Icelandic alphabet contains the letters ash, eth, thorn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the problem part.  The opportunity lies in the fact that the ISWOC annotations provide much more than mere POS tags.  Instead, each word is assigned values over an extensive set of morphological features.  These are listed in the XML for each annotated text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<parts-of-speech>\n",
    "      <value tag=\"A-\" summary=\"adjective\"/>\n",
    "      <value tag=\"Df\" summary=\"adverb\"/>\n",
    "      <value tag=\"S-\" summary=\"article\"/>\n",
    "      <value tag=\"Ma\" summary=\"cardinal numeral\"/>\n",
    "      <value tag=\"Nb\" summary=\"common noun\"/>\n",
    "      <value tag=\"C-\" summary=\"conjunction\"/>\n",
    "      <value tag=\"Pd\" summary=\"demonstrative pronoun\"/>\n",
    "      <value tag=\"F-\" summary=\"foreign word\"/>\n",
    "      <value tag=\"Px\" summary=\"indefinite pronoun\"/>\n",
    "      <value tag=\"N-\" summary=\"infinitive marker\"/>\n",
    "      <value tag=\"I-\" summary=\"interjection\"/>\n",
    "      <value tag=\"Du\" summary=\"interrogative adverb\"/>\n",
    "      <value tag=\"Pi\" summary=\"interrogative pronoun\"/>\n",
    "      <value tag=\"Mo\" summary=\"ordinal numeral\"/>\n",
    "      <value tag=\"Pp\" summary=\"personal pronoun\"/>\n",
    "      <value tag=\"Pk\" summary=\"personal reflexive pronoun\"/>\n",
    "      <value tag=\"Ps\" summary=\"possessive pronoun\"/>\n",
    "      <value tag=\"Pt\" summary=\"possessive reflexive pronoun\"/>\n",
    "      <value tag=\"R-\" summary=\"preposition\"/>\n",
    "      <value tag=\"Ne\" summary=\"proper noun\"/>\n",
    "      <value tag=\"Py\" summary=\"quantifier\"/>\n",
    "      <value tag=\"Pc\" summary=\"reciprocal pronoun\"/>\n",
    "      <value tag=\"Dq\" summary=\"relative adverb\"/>\n",
    "      <value tag=\"Pr\" summary=\"relative pronoun\"/>\n",
    "      <value tag=\"G-\" summary=\"subjunction\"/>\n",
    "      <value tag=\"V-\" summary=\"verb\"/>\n",
    "      <value tag=\"X-\" summary=\"unassigned\"/>\n",
    "    </parts-of-speech>\n",
    "    <morphology>\n",
    "      <field tag=\"person\">\n",
    "        <value tag=\"1\" summary=\"first person\"/>\n",
    "        <value tag=\"2\" summary=\"second person\"/>\n",
    "        <value tag=\"3\" summary=\"third person\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain person\"/>\n",
    "      </field>\n",
    "      <field tag=\"number\">\n",
    "        <value tag=\"s\" summary=\"singular\"/>\n",
    "        <value tag=\"d\" summary=\"dual\"/>\n",
    "        <value tag=\"p\" summary=\"plural\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain number\"/>\n",
    "      </field>\n",
    "      <field tag=\"tense\">\n",
    "        <value tag=\"p\" summary=\"present\"/>\n",
    "        <value tag=\"i\" summary=\"imperfect\"/>\n",
    "        <value tag=\"r\" summary=\"perfect\"/>\n",
    "        <value tag=\"s\" summary=\"resultative\"/>\n",
    "        <value tag=\"a\" summary=\"aorist\"/>\n",
    "        <value tag=\"u\" summary=\"past\"/>\n",
    "        <value tag=\"l\" summary=\"pluperfect\"/>\n",
    "        <value tag=\"f\" summary=\"future\"/>\n",
    "        <value tag=\"t\" summary=\"future perfect\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain tense\"/>\n",
    "      </field>\n",
    "      <field tag=\"mood\">\n",
    "        <value tag=\"i\" summary=\"indicative\"/>\n",
    "        <value tag=\"s\" summary=\"subjunctive\"/>\n",
    "        <value tag=\"m\" summary=\"imperative\"/>\n",
    "        <value tag=\"o\" summary=\"optative\"/>\n",
    "        <value tag=\"n\" summary=\"infinitive\"/>\n",
    "        <value tag=\"p\" summary=\"participle\"/>\n",
    "        <value tag=\"d\" summary=\"gerund\"/>\n",
    "        <value tag=\"g\" summary=\"gerundive\"/>\n",
    "        <value tag=\"u\" summary=\"supine\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain mood\"/>\n",
    "        <value tag=\"y\" summary=\"finiteness unspecified\"/>\n",
    "        <value tag=\"e\" summary=\"indicative or subjunctive\"/>\n",
    "        <value tag=\"f\" summary=\"indicative or imperative\"/>\n",
    "        <value tag=\"h\" summary=\"subjunctive or imperative\"/>\n",
    "        <value tag=\"t\" summary=\"finite\"/>\n",
    "      </field>\n",
    "      <field tag=\"voice\">\n",
    "        <value tag=\"a\" summary=\"active\"/>\n",
    "        <value tag=\"m\" summary=\"middle\"/>\n",
    "        <value tag=\"p\" summary=\"passive\"/>\n",
    "        <value tag=\"e\" summary=\"middle or passive\"/>\n",
    "        <value tag=\"x\" summary=\"unspecified\"/>\n",
    "      </field>\n",
    "      <field tag=\"gender\">\n",
    "        <value tag=\"m\" summary=\"masculine\"/>\n",
    "        <value tag=\"f\" summary=\"feminine\"/>\n",
    "        <value tag=\"n\" summary=\"neuter\"/>\n",
    "        <value tag=\"p\" summary=\"masculine or feminine\"/>\n",
    "        <value tag=\"o\" summary=\"masculine or neuter\"/>\n",
    "        <value tag=\"r\" summary=\"feminine or neuter\"/>\n",
    "        <value tag=\"q\" summary=\"masculine, feminine or neuter\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain gender\"/>\n",
    "      </field>\n",
    "      <field tag=\"case\">\n",
    "        <value tag=\"n\" summary=\"nominative\"/>\n",
    "        <value tag=\"a\" summary=\"accusative\"/>\n",
    "        <value tag=\"o\" summary=\"oblique\"/>\n",
    "        <value tag=\"g\" summary=\"genitive\"/>\n",
    "        <value tag=\"c\" summary=\"genitive or dative\"/>\n",
    "        <value tag=\"e\" summary=\"accusative or dative\"/>\n",
    "        <value tag=\"d\" summary=\"dative\"/>\n",
    "        <value tag=\"b\" summary=\"ablative\"/>\n",
    "        <value tag=\"i\" summary=\"instrumental\"/>\n",
    "        <value tag=\"l\" summary=\"locative\"/>\n",
    "        <value tag=\"v\" summary=\"vocative\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain case\"/>\n",
    "        <value tag=\"z\" summary=\"no case\"/>\n",
    "      </field>\n",
    "      <field tag=\"degree\">\n",
    "        <value tag=\"p\" summary=\"positive\"/>\n",
    "        <value tag=\"c\" summary=\"comparative\"/>\n",
    "        <value tag=\"s\" summary=\"superlative\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain degree\"/>\n",
    "        <value tag=\"z\" summary=\"no degree\"/>\n",
    "      </field>\n",
    "      <field tag=\"strength\">\n",
    "        <value tag=\"w\" summary=\"weak\"/>\n",
    "        <value tag=\"s\" summary=\"strong\"/>\n",
    "        <value tag=\"t\" summary=\"weak or strong\"/>\n",
    "      </field>\n",
    "      <field tag=\"inflection\">\n",
    "        <value tag=\"n\" summary=\"non-inflecting\"/>\n",
    "        <value tag=\"i\" summary=\"inflecting\"/>\n",
    "      </field>\n",
    "    </morphology>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two questions arise:\n",
    "1.  Can we use these annotations to construct a single classifier that outputs *all* morphosyntactic features, in addition to the category feature (POS tag)?\n",
    "2.  Is it just possible that the complete morphological analysis may help the individual task, e.g. POS tagging?\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ISWOC treebank contains a few OE \"books\" that have been mostly annotated, for a total of 28,300 tokens.  The book construct provides a useful unit for splitting the dataset, because each distinct book is likely to contain idionyscratic words.  That is, by training on one set of books and testing on another, it is likely that out-of-vocabulary (OOV) tokens will occur in the test set.\n",
    "\n",
    "We reserve one book, an Anglo-Saxon translation of Orosius' *Histories*,  as a test set (1701 tokens), while the rest form the wraining set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Perceptron training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horsmann and Zesch use a conditional random fields (CRF) tagger as the baseline.  I have found that NLTK's [Perceptron tagger](http://www.nltk.org/_modules/nltk/tag/perceptron.html) to be more accurate (if somewhat slower) than the CRF tagger, so I use it here.\n",
    "\n",
    "The following output shows, for each of the morphological features above, the performance of a separately-trained Perceptron classifer.  The following statistics are given: 10-fold CV, accuracy and Cohen's Kappa on the test set, a confusion matrix, and time to tag *Beowulf*.\n",
    "\n",
    "Because tokens are often unvalued for some of the features (e.g. tense), raw accuracy can be misleading.  Kappa statistics are more useful in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "---------- pos ----------\n",
    "CV fold 1 accuracy = 0.897 kappa = 0.872 in 13.036 seconds\n",
    "CV fold 2 accuracy = 0.917 kappa = 0.897 in 13.327 seconds\n",
    "CV fold 3 accuracy = 0.912 kappa = 0.891 in 13.462 seconds\n",
    "CV fold 4 accuracy = 0.917 kappa = 0.897 in 13.798 seconds\n",
    "CV fold 5 accuracy = 0.927 kappa = 0.909 in 14.648 seconds\n",
    "CV fold 6 accuracy = 0.935 kappa = 0.919 in 13.200 seconds\n",
    "CV fold 7 accuracy = 0.93 kappa = 0.911 in 12.101 seconds\n",
    "CV fold 8 accuracy = 0.914 kappa = 0.893 in 13.064 seconds\n",
    "CV fold 9 accuracy = 0.927 kappa = 0.907 in 13.386 seconds\n",
    "CV fold 10 accuracy = 0.926 kappa = 0.907 in 12.897 seconds\n",
    "10-fold validation of model perceptron = 0.920\n",
    "Test of model perceptron for feature pos on unseen text:\n",
    "        accuracy = 0.840\n",
    "        kappa = 0.809\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "    A-   C-   DF  DU  G-   NB  NE   PD  PI   PP  PS  PX  PY   R-   V-\n",
    "A-  31    0    3   0   0   22   1    0   0    0   0   0   2    0   19\n",
    "C-   0  117    0   0   2    0   0    0   0    0   0   0   1    0    1\n",
    "DF   3    1  115   0   2   16   3    8   0    0   0   0   2    3   10\n",
    "DU   0    0    0   2   0    0   0    0   0    0   0   0   0    0    0\n",
    "G-   0    2    6   0  59    1   0    7   0    0   0   0   0    1    2\n",
    "NB   3    0    3   0   0  244   0    0   0    0   0   0   0    0   14\n",
    "NE   0    0    2   0   0    8  86    0   0    0   0   0   0    0    0\n",
    "PD   0    0   10   0   8    3   1  128   0    0   0   0   0    0    1\n",
    "PI   0    2    0   0   0    0   0    0   1    0   0   0   0    0    0\n",
    "PP   1    0    0   0   0    3   0    0   0  126   2   0   0    0    0\n",
    "PS   0    0    1   0   0    0   0    0   0    1  27   0   0    0    0\n",
    "PX   0    0    0   0   0    4   0    0   0    0   0   7   0    0    0\n",
    "PY   8    1    1   0   0   22  14    1   0    0   1   0  74    0    3\n",
    "R-   0    0    4   0   0    6   1    0   0    0   0   0   4  159    0\n",
    "V-   3    0    1   0   0   28   0    0   0    0   0   0   2    8  230\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 2.158 secs at 9795.290 words/sec\n",
    "\n",
    "---------- person ----------\n",
    "CV fold 1 accuracy = 0.955 kappa = 0.834 in 6.394 seconds\n",
    "CV fold 2 accuracy = 0.961 kappa = 0.858 in 6.156 seconds\n",
    "CV fold 3 accuracy = 0.966 kappa = 0.878 in 6.376 seconds\n",
    "CV fold 4 accuracy = 0.964 kappa = 0.863 in 6.294 seconds\n",
    "CV fold 5 accuracy = 0.955 kappa = 0.849 in 5.946 seconds\n",
    "CV fold 6 accuracy = 0.962 kappa = 0.865 in 7.360 seconds\n",
    "CV fold 7 accuracy = 0.963 kappa = 0.867 in 6.452 seconds\n",
    "CV fold 8 accuracy = 0.97 kappa = 0.886 in 6.639 seconds\n",
    "CV fold 9 accuracy = 0.959 kappa = 0.857 in 6.775 seconds\n",
    "CV fold 10 accuracy = 0.965 kappa = 0.874 in 6.102 seconds\n",
    "10-fold validation of model perceptron = 0.962\n",
    "Test of model perceptron for feature person on unseen text:\n",
    "        accuracy = 0.956\n",
    "        kappa = 0.791\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      -  1  2    3\n",
    "-  1335  0  1   11\n",
    "1     0  2  0    0\n",
    "2     0  0  0    1\n",
    "3    59  0  0  292\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 0.973 secs at 21723.011 words/sec\n",
    "\n",
    "---------- number ----------\n",
    "CV fold 1 accuracy = 0.917 kappa = 0.843 in 8.375 seconds\n",
    "CV fold 2 accuracy = 0.909 kappa = 0.828 in 9.679 seconds\n",
    "CV fold 3 accuracy = 0.907 kappa = 0.836 in 9.840 seconds\n",
    "CV fold 4 accuracy = 0.911 kappa = 0.831 in 9.263 seconds\n",
    "CV fold 5 accuracy = 0.916 kappa = 0.848 in 8.697 seconds\n",
    "CV fold 6 accuracy = 0.912 kappa = 0.837 in 8.532 seconds\n",
    "CV fold 7 accuracy = 0.912 kappa = 0.834 in 8.704 seconds\n",
    "CV fold 8 accuracy = 0.913 kappa = 0.839 in 9.054 seconds\n",
    "CV fold 9 accuracy = 0.919 kappa = 0.846 in 9.099 seconds\n",
    "CV fold 10 accuracy = 0.909 kappa = 0.823 in 8.383 seconds\n",
    "10-fold validation of model perceptron = 0.913\n",
    "Test of model perceptron for feature number on unseen text:\n",
    "        accuracy = 0.827\n",
    "        kappa = 0.684\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "     -    P    S   X\n",
    "-  516   16   78   0\n",
    "P   22  168   62   0\n",
    "S   36   30  696   6\n",
    "X    1   13   34  23\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 1.234 secs at 17120.223 words/sec\n",
    "\n",
    "---------- tense ----------\n",
    "CV fold 1 accuracy = 0.97 kappa = 0.826 in 5.091 seconds\n",
    "CV fold 2 accuracy = 0.967 kappa = 0.824 in 5.721 seconds\n",
    "CV fold 3 accuracy = 0.97 kappa = 0.831 in 4.682 seconds\n",
    "CV fold 4 accuracy = 0.971 kappa = 0.829 in 4.706 seconds\n",
    "CV fold 5 accuracy = 0.965 kappa = 0.803 in 5.152 seconds\n",
    "CV fold 6 accuracy = 0.967 kappa = 0.814 in 5.052 seconds\n",
    "CV fold 7 accuracy = 0.97 kappa = 0.832 in 4.971 seconds\n",
    "CV fold 8 accuracy = 0.966 kappa = 0.813 in 4.778 seconds\n",
    "CV fold 9 accuracy = 0.969 kappa = 0.830 in 4.887 seconds\n",
    "CV fold 10 accuracy = 0.963 kappa = 0.795 in 4.817 seconds\n",
    "10-fold validation of model perceptron = 0.968\n",
    "Test of model perceptron for feature tense on unseen text:\n",
    "        accuracy = 0.960\n",
    "        kappa = 0.721\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      -   P    U\n",
    "-  1441   1   15\n",
    "P    16  33    2\n",
    "U    36   2  155\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 0.854 secs at 24752.985 words/sec\n",
    "\n",
    "---------- mood ----------\n",
    "CV fold 1 accuracy = 0.959 kappa = 0.791 in 6.372 seconds\n",
    "CV fold 2 accuracy = 0.958 kappa = 0.786 in 6.560 seconds\n",
    "CV fold 3 accuracy = 0.957 kappa = 0.790 in 6.696 seconds\n",
    "CV fold 4 accuracy = 0.961 kappa = 0.804 in 7.416 seconds\n",
    "CV fold 5 accuracy = 0.962 kappa = 0.802 in 6.186 seconds\n",
    "CV fold 6 accuracy = 0.957 kappa = 0.780 in 6.235 seconds\n",
    "CV fold 7 accuracy = 0.953 kappa = 0.762 in 6.148 seconds\n",
    "CV fold 8 accuracy = 0.962 kappa = 0.798 in 6.205 seconds\n",
    "CV fold 9 accuracy = 0.958 kappa = 0.791 in 6.199 seconds\n",
    "CV fold 10 accuracy = 0.961 kappa = 0.805 in 6.200 seconds\n",
    "10-fold validation of model perceptron = 0.959\n",
    "Test of model perceptron for feature mood on unseen text:\n",
    "        accuracy = 0.952\n",
    "        kappa = 0.699\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      -    I  M   N   P  S   X\n",
    "-  1415    5  0   0   3  0   6\n",
    "I    29  120  0   0   0  0   1\n",
    "M     0    1  0   0   0  0   0\n",
    "N    12    1  0  13   1  0   0\n",
    "P     9    2  0   0  12  0   0\n",
    "S     6    1  0   0   0  6   0\n",
    "X     7    1  0   0   1  0  49\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 1.067 secs at 19806.273 words/sec\n",
    "\n",
    "---------- gender ----------\n",
    "CV fold 1 accuracy = 0.89 kappa = 0.767 in 9.088 seconds\n",
    "CV fold 2 accuracy = 0.885 kappa = 0.756 in 9.011 seconds\n",
    "CV fold 3 accuracy = 0.888 kappa = 0.763 in 9.081 seconds\n",
    "CV fold 4 accuracy = 0.87 kappa = 0.725 in 9.156 seconds\n",
    "CV fold 5 accuracy = 0.885 kappa = 0.755 in 8.893 seconds\n",
    "CV fold 6 accuracy = 0.894 kappa = 0.777 in 9.337 seconds\n",
    "CV fold 7 accuracy = 0.901 kappa = 0.788 in 9.568 seconds\n",
    "CV fold 8 accuracy = 0.892 kappa = 0.772 in 9.374 seconds\n",
    "CV fold 9 accuracy = 0.884 kappa = 0.752 in 9.298 seconds\n",
    "CV fold 10 accuracy = 0.885 kappa = 0.755 in 9.722 seconds\n",
    "10-fold validation of model perceptron = 0.887\n",
    "Test of model perceptron for feature gender on unseen text:\n",
    "        accuracy = 0.798\n",
    "        kappa = 0.604\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "     -   F    M    N  O  Q   X\n",
    "-  767  17   42    7  0  0   0\n",
    "F   15  74   26    6  0  0   0\n",
    "M   39   7  334   35  0  0   6\n",
    "N   30  12   45  138  0  0   5\n",
    "O    0   0    0    2  0  0   0\n",
    "Q    0   0    1    0  0  0   0\n",
    "X    7   3   43    6  0  0  34\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 1.542 secs at 13705.564 words/sec\n",
    "\n",
    "---------- case ----------\n",
    "CV fold 1 accuracy = 0.888 kappa = 0.782 in 10.009 seconds\n",
    "CV fold 2 accuracy = 0.882 kappa = 0.763 in 10.338 seconds\n",
    "CV fold 3 accuracy = 0.883 kappa = 0.765 in 10.110 seconds\n",
    "CV fold 4 accuracy = 0.892 kappa = 0.783 in 9.754 seconds\n",
    "CV fold 5 accuracy = 0.874 kappa = 0.753 in 9.861 seconds\n",
    "CV fold 6 accuracy = 0.882 kappa = 0.764 in 10.964 seconds\n",
    "CV fold 7 accuracy = 0.878 kappa = 0.754 in 10.595 seconds\n",
    "CV fold 8 accuracy = 0.889 kappa = 0.779 in 12.556 seconds\n",
    "CV fold 9 accuracy = 0.881 kappa = 0.764 in 10.202 seconds\n",
    "CV fold 10 accuracy = 0.874 kappa = 0.752 in 9.930 seconds\n",
    "10-fold validation of model perceptron = 0.882\n",
    "Test of model perceptron for feature case on unseen text:\n",
    "        accuracy = 0.814\n",
    "        kappa = 0.635\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "     -    A    D   G  I    N   X\n",
    "-  775   13   19   4  0   19   2\n",
    "A   25  122    1   2  2   37   3\n",
    "D    7    4  193   0  0    3   4\n",
    "G    8    5    3  60  0    9   0\n",
    "I    1    0    1   1  2    1   0\n",
    "N   40   37    5   2  0  192   7\n",
    "X   13   13    8  10  0   16  32\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 1.408 secs at 15012.101 words/sec\n",
    "\n",
    "---------- degree ----------\n",
    "CV fold 1 accuracy = 0.969 kappa = 0.611 in 2.920 seconds\n",
    "CV fold 2 accuracy = 0.962 kappa = 0.578 in 2.767 seconds\n",
    "CV fold 3 accuracy = 0.959 kappa = 0.545 in 2.562 seconds\n",
    "CV fold 4 accuracy = 0.964 kappa = 0.577 in 2.597 seconds\n",
    "CV fold 5 accuracy = 0.964 kappa = 0.619 in 2.525 seconds\n",
    "CV fold 6 accuracy = 0.962 kappa = 0.543 in 2.515 seconds\n",
    "CV fold 7 accuracy = 0.961 kappa = 0.569 in 2.541 seconds\n",
    "CV fold 8 accuracy = 0.976 kappa = 0.689 in 2.641 seconds\n",
    "CV fold 9 accuracy = 0.957 kappa = 0.519 in 2.411 seconds\n",
    "CV fold 10 accuracy = 0.96 kappa = 0.558 in 2.509 seconds\n",
    "10-fold validation of model perceptron = 0.964\n",
    "Test of model perceptron for feature degree on unseen text:\n",
    "        accuracy = 0.907\n",
    "        kappa = 0.383\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      -  C   P  S  X   Z\n",
    "-  1436  0   4  1  0   4\n",
    "C     7  4   0  0  0   0\n",
    "P    72  0  64  0  0   3\n",
    "S     7  0   2  6  0   0\n",
    "X    18  0   0  0  0   8\n",
    "Z    28  0   5  0  0  32\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 0.408 secs at 51824.308 words/sec\n",
    "\n",
    "---------- strength ----------\n",
    "CV fold 1 accuracy = 0.966 kappa = 0.629 in 2.157 seconds\n",
    "CV fold 2 accuracy = 0.966 kappa = 0.639 in 2.215 seconds\n",
    "CV fold 3 accuracy = 0.969 kappa = 0.690 in 2.150 seconds\n",
    "CV fold 4 accuracy = 0.965 kappa = 0.646 in 2.244 seconds\n",
    "CV fold 5 accuracy = 0.97 kappa = 0.673 in 2.120 seconds\n",
    "CV fold 6 accuracy = 0.966 kappa = 0.621 in 2.218 seconds\n",
    "CV fold 7 accuracy = 0.961 kappa = 0.609 in 2.145 seconds\n",
    "CV fold 8 accuracy = 0.968 kappa = 0.663 in 2.177 seconds\n",
    "CV fold 9 accuracy = 0.971 kappa = 0.680 in 2.199 seconds\n",
    "CV fold 10 accuracy = 0.96 kappa = 0.597 in 2.161 seconds\n",
    "10-fold validation of model perceptron = 0.966\n",
    "Test of model perceptron for feature strength on unseen text:\n",
    "        accuracy = 0.927\n",
    "        kappa = 0.439\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      -   S   T  W\n",
    "-  1467  11   0  0\n",
    "S    47  77   4  1\n",
    "T    44   7  25  0\n",
    "W    13   0   0  5\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 0.372 secs at 56854.136 words/sec\n",
    "\n",
    "---------- inflection ----------\n",
    "CV fold 1 accuracy = 0.955 kappa = 0.863 in 2.307 seconds\n",
    "CV fold 2 accuracy = 0.961 kappa = 0.876 in 2.115 seconds\n",
    "CV fold 3 accuracy = 0.957 kappa = 0.860 in 2.174 seconds\n",
    "CV fold 4 accuracy = 0.958 kappa = 0.870 in 2.115 seconds\n",
    "CV fold 5 accuracy = 0.961 kappa = 0.879 in 2.118 seconds\n",
    "CV fold 6 accuracy = 0.956 kappa = 0.866 in 2.192 seconds\n",
    "CV fold 7 accuracy = 0.965 kappa = 0.889 in 2.176 seconds\n",
    "CV fold 8 accuracy = 0.962 kappa = 0.879 in 2.174 seconds\n",
    "CV fold 9 accuracy = 0.968 kappa = 0.900 in 2.292 seconds\n",
    "CV fold 10 accuracy = 0.967 kappa = 0.895 in 2.251 seconds\n",
    "10-fold validation of model perceptron = 0.961\n",
    "Test of model perceptron for feature inflection on unseen text:\n",
    "        accuracy = 0.931\n",
    "        kappa = 0.788\n",
    "\n",
    "Confusion matrix (rows = gold):\n",
    "      I    N\n",
    "I  1115   33\n",
    "N    87  466\n",
    "\n",
    "Time for model perceptron to tag texts/oe/beowulf.txt = 0.357 secs at 59155.522 words/sec\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10-fold CV accuracy for POS tagging is 92%, placing the tagger in the general range of the Germanic LSTM-based approaches considered by Horsmann and Zesch. \n",
    "\n",
    "Test accuracy for POS tagging is considerably lower, at 84%, likely because of OOV terms.  The question then is whether this score can be improved upon. End users are likely to require at least 90% accuracy -- likely a lot more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The boring bits ...\n",
    "### Character-level representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the inputs into the model described below consists of words in sentences represented as indices into an alphabet of letters.\n",
    "\n",
    "Script files for downloading and preprocessing the ISWOC treebank are available as part of the [old_english_models_cltk](https://github.com/cltk/old_english_models_cltk) repository.\n",
    "\n",
    "Let's load the sentences in the full set, along with their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_corpus(filename, tagged=True):\n",
    "    with open(filename, \"r\") as f:\n",
    "        sentences = []\n",
    "        words = {}\n",
    "        for line in f:\n",
    "            sentence = []\n",
    "            pairs = line.rstrip('\\n').split(' ')\n",
    "            for pair in pairs:\n",
    "                if pair != '':\n",
    "                    if tagged == True:\n",
    "                        word, tag = pair.split('/')\n",
    "                        sentence.append((word, tag))\n",
    "                        words[(word, tag)] = words.get((word, tag), 0) + 1                    \n",
    "                    else:\n",
    "                        sentence.append(pair)\n",
    "                        words[pair] = words.get(pair, 0) + 1\n",
    "            sentences.append(sentence)\n",
    "\n",
    "\n",
    "        return words, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, sents = load_corpus('../corpora/oe/oe.all_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences are just lists of (token, taglist) pairs, where a taglist is the concatenation of all morphosyntactic features for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Mæg', 'V-3spi-----i'),\n",
       "  ('gehyran', 'V----n-----n'),\n",
       "  ('se', 'Pd-s---mn--i'),\n",
       "  ('ðe', 'G----------n'),\n",
       "  ('wyle', 'V-3spx-----i'),\n",
       "  ('be', 'R----------n'),\n",
       "  ('þam', 'Pd-s---nd--i'),\n",
       "  ('halgan', 'A--s---ndpwi'),\n",
       "  ('mædene', 'Nb-s---nd--i'),\n",
       "  ('Eugenian', 'Ne-s---fd--i'),\n",
       "  ('Philyppus', 'Ne-s---mg--i'),\n",
       "  ('dæhter', 'Nb-s---fd--i'),\n",
       "  ('hu', 'Du---------n'),\n",
       "  ('heo', 'Pp3s---fn--i'),\n",
       "  ('ðurh', 'R----------n'),\n",
       "  ('mægðhad', 'Nb-s---ma--i'),\n",
       "  ('mærlice', 'Df-------p-i'),\n",
       "  ('þeah', 'V-3sui-----i'),\n",
       "  ('and', 'C----------n'),\n",
       "  ('þurh', 'R----------n'),\n",
       "  ('martyrdom', 'Nb-s---ma--i'),\n",
       "  ('þisne', 'Pd-s---ma--i'),\n",
       "  ('middaneard', 'Nb-s---ma--i'),\n",
       "  ('oferswað', 'V-xxui-----i')],\n",
       " [('Sum', 'Py-s---mnpsi'),\n",
       "  ('æþelboren', 'A--s---mnpsi'),\n",
       "  ('þægn', 'Nb-s---mn--i'),\n",
       "  ('wæs', 'V-3sui-----i'),\n",
       "  ('Philippus', 'Ne-s---mn--i'),\n",
       "  ('gehaten', 'V--xup-xx-ti'),\n",
       "  ('ðone', 'Pd-s---ma--i'),\n",
       "  ('asende', 'V-3spx-----i'),\n",
       "  ('se', 'Pd-s---mn--i'),\n",
       "  ('casere', 'Nb-s---mn--i'),\n",
       "  ('Commodus', 'Ne-s---mn--i'),\n",
       "  ('þe', 'G----------n'),\n",
       "  ('on', 'R----------n'),\n",
       "  ('ðam', 'Pd-p---md--i'),\n",
       "  ('dagum', 'Nb-p---md--i'),\n",
       "  ('rixode', 'V-3sux-----i'),\n",
       "  ('fram', 'R----------n'),\n",
       "  ('Rome.byrig', 'Ne-s---fd--i'),\n",
       "  ('to', 'R----------n'),\n",
       "  ('ðære', 'Pd-s---fd--i'),\n",
       "  ('byrig', 'Nb-s---fd--i'),\n",
       "  ('ðe', 'G----------n'),\n",
       "  ('is', 'V-3spi-----i'),\n",
       "  ('gehaten', 'V--xup-xx-ti'),\n",
       "  ('Alexandria', 'Ne-s---xx--i')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need the complete alphabet for the orthography.  In passing we'll get the maximum word length in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alphabet(words):\n",
    "    alphabet = {}\n",
    "    max_word_len = 0\n",
    "    \n",
    "    for word in words:\n",
    "        max_word_len = max(len(word), max_word_len)\n",
    "        for letter in word:\n",
    "            alphabet[letter] = alphabet.get(letter, 0) + 1\n",
    "            \n",
    "    return list(alphabet.keys()), max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "alpha, max_word_len = build_alphabet([word for word,_ in words.keys()])\n",
    "print(max_word_len)\n",
    "max_word_len = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a shorter word length to avoid too many zeros in the representations.\n",
    "\n",
    "Similarly, looking at the lengths of sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = 12\n",
    "max_sent_len = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... it looks as if we won't lose too much annotated data if we cut off the sentences at a maximum of 48 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the textual representation of tokens into vectors of indices into the alphabet, and break up the feature string into distinct sets of one-shot vectors representing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "feature_names = ['pos', 'person', 'number', 'tense', 'mood', 'voice', 'gender', 'case', 'degree', 'strength', 'inflection']\n",
    "\n",
    "def indexify(string_list):\n",
    "    vocab = list(set(string_list))\n",
    "    return np.asarray([vocab.index(elem)+1 for elem in string_list]), vocab\n",
    "\n",
    "def convert_morphology(tagged_words):\n",
    "    slicers = [(0,2),2,3,4,5,6,7,8,9,10,11]\n",
    "    feature_tags = [[] for _ in slicers]\n",
    "    \n",
    "    for _, tag in tagged_words:\n",
    "        for i, slicer in enumerate(slicers):\n",
    "            start, end = slicer if type(slicer) == tuple else (slicer, slicer + 1)\n",
    "            feature_tags[i].append(tag[start:end])\n",
    "        \n",
    "    vectors = []\n",
    "    label_sets = []\n",
    "    indices = []\n",
    "    for feature in feature_tags:\n",
    "        idx, labels = indexify(feature)\n",
    "        label_sets.append(labels)\n",
    "        indices.append(idx)\n",
    "        vectors.append(to_categorical(idx))\n",
    "        \n",
    "    return vectors, label_sets, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_letter_indices(word, alphabet, max_word_len):\n",
    "    v = np.zeros((max_word_len))\n",
    "    \n",
    "    for i in range(min(len(word), max_word_len)):\n",
    "        v[i] = alphabet.index(word[i]) + 1 if word[i] in alphabet else 0\n",
    "        \n",
    "    return v\n",
    "    \n",
    "def create_char_dataset(tagged_words, sentences, alphabet, max_sent_len, max_word_len):\n",
    "    num_sentences = len(sentences)\n",
    "    X = np.zeros((num_sentences, max_sent_len, max_word_len), dtype='int32')\n",
    "    \n",
    "    tagged_tokens = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_len = len(sent)\n",
    "        for j, (word, tag) in enumerate(sent):\n",
    "            if j >= max_sent_len:\n",
    "                break\n",
    "            tagged_tokens.append((word, tag))\n",
    "            X[i, j, :] = select_letter_indices(word, alphabet, max_word_len)\n",
    "        \n",
    "    features, labels, _ = convert_morphology(tagged_tokens)\n",
    "    \n",
    "    Y = [np.zeros((num_sentences, max_sent_len, F.shape[1])) for F in features]\n",
    "    i = 0\n",
    "    for j, sent in enumerate(sentences):\n",
    "        for k in range(min(max_sent_len, len(sent))):\n",
    "            for f in range(len(features)):\n",
    "                Y[f][j, k, :] = features[f][i]\n",
    "            i += 1\n",
    "\n",
    "    return X, Y, labels, tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, labels, _ = create_char_dataset(words, sents, alpha, max_sent_len, max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ne',\n",
       "  'Pd',\n",
       "  'F-',\n",
       "  'N-',\n",
       "  'I-',\n",
       "  'Nb',\n",
       "  'R-',\n",
       "  'Pi',\n",
       "  'G-',\n",
       "  'V-',\n",
       "  'Pp',\n",
       "  'C-',\n",
       "  'Ps',\n",
       "  'Du',\n",
       "  'Df',\n",
       "  'A-',\n",
       "  'Py',\n",
       "  'Px'],\n",
       " ['1', '-', '3', 'x', '2'],\n",
       " ['p', 'd', '-', 's', 'x'],\n",
       " ['p', 'u', '-'],\n",
       " ['n', 'p', 'i', '-', 's', 'x', 'm'],\n",
       " ['-'],\n",
       " ['n', 'p', 'r', 'o', '-', 'q', 'x', 'm', 'f'],\n",
       " ['n', 'd', 'o', 'i', 'g', '-', 'x', 'z', 'a'],\n",
       " ['p', '-', 'c', 's', 'x', 'z'],\n",
       " ['s', '-', 'w', 't'],\n",
       " ['n', 'i']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is a 3D tensor of dimensions (sentences, words, letters), and the output a list of 3D tensors of dimensions (sentences, words, one-shot-vector), one per feature (POS, case, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2536, 48, 12), (2536, 48, 19))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the datasets into training and test sets.  The last 110 sentences of the corpus correspond to the Orosius text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = X[-110:], [y[-110:] for y in Y]\n",
    "X = X[:-110]\n",
    "Y = [y[:-110] for y in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic vectors for words\n",
    "\n",
    "The second input consists of word2vec vectors representing the semantics of whole words.  The vectors were previously computed using the well-known skipgram + negative sampling method.\n",
    "\n",
    "Let's load the vocabulary and create forward and reverse indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    vocab = {}\n",
    "    rev_vocab = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for pair in f:\n",
    "            idx, word = pair.split()\n",
    "            vocab[word] = int(idx)\n",
    "            rev_vocab[int(idx)] = word\n",
    "            \n",
    "    return vocab, rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, rev_vocab = load_vocab('../models/oe/oe_types.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors were computed using OE text found online.  All of the ISWOC corpus texts are included in this larger corpus, so there will be zero OOV terms during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325198"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load and normalize the vectors themselves.  The zeroeth vector contains zeros, for empty word slots in sentences, and thus supporing Keras masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_vectors(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        vectors = pickle.load(f)\n",
    "        \n",
    "    vectors = np.concatenate([np.zeros((1,300)),vectors],axis=0)\n",
    "    norm_vectors = np.divide(vectors, np.linalg.norm(vectors, axis=-1, keepdims=True))\n",
    "    norm_vectors[0] = 0\n",
    "    \n",
    "    return norm_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jds/tensorflow-gpu/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vectors = load_vectors('../models/oe/oe_vectors.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we indexify the text into the vocabulary, thus linking the input sentences to entries in the vector table.  The latter will get loaded into a Keras embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dataset(sentences, vocab, max_sent_len):\n",
    "    X = np.zeros((len(sentences), max_sent_len))\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for j, (word, _) in enumerate(sent):\n",
    "            if j == max_sent_len:\n",
    "                break\n",
    "            X[i,j] = vocab[word]\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = create_word_dataset(sents, vocab, max_sent_len)\n",
    "X2_train = X2[:-110]\n",
    "X2_test = X2[-110:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Now we can look at the model.  It is worth noting that the model presented here is the result of a long search over possible approaches, including various mixes of convolutional and recurrent networks, including encoder-decoder setups, inception convents with attention, character-only, word-only, and more.  \n",
    "\n",
    "Which is hardly to say this is the *best* approach; just the one that so far has worked best for the specific problem of morphological analysis of OE using the ISWOC data set.\n",
    "\n",
    "### The basic idea\n",
    "\n",
    "1.  We have two embedding matrices: one, entirely trained from random initialization, representing characters; the other representing semantic word vectors.  The latter is not trained.\n",
    "2.  The character vectors are each fed to a character Bi-LSTM, to yield a vector per word.\n",
    "3.  These character-based word vectors are concatenated to word2vec vectors for each word in the sentence.\n",
    "4.  The concatenated vectors from (3) are fed to a second Bi-LSTM, that again yields a vector per word.\n",
    "5.  The BiLSTM sequence output is passed through three fully-connected feedforward layers.\n",
    "6.  A sepearate softmax layer, one per feature, is then run over the result of step 5.\n",
    "\n",
    "This model is roughly comparable to the Word+Char model tested by Horsmann and Zesch.\n",
    "\n",
    "Note 1: One *could* use convolutions for processing the character-level information.  The trouble is that Keras convolutional networks do not support masking, while RNNs do.  Hence that use of RNNs throughout.\n",
    "\n",
    "Note 2: LSTMs tend to overfit easily, hence the aggressive use of regularization in the LSTM layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "def make_char_word_rnn_model(\n",
    "    word_vectors,\n",
    "    alphabet_size, \n",
    "    char_vector_width, \n",
    "    max_sent_len,\n",
    "    max_word_len, \n",
    "    char_lstm_size,\n",
    "    word_lstm_size,\n",
    "    morpho_sizes,\n",
    "    morpho_names,\n",
    "    dense_size):\n",
    "    \n",
    "    \n",
    "    # The Char portion\n",
    "    # sub-word features\n",
    "    char_embed = layers.Embedding(alphabet_size + 1,\n",
    "                             char_vector_width,\n",
    "                             input_length = max_word_len,\n",
    "                             trainable = True,\n",
    "                             mask_zero = True)  \n",
    "    \n",
    "    # encode the input\n",
    "    char_input = Input(shape=(max_sent_len, max_word_len, ), dtype='float32')\n",
    "    vectors = layers.TimeDistributed(char_embed)(char_input)\n",
    "    \n",
    "    # run word-level RNNs over the letters\n",
    "    word_rnn = layers.Bidirectional(layers.GRU(char_lstm_size, activation='tanh', \n",
    "                                                dropout=0.2, recurrent_dropout=0.2))\n",
    "    char_vectors = layers.TimeDistributed(word_rnn)(vectors)\n",
    "   \n",
    "    # The Word portion\n",
    "    # word vectors\n",
    "    word_embed = layers.Embedding(word_vectors.shape[0],\n",
    "                                  word_vectors.shape[1],\n",
    "                                  input_length=max_sent_len,\n",
    "                                  weights=[word_vectors],\n",
    "                                  trainable = False,\n",
    "                                  mask_zero = True)\n",
    "    \n",
    "    word_input = Input(shape=(max_sent_len,), dtype='float32')\n",
    "    word_vectors = word_embed(word_input)\n",
    "    \n",
    "    char_word_vectors = layers.concatenate([char_vectors, word_vectors], axis=-1)\n",
    "    \n",
    "    word_lstm = layers.Bidirectional(layers.LSTM(word_lstm_size, return_sequences=True, \n",
    "                                                 name='word_encoder',\n",
    "                                                 activation='tanh',\n",
    "                                                 kernel_regularizer=regularizers.l2(0.01),\n",
    "                                                 activity_regularizer=regularizers.l2(0.01),\n",
    "                                                 dropout=0.4, recurrent_dropout=0.4))(char_word_vectors)\n",
    "    \n",
    "    \n",
    "    # push through dense layers\n",
    "    out = layers.Dense(dense_size, activation='relu')(word_lstm)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    \n",
    "    # generate the output via softmax layers\n",
    "    outputs = [layers.Dense(morpho_size, activation='softmax', name=morpho_names[i])(out) \n",
    "               for i, morpho_size in enumerate(morpho_sizes)]\n",
    "\n",
    "    model = Model([char_input, word_input], outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jds/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jds/tensorflow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 48, 12, 60)   3540        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 48, 200)      96600       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 48, 300)      97559700    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 500)      0           time_distributed_2[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 48, 200)      480800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48, 200)      40200       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 48, 200)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48, 200)      40200       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 48, 200)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 48, 200)      40200       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 48, 200)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pos (Dense)                     (None, 48, 19)       3819        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "person (Dense)                  (None, 48, 6)        1206        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "number (Dense)                  (None, 48, 6)        1206        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tense (Dense)                   (None, 48, 4)        804         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mood (Dense)                    (None, 48, 8)        1608        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "voice (Dense)                   (None, 48, 2)        402         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gender (Dense)                  (None, 48, 10)       2010        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "case (Dense)                    (None, 48, 10)       2010        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "degree (Dense)                  (None, 48, 7)        1407        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "strength (Dense)                (None, 48, 5)        1005        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inflection (Dense)              (None, 48, 3)        603         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 98,277,320\n",
      "Trainable params: 717,620\n",
      "Non-trainable params: 97,559,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1 = make_char_word_rnn_model(vectors, len(alpha), \n",
    "                              60, max_sent_len, max_word_len,\n",
    "                              100, 100,\n",
    "                              [y.shape[2] for y in Y],\n",
    "                              feature_names,\n",
    "                              200)\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback will help visualize the training process.  Three plots will be produced:\n",
    "\n",
    "1.  Training and validation losses.\n",
    "2.  Training accuracy for each feature type.\n",
    "3.  Validation accuracy for each feature type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-81ae5c3d799d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import keras\n",
    "\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.accs = {}\n",
    "        self.val_accs = {}\n",
    "        for f in feature_names:\n",
    "            self.accs[f] = []\n",
    "            self.val_accs[f] = []\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "        for f in feature_names:\n",
    "            self.accs[f].append(logs.get(f + '_acc'))\n",
    "            self.val_accs[f].append(logs.get('val_' + f + '_acc'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        _, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex=True, figsize=(20, 10))\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        \n",
    "        for f in feature_names:\n",
    "            ax2.plot(self.x, self.accs[f], label=(f + \"_acc\"))\n",
    "            ax3.plot(self.x, self.val_accs[f], label=\"val_\" + f + \"_acc\")\n",
    "            \n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        ax3.legend()\n",
    "        plt.show();\n",
    "        \n",
    "        for f in feature_names:\n",
    "            print(f +  \" best:\", np.round(np.max(self.val_accs[f]), 3), \"last:\", np.round(self.val_accs[f][-1], 3))\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the model.  A smaller batch-size seems to work earlier on, though in a careful training regime it emerges that increasing the batch size later yields slightly higher test accuracy.\n",
    "\n",
    "We will split the training dataset again 95/05 so that a validation (\"dev\") set will provide information about the fitting process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jds/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jds/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2426 samples, validate on 110 samples\n",
      "Epoch 1/100\n",
      "2426/2426 [==============================] - 24s 10ms/step - loss: 12.5392 - pos_loss: 2.3786 - person_loss: 0.8762 - number_loss: 1.1200 - tense_loss: 0.6319 - mood_loss: 0.8580 - voice_loss: 0.0538 - gender_loss: 1.3342 - case_loss: 1.4982 - degree_loss: 0.4786 - strength_loss: 0.4761 - inflection_loss: 0.6083 - pos_acc: 0.2046 - person_acc: 0.7064 - number_acc: 0.5022 - tense_acc: 0.8040 - mood_acc: 0.7848 - voice_acc: 0.9915 - gender_acc: 0.5091 - case_acc: 0.4699 - degree_acc: 0.9115 - strength_acc: 0.8999 - inflection_acc: 0.7229 - val_loss: 8.9812 - val_pos_loss: 2.0626 - val_person_loss: 0.5462 - val_number_loss: 0.9116 - val_tense_loss: 0.4543 - val_mood_loss: 0.5858 - val_voice_loss: 0.0012 - val_gender_loss: 1.2079 - val_case_loss: 1.3246 - val_degree_loss: 0.6234 - val_strength_loss: 0.4748 - val_inflection_loss: 0.3513 - val_pos_acc: 0.2410 - val_person_acc: 0.7983 - val_number_acc: 0.6438 - val_tense_acc: 0.8605 - val_mood_acc: 0.8447 - val_voice_acc: 1.0000 - val_gender_acc: 0.5123 - val_case_acc: 0.4828 - val_degree_acc: 0.8403 - val_strength_acc: 0.8624 - val_inflection_acc: 0.8432\n",
      "Epoch 2/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 7.8565 - pos_loss: 1.6924 - person_loss: 0.6852 - number_loss: 0.7882 - tense_loss: 0.4313 - mood_loss: 0.6129 - voice_loss: 7.3023e-04 - gender_loss: 1.0458 - case_loss: 1.2140 - degree_loss: 0.3350 - strength_loss: 0.3560 - inflection_loss: 0.2729 - pos_acc: 0.4161 - person_acc: 0.7349 - number_acc: 0.6980 - tense_acc: 0.8337 - mood_acc: 0.8080 - voice_acc: 1.0000 - gender_acc: 0.6018 - case_acc: 0.5560 - degree_acc: 0.9174 - strength_acc: 0.9078 - inflection_acc: 0.8983 - val_loss: 7.5659 - val_pos_loss: 1.5481 - val_person_loss: 0.3989 - val_number_loss: 0.8826 - val_tense_loss: 0.2799 - val_mood_loss: 0.4097 - val_voice_loss: 4.1087e-04 - val_gender_loss: 1.0823 - val_case_loss: 1.2042 - val_degree_loss: 0.5886 - val_strength_loss: 0.4765 - val_inflection_loss: 0.3067 - val_pos_acc: 0.4874 - val_person_acc: 0.8855 - val_number_acc: 0.6576 - val_tense_acc: 0.8973 - val_mood_acc: 0.8725 - val_voice_acc: 1.0000 - val_gender_acc: 0.6160 - val_case_acc: 0.5404 - val_degree_acc: 0.8403 - val_strength_acc: 0.8624 - val_inflection_acc: 0.8749\n",
      "Epoch 3/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 6.6269 - pos_loss: 1.3030 - person_loss: 0.5038 - number_loss: 0.7486 - tense_loss: 0.3077 - mood_loss: 0.4689 - voice_loss: 2.9589e-04 - gender_loss: 0.9115 - case_loss: 1.0672 - degree_loss: 0.3100 - strength_loss: 0.3332 - inflection_loss: 0.2442 - pos_acc: 0.5826 - person_acc: 0.8389 - number_acc: 0.7052 - tense_acc: 0.8832 - mood_acc: 0.8578 - voice_acc: 1.0000 - gender_acc: 0.6815 - case_acc: 0.6094 - degree_acc: 0.9171 - strength_acc: 0.9081 - inflection_acc: 0.9061 - val_loss: 6.7277 - val_pos_loss: 1.3162 - val_person_loss: 0.2257 - val_number_loss: 0.8406 - val_tense_loss: 0.2467 - val_mood_loss: 0.3539 - val_voice_loss: 1.1798e-04 - val_gender_loss: 0.9801 - val_case_loss: 1.0640 - val_degree_loss: 0.5793 - val_strength_loss: 0.4411 - val_inflection_loss: 0.2788 - val_pos_acc: 0.5717 - val_person_acc: 0.9381 - val_number_acc: 0.6820 - val_tense_acc: 0.9108 - val_mood_acc: 0.8949 - val_voice_acc: 1.0000 - val_gender_acc: 0.6468 - val_case_acc: 0.6095 - val_degree_acc: 0.8403 - val_strength_acc: 0.8624 - val_inflection_acc: 0.9014\n",
      "Epoch 4/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 5.9559 - pos_loss: 1.0805 - person_loss: 0.3986 - number_loss: 0.7062 - tense_loss: 0.2817 - mood_loss: 0.4231 - voice_loss: 1.6588e-04 - gender_loss: 0.8306 - case_loss: 0.9899 - degree_loss: 0.2920 - strength_loss: 0.3100 - inflection_loss: 0.2158 - pos_acc: 0.6542 - person_acc: 0.8709 - number_acc: 0.7161 - tense_acc: 0.8926 - mood_acc: 0.8695 - voice_acc: 1.0000 - gender_acc: 0.7079 - case_acc: 0.6326 - degree_acc: 0.9174 - strength_acc: 0.9079 - inflection_acc: 0.9182 - val_loss: 6.3249 - val_pos_loss: 1.1719 - val_person_loss: 0.2121 - val_number_loss: 0.7835 - val_tense_loss: 0.2175 - val_mood_loss: 0.3171 - val_voice_loss: 1.0042e-04 - val_gender_loss: 0.9477 - val_case_loss: 1.0167 - val_degree_loss: 0.5594 - val_strength_loss: 0.4300 - val_inflection_loss: 0.2518 - val_pos_acc: 0.6464 - val_person_acc: 0.9369 - val_number_acc: 0.6877 - val_tense_acc: 0.9161 - val_mood_acc: 0.9013 - val_voice_acc: 1.0000 - val_gender_acc: 0.6510 - val_case_acc: 0.6124 - val_degree_acc: 0.8403 - val_strength_acc: 0.8624 - val_inflection_acc: 0.9029\n",
      "Epoch 5/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 5.5648 - pos_loss: 0.9599 - person_loss: 0.3595 - number_loss: 0.6640 - tense_loss: 0.2624 - mood_loss: 0.3943 - voice_loss: 1.0374e-04 - gender_loss: 0.7895 - case_loss: 0.9287 - degree_loss: 0.2831 - strength_loss: 0.2965 - inflection_loss: 0.1969 - pos_acc: 0.6930 - person_acc: 0.8866 - number_acc: 0.7287 - tense_acc: 0.8983 - mood_acc: 0.8779 - voice_acc: 1.0000 - gender_acc: 0.7198 - case_acc: 0.6581 - degree_acc: 0.9175 - strength_acc: 0.9077 - inflection_acc: 0.9227 - val_loss: 6.0983 - val_pos_loss: 1.0778 - val_person_loss: 0.2013 - val_number_loss: 0.7704 - val_tense_loss: 0.2076 - val_mood_loss: 0.3174 - val_voice_loss: 1.0678e-04 - val_gender_loss: 0.9284 - val_case_loss: 0.9845 - val_degree_loss: 0.5395 - val_strength_loss: 0.4201 - val_inflection_loss: 0.2456 - val_pos_acc: 0.6791 - val_person_acc: 0.9483 - val_number_acc: 0.7014 - val_tense_acc: 0.9303 - val_mood_acc: 0.9050 - val_voice_acc: 1.0000 - val_gender_acc: 0.6616 - val_case_acc: 0.6478 - val_degree_acc: 0.8403 - val_strength_acc: 0.8624 - val_inflection_acc: 0.9062\n",
      "Epoch 6/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 5.2213 - pos_loss: 0.8665 - person_loss: 0.3304 - number_loss: 0.6257 - tense_loss: 0.2438 - mood_loss: 0.3577 - voice_loss: 6.7731e-05 - gender_loss: 0.7555 - case_loss: 0.8771 - degree_loss: 0.2718 - strength_loss: 0.2798 - inflection_loss: 0.1826 - pos_acc: 0.7269 - person_acc: 0.8952 - number_acc: 0.7410 - tense_acc: 0.9075 - mood_acc: 0.8882 - voice_acc: 1.0000 - gender_acc: 0.7292 - case_acc: 0.6784 - degree_acc: 0.9179 - strength_acc: 0.9086 - inflection_acc: 0.9280 - val_loss: 5.8782 - val_pos_loss: 1.0395 - val_person_loss: 0.1737 - val_number_loss: 0.7587 - val_tense_loss: 0.1816 - val_mood_loss: 0.2786 - val_voice_loss: 2.7529e-05 - val_gender_loss: 0.9007 - val_case_loss: 0.9413 - val_degree_loss: 0.5400 - val_strength_loss: 0.4004 - val_inflection_loss: 0.2495 - val_pos_acc: 0.6836 - val_person_acc: 0.9522 - val_number_acc: 0.7085 - val_tense_acc: 0.9329 - val_mood_acc: 0.9207 - val_voice_acc: 1.0000 - val_gender_acc: 0.6548 - val_case_acc: 0.6517 - val_degree_acc: 0.8403 - val_strength_acc: 0.8656 - val_inflection_acc: 0.9131\n",
      "Epoch 7/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.9851 - pos_loss: 0.7994 - person_loss: 0.3128 - number_loss: 0.5876 - tense_loss: 0.2296 - mood_loss: 0.3304 - voice_loss: 4.6834e-05 - gender_loss: 0.7360 - case_loss: 0.8351 - degree_loss: 0.2671 - strength_loss: 0.2693 - inflection_loss: 0.1699 - pos_acc: 0.7473 - person_acc: 0.8989 - number_acc: 0.7605 - tense_acc: 0.9142 - mood_acc: 0.8976 - voice_acc: 1.0000 - gender_acc: 0.7360 - case_acc: 0.6899 - degree_acc: 0.9176 - strength_acc: 0.9092 - inflection_acc: 0.9332 - val_loss: 5.6229 - val_pos_loss: 0.9559 - val_person_loss: 0.1697 - val_number_loss: 0.6978 - val_tense_loss: 0.1682 - val_mood_loss: 0.2548 - val_voice_loss: 1.8619e-05 - val_gender_loss: 0.8969 - val_case_loss: 0.8760 - val_degree_loss: 0.5462 - val_strength_loss: 0.3963 - val_inflection_loss: 0.2355 - val_pos_acc: 0.7102 - val_person_acc: 0.9558 - val_number_acc: 0.7315 - val_tense_acc: 0.9387 - val_mood_acc: 0.9166 - val_voice_acc: 1.0000 - val_gender_acc: 0.6679 - val_case_acc: 0.6804 - val_degree_acc: 0.8403 - val_strength_acc: 0.8688 - val_inflection_acc: 0.9049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.6936 - pos_loss: 0.7324 - person_loss: 0.2914 - number_loss: 0.5481 - tense_loss: 0.2063 - mood_loss: 0.3015 - voice_loss: 3.1668e-05 - gender_loss: 0.7024 - case_loss: 0.7877 - degree_loss: 0.2581 - strength_loss: 0.2554 - inflection_loss: 0.1575 - pos_acc: 0.7695 - person_acc: 0.9067 - number_acc: 0.7805 - tense_acc: 0.9251 - mood_acc: 0.9065 - voice_acc: 1.0000 - gender_acc: 0.7455 - case_acc: 0.7085 - degree_acc: 0.9179 - strength_acc: 0.9128 - inflection_acc: 0.9373 - val_loss: 5.5096 - val_pos_loss: 0.9122 - val_person_loss: 0.1533 - val_number_loss: 0.7064 - val_tense_loss: 0.1668 - val_mood_loss: 0.2402 - val_voice_loss: 1.7809e-05 - val_gender_loss: 0.8756 - val_case_loss: 0.8457 - val_degree_loss: 0.5355 - val_strength_loss: 0.4089 - val_inflection_loss: 0.2232 - val_pos_acc: 0.7303 - val_person_acc: 0.9540 - val_number_acc: 0.7335 - val_tense_acc: 0.9382 - val_mood_acc: 0.9175 - val_voice_acc: 1.0000 - val_gender_acc: 0.6780 - val_case_acc: 0.6875 - val_degree_acc: 0.8449 - val_strength_acc: 0.8787 - val_inflection_acc: 0.9067\n",
      "Epoch 9/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.5409 - pos_loss: 0.6948 - person_loss: 0.2827 - number_loss: 0.5186 - tense_loss: 0.2012 - mood_loss: 0.2854 - voice_loss: 3.7511e-05 - gender_loss: 0.6873 - case_loss: 0.7630 - degree_loss: 0.2511 - strength_loss: 0.2434 - inflection_loss: 0.1559 - pos_acc: 0.7844 - person_acc: 0.9102 - number_acc: 0.7951 - tense_acc: 0.9262 - mood_acc: 0.9129 - voice_acc: 1.0000 - gender_acc: 0.7472 - case_acc: 0.7149 - degree_acc: 0.9192 - strength_acc: 0.9187 - inflection_acc: 0.9396 - val_loss: 5.3320 - val_pos_loss: 0.8621 - val_person_loss: 0.1806 - val_number_loss: 0.6714 - val_tense_loss: 0.1495 - val_mood_loss: 0.2271 - val_voice_loss: 2.6431e-05 - val_gender_loss: 0.8415 - val_case_loss: 0.8034 - val_degree_loss: 0.5366 - val_strength_loss: 0.3944 - val_inflection_loss: 0.2150 - val_pos_acc: 0.7450 - val_person_acc: 0.9503 - val_number_acc: 0.7435 - val_tense_acc: 0.9403 - val_mood_acc: 0.9290 - val_voice_acc: 1.0000 - val_gender_acc: 0.6831 - val_case_acc: 0.7000 - val_degree_acc: 0.8443 - val_strength_acc: 0.8781 - val_inflection_acc: 0.9125\n",
      "Epoch 10/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.3773 - pos_loss: 0.6569 - person_loss: 0.2677 - number_loss: 0.4997 - tense_loss: 0.1928 - mood_loss: 0.2699 - voice_loss: 3.2829e-05 - gender_loss: 0.6641 - case_loss: 0.7330 - degree_loss: 0.2458 - strength_loss: 0.2348 - inflection_loss: 0.1481 - pos_acc: 0.7949 - person_acc: 0.9148 - number_acc: 0.8022 - tense_acc: 0.9312 - mood_acc: 0.9171 - voice_acc: 1.0000 - gender_acc: 0.7569 - case_acc: 0.7299 - degree_acc: 0.9216 - strength_acc: 0.9225 - inflection_acc: 0.9425 - val_loss: 5.2606 - val_pos_loss: 0.8528 - val_person_loss: 0.1537 - val_number_loss: 0.6542 - val_tense_loss: 0.1458 - val_mood_loss: 0.2201 - val_voice_loss: 2.0932e-05 - val_gender_loss: 0.8653 - val_case_loss: 0.7936 - val_degree_loss: 0.5170 - val_strength_loss: 0.3911 - val_inflection_loss: 0.2142 - val_pos_acc: 0.7472 - val_person_acc: 0.9501 - val_number_acc: 0.7454 - val_tense_acc: 0.9478 - val_mood_acc: 0.9229 - val_voice_acc: 1.0000 - val_gender_acc: 0.6834 - val_case_acc: 0.7042 - val_degree_acc: 0.8477 - val_strength_acc: 0.8801 - val_inflection_acc: 0.9068\n",
      "Epoch 11/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.2169 - pos_loss: 0.6190 - person_loss: 0.2570 - number_loss: 0.4770 - tense_loss: 0.1882 - mood_loss: 0.2577 - voice_loss: 2.9568e-05 - gender_loss: 0.6460 - case_loss: 0.7025 - degree_loss: 0.2377 - strength_loss: 0.2198 - inflection_loss: 0.1389 - pos_acc: 0.8074 - person_acc: 0.9166 - number_acc: 0.8112 - tense_acc: 0.9317 - mood_acc: 0.9194 - voice_acc: 1.0000 - gender_acc: 0.7611 - case_acc: 0.7389 - degree_acc: 0.9230 - strength_acc: 0.9276 - inflection_acc: 0.9476 - val_loss: 5.1466 - val_pos_loss: 0.8158 - val_person_loss: 0.1440 - val_number_loss: 0.6459 - val_tense_loss: 0.1553 - val_mood_loss: 0.2284 - val_voice_loss: 9.5528e-06 - val_gender_loss: 0.8066 - val_case_loss: 0.7654 - val_degree_loss: 0.5384 - val_strength_loss: 0.3834 - val_inflection_loss: 0.2069 - val_pos_acc: 0.7718 - val_person_acc: 0.9549 - val_number_acc: 0.7535 - val_tense_acc: 0.9457 - val_mood_acc: 0.9194 - val_voice_acc: 1.0000 - val_gender_acc: 0.7056 - val_case_acc: 0.7144 - val_degree_acc: 0.8525 - val_strength_acc: 0.8869 - val_inflection_acc: 0.9180\n",
      "Epoch 12/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 4.0889 - pos_loss: 0.5822 - person_loss: 0.2476 - number_loss: 0.4714 - tense_loss: 0.1803 - mood_loss: 0.2476 - voice_loss: 2.3341e-05 - gender_loss: 0.6254 - case_loss: 0.6850 - degree_loss: 0.2288 - strength_loss: 0.2092 - inflection_loss: 0.1412 - pos_acc: 0.8176 - person_acc: 0.9207 - number_acc: 0.8161 - tense_acc: 0.9350 - mood_acc: 0.9224 - voice_acc: 1.0000 - gender_acc: 0.7696 - case_acc: 0.7500 - degree_acc: 0.9264 - strength_acc: 0.9319 - inflection_acc: 0.9473 - val_loss: 5.0099 - val_pos_loss: 0.7885 - val_person_loss: 0.1396 - val_number_loss: 0.6303 - val_tense_loss: 0.1309 - val_mood_loss: 0.1913 - val_voice_loss: 9.6650e-06 - val_gender_loss: 0.8025 - val_case_loss: 0.7515 - val_degree_loss: 0.5230 - val_strength_loss: 0.3768 - val_inflection_loss: 0.2144 - val_pos_acc: 0.7730 - val_person_acc: 0.9550 - val_number_acc: 0.7640 - val_tense_acc: 0.9479 - val_mood_acc: 0.9356 - val_voice_acc: 1.0000 - val_gender_acc: 0.7144 - val_case_acc: 0.7172 - val_degree_acc: 0.8587 - val_strength_acc: 0.8938 - val_inflection_acc: 0.9129\n",
      "Epoch 13/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.9665 - pos_loss: 0.5621 - person_loss: 0.2368 - number_loss: 0.4516 - tense_loss: 0.1710 - mood_loss: 0.2330 - voice_loss: 1.9986e-05 - gender_loss: 0.6112 - case_loss: 0.6578 - degree_loss: 0.2264 - strength_loss: 0.2030 - inflection_loss: 0.1334 - pos_acc: 0.8231 - person_acc: 0.9226 - number_acc: 0.8227 - tense_acc: 0.9389 - mood_acc: 0.9267 - voice_acc: 1.0000 - gender_acc: 0.7717 - case_acc: 0.7563 - degree_acc: 0.9254 - strength_acc: 0.9335 - inflection_acc: 0.9484 - val_loss: 4.9837 - val_pos_loss: 0.7929 - val_person_loss: 0.1459 - val_number_loss: 0.6115 - val_tense_loss: 0.1391 - val_mood_loss: 0.2033 - val_voice_loss: 8.5808e-06 - val_gender_loss: 0.7950 - val_case_loss: 0.7411 - val_degree_loss: 0.5130 - val_strength_loss: 0.3754 - val_inflection_loss: 0.2022 - val_pos_acc: 0.7720 - val_person_acc: 0.9545 - val_number_acc: 0.7705 - val_tense_acc: 0.9503 - val_mood_acc: 0.9306 - val_voice_acc: 1.0000 - val_gender_acc: 0.7092 - val_case_acc: 0.7247 - val_degree_acc: 0.8603 - val_strength_acc: 0.8900 - val_inflection_acc: 0.9284\n",
      "Epoch 14/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.8582 - pos_loss: 0.5288 - person_loss: 0.2264 - number_loss: 0.4416 - tense_loss: 0.1673 - mood_loss: 0.2314 - voice_loss: 1.8797e-05 - gender_loss: 0.5959 - case_loss: 0.6399 - degree_loss: 0.2193 - strength_loss: 0.1944 - inflection_loss: 0.1295 - pos_acc: 0.8346 - person_acc: 0.9254 - number_acc: 0.8257 - tense_acc: 0.9419 - mood_acc: 0.9305 - voice_acc: 1.0000 - gender_acc: 0.7796 - case_acc: 0.7632 - degree_acc: 0.9292 - strength_acc: 0.9376 - inflection_acc: 0.9501 - val_loss: 4.8459 - val_pos_loss: 0.7574 - val_person_loss: 0.1492 - val_number_loss: 0.5965 - val_tense_loss: 0.1305 - val_mood_loss: 0.1914 - val_voice_loss: 1.0508e-05 - val_gender_loss: 0.7693 - val_case_loss: 0.7222 - val_degree_loss: 0.5001 - val_strength_loss: 0.3675 - val_inflection_loss: 0.1929 - val_pos_acc: 0.7856 - val_person_acc: 0.9603 - val_number_acc: 0.7687 - val_tense_acc: 0.9554 - val_mood_acc: 0.9385 - val_voice_acc: 1.0000 - val_gender_acc: 0.7268 - val_case_acc: 0.7398 - val_degree_acc: 0.8531 - val_strength_acc: 0.8899 - val_inflection_acc: 0.9344\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.7833 - pos_loss: 0.5151 - person_loss: 0.2210 - number_loss: 0.4316 - tense_loss: 0.1609 - mood_loss: 0.2196 - voice_loss: 1.5463e-05 - gender_loss: 0.5871 - case_loss: 0.6291 - degree_loss: 0.2137 - strength_loss: 0.1872 - inflection_loss: 0.1248 - pos_acc: 0.8386 - person_acc: 0.9276 - number_acc: 0.8321 - tense_acc: 0.9433 - mood_acc: 0.9312 - voice_acc: 1.0000 - gender_acc: 0.7828 - case_acc: 0.7668 - degree_acc: 0.9293 - strength_acc: 0.9381 - inflection_acc: 0.9523 - val_loss: 4.9069 - val_pos_loss: 0.7513 - val_person_loss: 0.1631 - val_number_loss: 0.6126 - val_tense_loss: 0.1403 - val_mood_loss: 0.2090 - val_voice_loss: 1.0369e-05 - val_gender_loss: 0.7746 - val_case_loss: 0.7333 - val_degree_loss: 0.4944 - val_strength_loss: 0.3618 - val_inflection_loss: 0.1975 - val_pos_acc: 0.7906 - val_person_acc: 0.9512 - val_number_acc: 0.7733 - val_tense_acc: 0.9547 - val_mood_acc: 0.9334 - val_voice_acc: 1.0000 - val_gender_acc: 0.7227 - val_case_acc: 0.7213 - val_degree_acc: 0.8599 - val_strength_acc: 0.8915 - val_inflection_acc: 0.9295\n",
      "Epoch 16/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.6749 - pos_loss: 0.4912 - person_loss: 0.2118 - number_loss: 0.4237 - tense_loss: 0.1570 - mood_loss: 0.2145 - voice_loss: 1.4522e-05 - gender_loss: 0.5704 - case_loss: 0.6075 - degree_loss: 0.2085 - strength_loss: 0.1824 - inflection_loss: 0.1229 - pos_acc: 0.8460 - person_acc: 0.9296 - number_acc: 0.8342 - tense_acc: 0.9436 - mood_acc: 0.9312 - voice_acc: 1.0000 - gender_acc: 0.7880 - case_acc: 0.7757 - degree_acc: 0.9314 - strength_acc: 0.9394 - inflection_acc: 0.9548 - val_loss: 4.7866 - val_pos_loss: 0.7332 - val_person_loss: 0.1399 - val_number_loss: 0.6026 - val_tense_loss: 0.1306 - val_mood_loss: 0.2007 - val_voice_loss: 5.3522e-06 - val_gender_loss: 0.7722 - val_case_loss: 0.7084 - val_degree_loss: 0.4754 - val_strength_loss: 0.3387 - val_inflection_loss: 0.2090 - val_pos_acc: 0.7922 - val_person_acc: 0.9599 - val_number_acc: 0.7623 - val_tense_acc: 0.9506 - val_mood_acc: 0.9354 - val_voice_acc: 1.0000 - val_gender_acc: 0.7262 - val_case_acc: 0.7431 - val_degree_acc: 0.8593 - val_strength_acc: 0.8952 - val_inflection_acc: 0.9249\n",
      "Epoch 17/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.6821 - pos_loss: 0.4944 - person_loss: 0.2154 - number_loss: 0.4203 - tense_loss: 0.1586 - mood_loss: 0.2178 - voice_loss: 1.4988e-05 - gender_loss: 0.5661 - case_loss: 0.6027 - degree_loss: 0.2084 - strength_loss: 0.1845 - inflection_loss: 0.1235 - pos_acc: 0.8463 - person_acc: 0.9287 - number_acc: 0.8386 - tense_acc: 0.9425 - mood_acc: 0.9324 - voice_acc: 1.0000 - gender_acc: 0.7922 - case_acc: 0.7769 - degree_acc: 0.9310 - strength_acc: 0.9412 - inflection_acc: 0.9539 - val_loss: 4.6045 - val_pos_loss: 0.7064 - val_person_loss: 0.1296 - val_number_loss: 0.5609 - val_tense_loss: 0.1218 - val_mood_loss: 0.1832 - val_voice_loss: 5.8490e-06 - val_gender_loss: 0.7422 - val_case_loss: 0.6823 - val_degree_loss: 0.4783 - val_strength_loss: 0.3379 - val_inflection_loss: 0.1851 - val_pos_acc: 0.7967 - val_person_acc: 0.9622 - val_number_acc: 0.7969 - val_tense_acc: 0.9589 - val_mood_acc: 0.9364 - val_voice_acc: 1.0000 - val_gender_acc: 0.7266 - val_case_acc: 0.7415 - val_degree_acc: 0.8589 - val_strength_acc: 0.9026 - val_inflection_acc: 0.9317\n",
      "Epoch 18/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.5489 - pos_loss: 0.4632 - person_loss: 0.2002 - number_loss: 0.4090 - tense_loss: 0.1471 - mood_loss: 0.2032 - voice_loss: 9.2498e-06 - gender_loss: 0.5583 - case_loss: 0.5848 - degree_loss: 0.1996 - strength_loss: 0.1727 - inflection_loss: 0.1201 - pos_acc: 0.8566 - person_acc: 0.9330 - number_acc: 0.8425 - tense_acc: 0.9463 - mood_acc: 0.9357 - voice_acc: 1.0000 - gender_acc: 0.7932 - case_acc: 0.7843 - degree_acc: 0.9336 - strength_acc: 0.9446 - inflection_acc: 0.9548 - val_loss: 4.6416 - val_pos_loss: 0.6947 - val_person_loss: 0.1442 - val_number_loss: 0.5833 - val_tense_loss: 0.1223 - val_mood_loss: 0.2000 - val_voice_loss: 1.1482e-05 - val_gender_loss: 0.7439 - val_case_loss: 0.6864 - val_degree_loss: 0.4649 - val_strength_loss: 0.3317 - val_inflection_loss: 0.1926 - val_pos_acc: 0.8002 - val_person_acc: 0.9612 - val_number_acc: 0.7716 - val_tense_acc: 0.9569 - val_mood_acc: 0.9343 - val_voice_acc: 1.0000 - val_gender_acc: 0.7330 - val_case_acc: 0.7419 - val_degree_acc: 0.8643 - val_strength_acc: 0.9015 - val_inflection_acc: 0.9217\n",
      "Epoch 19/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.4786 - pos_loss: 0.4540 - person_loss: 0.1964 - number_loss: 0.3990 - tense_loss: 0.1422 - mood_loss: 0.1926 - voice_loss: 9.8909e-06 - gender_loss: 0.5445 - case_loss: 0.5719 - degree_loss: 0.1995 - strength_loss: 0.1696 - inflection_loss: 0.1153 - pos_acc: 0.8601 - person_acc: 0.9339 - number_acc: 0.8467 - tense_acc: 0.9494 - mood_acc: 0.9390 - voice_acc: 1.0000 - gender_acc: 0.7985 - case_acc: 0.7883 - degree_acc: 0.9346 - strength_acc: 0.9457 - inflection_acc: 0.9562 - val_loss: 4.5126 - val_pos_loss: 0.6836 - val_person_loss: 0.1420 - val_number_loss: 0.5471 - val_tense_loss: 0.1177 - val_mood_loss: 0.1865 - val_voice_loss: 6.3339e-06 - val_gender_loss: 0.7406 - val_case_loss: 0.6513 - val_degree_loss: 0.4505 - val_strength_loss: 0.3383 - val_inflection_loss: 0.1750 - val_pos_acc: 0.8049 - val_person_acc: 0.9596 - val_number_acc: 0.8147 - val_tense_acc: 0.9599 - val_mood_acc: 0.9406 - val_voice_acc: 1.0000 - val_gender_acc: 0.7294 - val_case_acc: 0.7536 - val_degree_acc: 0.8665 - val_strength_acc: 0.8988 - val_inflection_acc: 0.9423\n",
      "Epoch 20/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.4331 - pos_loss: 0.4420 - person_loss: 0.1904 - number_loss: 0.3968 - tense_loss: 0.1426 - mood_loss: 0.1957 - voice_loss: 7.4115e-06 - gender_loss: 0.5389 - case_loss: 0.5642 - degree_loss: 0.1967 - strength_loss: 0.1678 - inflection_loss: 0.1111 - pos_acc: 0.8622 - person_acc: 0.9360 - number_acc: 0.8486 - tense_acc: 0.9485 - mood_acc: 0.9379 - voice_acc: 1.0000 - gender_acc: 0.7994 - case_acc: 0.7913 - degree_acc: 0.9327 - strength_acc: 0.9435 - inflection_acc: 0.9585 - val_loss: 4.5870 - val_pos_loss: 0.6869 - val_person_loss: 0.1450 - val_number_loss: 0.5867 - val_tense_loss: 0.1171 - val_mood_loss: 0.1797 - val_voice_loss: 4.3818e-06 - val_gender_loss: 0.7519 - val_case_loss: 0.6738 - val_degree_loss: 0.4512 - val_strength_loss: 0.3296 - val_inflection_loss: 0.1974 - val_pos_acc: 0.8106 - val_person_acc: 0.9578 - val_number_acc: 0.7825 - val_tense_acc: 0.9609 - val_mood_acc: 0.9403 - val_voice_acc: 1.0000 - val_gender_acc: 0.7289 - val_case_acc: 0.7570 - val_degree_acc: 0.8694 - val_strength_acc: 0.9036 - val_inflection_acc: 0.9357\n",
      "Epoch 21/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.3564 - pos_loss: 0.4298 - person_loss: 0.1911 - number_loss: 0.3868 - tense_loss: 0.1400 - mood_loss: 0.1842 - voice_loss: 6.8234e-06 - gender_loss: 0.5279 - case_loss: 0.5529 - degree_loss: 0.1881 - strength_loss: 0.1613 - inflection_loss: 0.1094 - pos_acc: 0.8637 - person_acc: 0.9353 - number_acc: 0.8523 - tense_acc: 0.9500 - mood_acc: 0.9420 - voice_acc: 1.0000 - gender_acc: 0.8063 - case_acc: 0.7955 - degree_acc: 0.9383 - strength_acc: 0.9473 - inflection_acc: 0.9595 - val_loss: 4.5605 - val_pos_loss: 0.6972 - val_person_loss: 0.1468 - val_number_loss: 0.5502 - val_tense_loss: 0.1243 - val_mood_loss: 0.1931 - val_voice_loss: 5.2086e-06 - val_gender_loss: 0.7373 - val_case_loss: 0.6563 - val_degree_loss: 0.4598 - val_strength_loss: 0.3354 - val_inflection_loss: 0.1886 - val_pos_acc: 0.8085 - val_person_acc: 0.9584 - val_number_acc: 0.8137 - val_tense_acc: 0.9575 - val_mood_acc: 0.9379 - val_voice_acc: 1.0000 - val_gender_acc: 0.7356 - val_case_acc: 0.7580 - val_degree_acc: 0.8592 - val_strength_acc: 0.8978 - val_inflection_acc: 0.9366\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.3114 - pos_loss: 0.4231 - person_loss: 0.1858 - number_loss: 0.3822 - tense_loss: 0.1336 - mood_loss: 0.1782 - voice_loss: 6.3914e-06 - gender_loss: 0.5206 - case_loss: 0.5421 - degree_loss: 0.1882 - strength_loss: 0.1581 - inflection_loss: 0.1069 - pos_acc: 0.8679 - person_acc: 0.9395 - number_acc: 0.8509 - tense_acc: 0.9532 - mood_acc: 0.9445 - voice_acc: 1.0000 - gender_acc: 0.8066 - case_acc: 0.7999 - degree_acc: 0.9359 - strength_acc: 0.9467 - inflection_acc: 0.9593 - val_loss: 4.3863 - val_pos_loss: 0.6525 - val_person_loss: 0.1163 - val_number_loss: 0.5281 - val_tense_loss: 0.1099 - val_mood_loss: 0.1726 - val_voice_loss: 4.7714e-06 - val_gender_loss: 0.7178 - val_case_loss: 0.6525 - val_degree_loss: 0.4415 - val_strength_loss: 0.3184 - val_inflection_loss: 0.1889 - val_pos_acc: 0.8227 - val_person_acc: 0.9694 - val_number_acc: 0.8241 - val_tense_acc: 0.9618 - val_mood_acc: 0.9444 - val_voice_acc: 1.0000 - val_gender_acc: 0.7373 - val_case_acc: 0.7606 - val_degree_acc: 0.8706 - val_strength_acc: 0.9027 - val_inflection_acc: 0.9371\n",
      "Epoch 23/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.2531 - pos_loss: 0.4112 - person_loss: 0.1765 - number_loss: 0.3692 - tense_loss: 0.1306 - mood_loss: 0.1813 - voice_loss: 6.4584e-06 - gender_loss: 0.5140 - case_loss: 0.5353 - degree_loss: 0.1836 - strength_loss: 0.1536 - inflection_loss: 0.1048 - pos_acc: 0.8716 - person_acc: 0.9403 - number_acc: 0.8576 - tense_acc: 0.9538 - mood_acc: 0.9423 - voice_acc: 1.0000 - gender_acc: 0.8101 - case_acc: 0.8011 - degree_acc: 0.9382 - strength_acc: 0.9493 - inflection_acc: 0.9609 - val_loss: 4.3787 - val_pos_loss: 0.6711 - val_person_loss: 0.1323 - val_number_loss: 0.5300 - val_tense_loss: 0.1090 - val_mood_loss: 0.1647 - val_voice_loss: 4.8414e-06 - val_gender_loss: 0.7264 - val_case_loss: 0.6356 - val_degree_loss: 0.4339 - val_strength_loss: 0.3214 - val_inflection_loss: 0.1837 - val_pos_acc: 0.8099 - val_person_acc: 0.9652 - val_number_acc: 0.7965 - val_tense_acc: 0.9681 - val_mood_acc: 0.9497 - val_voice_acc: 1.0000 - val_gender_acc: 0.7384 - val_case_acc: 0.7621 - val_degree_acc: 0.8722 - val_strength_acc: 0.9108 - val_inflection_acc: 0.9407\n",
      "Epoch 24/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.1892 - pos_loss: 0.4042 - person_loss: 0.1747 - number_loss: 0.3627 - tense_loss: 0.1247 - mood_loss: 0.1713 - voice_loss: 6.3439e-06 - gender_loss: 0.5025 - case_loss: 0.5243 - degree_loss: 0.1805 - strength_loss: 0.1521 - inflection_loss: 0.1047 - pos_acc: 0.8752 - person_acc: 0.9409 - number_acc: 0.8600 - tense_acc: 0.9564 - mood_acc: 0.9459 - voice_acc: 1.0000 - gender_acc: 0.8157 - case_acc: 0.8066 - degree_acc: 0.9397 - strength_acc: 0.9501 - inflection_acc: 0.9610 - val_loss: 4.2493 - val_pos_loss: 0.6342 - val_person_loss: 0.1066 - val_number_loss: 0.5305 - val_tense_loss: 0.0928 - val_mood_loss: 0.1560 - val_voice_loss: 2.6757e-06 - val_gender_loss: 0.7076 - val_case_loss: 0.6216 - val_degree_loss: 0.4381 - val_strength_loss: 0.3026 - val_inflection_loss: 0.1879 - val_pos_acc: 0.8176 - val_person_acc: 0.9683 - val_number_acc: 0.7927 - val_tense_acc: 0.9679 - val_mood_acc: 0.9457 - val_voice_acc: 1.0000 - val_gender_acc: 0.7403 - val_case_acc: 0.7764 - val_degree_acc: 0.8709 - val_strength_acc: 0.9040 - val_inflection_acc: 0.9349\n",
      "Epoch 25/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.1483 - pos_loss: 0.3928 - person_loss: 0.1710 - number_loss: 0.3576 - tense_loss: 0.1254 - mood_loss: 0.1672 - voice_loss: 5.4752e-06 - gender_loss: 0.4996 - case_loss: 0.5147 - degree_loss: 0.1766 - strength_loss: 0.1477 - inflection_loss: 0.1017 - pos_acc: 0.8788 - person_acc: 0.9424 - number_acc: 0.8650 - tense_acc: 0.9559 - mood_acc: 0.9458 - voice_acc: 1.0000 - gender_acc: 0.8177 - case_acc: 0.8097 - degree_acc: 0.9418 - strength_acc: 0.9513 - inflection_acc: 0.9624 - val_loss: 4.3363 - val_pos_loss: 0.6538 - val_person_loss: 0.1314 - val_number_loss: 0.5135 - val_tense_loss: 0.1091 - val_mood_loss: 0.1823 - val_voice_loss: 3.8094e-06 - val_gender_loss: 0.7018 - val_case_loss: 0.6333 - val_degree_loss: 0.4348 - val_strength_loss: 0.3078 - val_inflection_loss: 0.1870 - val_pos_acc: 0.8121 - val_person_acc: 0.9611 - val_number_acc: 0.8202 - val_tense_acc: 0.9632 - val_mood_acc: 0.9459 - val_voice_acc: 1.0000 - val_gender_acc: 0.7419 - val_case_acc: 0.7671 - val_degree_acc: 0.8777 - val_strength_acc: 0.9104 - val_inflection_acc: 0.9331\n",
      "Epoch 26/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.1213 - pos_loss: 0.3896 - person_loss: 0.1703 - number_loss: 0.3531 - tense_loss: 0.1214 - mood_loss: 0.1664 - voice_loss: 6.1251e-06 - gender_loss: 0.4946 - case_loss: 0.5115 - degree_loss: 0.1746 - strength_loss: 0.1466 - inflection_loss: 0.1019 - pos_acc: 0.8778 - person_acc: 0.9417 - number_acc: 0.8653 - tense_acc: 0.9575 - mood_acc: 0.9469 - voice_acc: 1.0000 - gender_acc: 0.8190 - case_acc: 0.8103 - degree_acc: 0.9418 - strength_acc: 0.9524 - inflection_acc: 0.9615 - val_loss: 4.1914 - val_pos_loss: 0.6220 - val_person_loss: 0.1128 - val_number_loss: 0.5028 - val_tense_loss: 0.0978 - val_mood_loss: 0.1569 - val_voice_loss: 3.4503e-06 - val_gender_loss: 0.7013 - val_case_loss: 0.6126 - val_degree_loss: 0.4309 - val_strength_loss: 0.3032 - val_inflection_loss: 0.1830 - val_pos_acc: 0.8319 - val_person_acc: 0.9690 - val_number_acc: 0.8248 - val_tense_acc: 0.9643 - val_mood_acc: 0.9512 - val_voice_acc: 1.0000 - val_gender_acc: 0.7556 - val_case_acc: 0.7743 - val_degree_acc: 0.8789 - val_strength_acc: 0.9108 - val_inflection_acc: 0.9401\n",
      "Epoch 27/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.0767 - pos_loss: 0.3800 - person_loss: 0.1683 - number_loss: 0.3474 - tense_loss: 0.1213 - mood_loss: 0.1642 - voice_loss: 5.4933e-06 - gender_loss: 0.4893 - case_loss: 0.5041 - degree_loss: 0.1719 - strength_loss: 0.1410 - inflection_loss: 0.0985 - pos_acc: 0.8820 - person_acc: 0.9428 - number_acc: 0.8687 - tense_acc: 0.9571 - mood_acc: 0.9470 - voice_acc: 1.0000 - gender_acc: 0.8199 - case_acc: 0.8119 - degree_acc: 0.9441 - strength_acc: 0.9529 - inflection_acc: 0.9643 - val_loss: 4.3233 - val_pos_loss: 0.6382 - val_person_loss: 0.1189 - val_number_loss: 0.5322 - val_tense_loss: 0.1092 - val_mood_loss: 0.1658 - val_voice_loss: 2.6601e-06 - val_gender_loss: 0.7222 - val_case_loss: 0.6317 - val_degree_loss: 0.4299 - val_strength_loss: 0.3117 - val_inflection_loss: 0.1927 - val_pos_acc: 0.8205 - val_person_acc: 0.9649 - val_number_acc: 0.8189 - val_tense_acc: 0.9595 - val_mood_acc: 0.9467 - val_voice_acc: 1.0000 - val_gender_acc: 0.7577 - val_case_acc: 0.7673 - val_degree_acc: 0.8801 - val_strength_acc: 0.9099 - val_inflection_acc: 0.9363\n",
      "Epoch 28/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 3.0524 - pos_loss: 0.3775 - person_loss: 0.1678 - number_loss: 0.3419 - tense_loss: 0.1189 - mood_loss: 0.1618 - voice_loss: 4.6086e-06 - gender_loss: 0.4840 - case_loss: 0.4958 - degree_loss: 0.1686 - strength_loss: 0.1412 - inflection_loss: 0.1000 - pos_acc: 0.8827 - person_acc: 0.9442 - number_acc: 0.8722 - tense_acc: 0.9580 - mood_acc: 0.9483 - voice_acc: 1.0000 - gender_acc: 0.8231 - case_acc: 0.8163 - degree_acc: 0.9437 - strength_acc: 0.9543 - inflection_acc: 0.9641 - val_loss: 4.1651 - val_pos_loss: 0.6181 - val_person_loss: 0.1205 - val_number_loss: 0.4828 - val_tense_loss: 0.1029 - val_mood_loss: 0.1602 - val_voice_loss: 3.3394e-06 - val_gender_loss: 0.7011 - val_case_loss: 0.6045 - val_degree_loss: 0.4186 - val_strength_loss: 0.3101 - val_inflection_loss: 0.1660 - val_pos_acc: 0.8260 - val_person_acc: 0.9609 - val_number_acc: 0.8394 - val_tense_acc: 0.9637 - val_mood_acc: 0.9416 - val_voice_acc: 1.0000 - val_gender_acc: 0.7525 - val_case_acc: 0.7775 - val_degree_acc: 0.8779 - val_strength_acc: 0.9122 - val_inflection_acc: 0.9433\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.9708 - pos_loss: 0.3605 - person_loss: 0.1559 - number_loss: 0.3364 - tense_loss: 0.1147 - mood_loss: 0.1557 - voice_loss: 4.4747e-06 - gender_loss: 0.4769 - case_loss: 0.4878 - degree_loss: 0.1621 - strength_loss: 0.1322 - inflection_loss: 0.0957 - pos_acc: 0.8877 - person_acc: 0.9458 - number_acc: 0.8732 - tense_acc: 0.9592 - mood_acc: 0.9502 - voice_acc: 1.0000 - gender_acc: 0.8264 - case_acc: 0.8186 - degree_acc: 0.9467 - strength_acc: 0.9565 - inflection_acc: 0.9643 - val_loss: 4.0892 - val_pos_loss: 0.6026 - val_person_loss: 0.1147 - val_number_loss: 0.4839 - val_tense_loss: 0.1070 - val_mood_loss: 0.1581 - val_voice_loss: 1.8826e-06 - val_gender_loss: 0.6754 - val_case_loss: 0.6111 - val_degree_loss: 0.4131 - val_strength_loss: 0.2863 - val_inflection_loss: 0.1746 - val_pos_acc: 0.8329 - val_person_acc: 0.9674 - val_number_acc: 0.8440 - val_tense_acc: 0.9642 - val_mood_acc: 0.9505 - val_voice_acc: 1.0000 - val_gender_acc: 0.7710 - val_case_acc: 0.7764 - val_degree_acc: 0.8857 - val_strength_acc: 0.9209 - val_inflection_acc: 0.9447\n",
      "Epoch 30/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.9330 - pos_loss: 0.3590 - person_loss: 0.1553 - number_loss: 0.3296 - tense_loss: 0.1126 - mood_loss: 0.1511 - voice_loss: 4.0367e-06 - gender_loss: 0.4657 - case_loss: 0.4788 - degree_loss: 0.1654 - strength_loss: 0.1365 - inflection_loss: 0.0963 - pos_acc: 0.8886 - person_acc: 0.9469 - number_acc: 0.8754 - tense_acc: 0.9601 - mood_acc: 0.9512 - voice_acc: 1.0000 - gender_acc: 0.8296 - case_acc: 0.8238 - degree_acc: 0.9443 - strength_acc: 0.9546 - inflection_acc: 0.9652 - val_loss: 4.0742 - val_pos_loss: 0.6199 - val_person_loss: 0.1096 - val_number_loss: 0.4820 - val_tense_loss: 0.0969 - val_mood_loss: 0.1444 - val_voice_loss: 2.8070e-06 - val_gender_loss: 0.6631 - val_case_loss: 0.5818 - val_degree_loss: 0.4385 - val_strength_loss: 0.2909 - val_inflection_loss: 0.1812 - val_pos_acc: 0.8379 - val_person_acc: 0.9674 - val_number_acc: 0.8454 - val_tense_acc: 0.9678 - val_mood_acc: 0.9530 - val_voice_acc: 1.0000 - val_gender_acc: 0.7791 - val_case_acc: 0.7871 - val_degree_acc: 0.8844 - val_strength_acc: 0.9194 - val_inflection_acc: 0.9380\n",
      "Epoch 31/100\n",
      "2426/2426 [==============================] - 21s 9ms/step - loss: 2.9111 - pos_loss: 0.3521 - person_loss: 0.1574 - number_loss: 0.3281 - tense_loss: 0.1091 - mood_loss: 0.1502 - voice_loss: 3.3035e-06 - gender_loss: 0.4703 - case_loss: 0.4763 - degree_loss: 0.1613 - strength_loss: 0.1320 - inflection_loss: 0.0940 - pos_acc: 0.8890 - person_acc: 0.9482 - number_acc: 0.8800 - tense_acc: 0.9623 - mood_acc: 0.9517 - voice_acc: 1.0000 - gender_acc: 0.8287 - case_acc: 0.8259 - degree_acc: 0.9465 - strength_acc: 0.9567 - inflection_acc: 0.9660 - val_loss: 4.0117 - val_pos_loss: 0.5884 - val_person_loss: 0.1128 - val_number_loss: 0.4842 - val_tense_loss: 0.1008 - val_mood_loss: 0.1570 - val_voice_loss: 2.7302e-06 - val_gender_loss: 0.6658 - val_case_loss: 0.5831 - val_degree_loss: 0.3972 - val_strength_loss: 0.2724 - val_inflection_loss: 0.1826 - val_pos_acc: 0.8380 - val_person_acc: 0.9661 - val_number_acc: 0.8345 - val_tense_acc: 0.9669 - val_mood_acc: 0.9480 - val_voice_acc: 1.0000 - val_gender_acc: 0.7668 - val_case_acc: 0.7970 - val_degree_acc: 0.8881 - val_strength_acc: 0.9202 - val_inflection_acc: 0.9430\n",
      "Epoch 32/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.8250 - pos_loss: 0.3427 - person_loss: 0.1492 - number_loss: 0.3143 - tense_loss: 0.1041 - mood_loss: 0.1446 - voice_loss: 3.4863e-06 - gender_loss: 0.4539 - case_loss: 0.4609 - degree_loss: 0.1587 - strength_loss: 0.1272 - inflection_loss: 0.0882 - pos_acc: 0.8925 - person_acc: 0.9493 - number_acc: 0.8834 - tense_acc: 0.9639 - mood_acc: 0.9528 - voice_acc: 1.0000 - gender_acc: 0.8333 - case_acc: 0.8285 - degree_acc: 0.9498 - strength_acc: 0.9578 - inflection_acc: 0.9676 - val_loss: 4.0504 - val_pos_loss: 0.5919 - val_person_loss: 0.1429 - val_number_loss: 0.4881 - val_tense_loss: 0.1085 - val_mood_loss: 0.1601 - val_voice_loss: 2.9363e-06 - val_gender_loss: 0.6696 - val_case_loss: 0.5787 - val_degree_loss: 0.3944 - val_strength_loss: 0.2833 - val_inflection_loss: 0.1781 - val_pos_acc: 0.8456 - val_person_acc: 0.9586 - val_number_acc: 0.8381 - val_tense_acc: 0.9657 - val_mood_acc: 0.9526 - val_voice_acc: 1.0000 - val_gender_acc: 0.7736 - val_case_acc: 0.7866 - val_degree_acc: 0.8913 - val_strength_acc: 0.9230 - val_inflection_acc: 0.9462\n",
      "Epoch 33/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.8195 - pos_loss: 0.3359 - person_loss: 0.1498 - number_loss: 0.3188 - tense_loss: 0.1036 - mood_loss: 0.1424 - voice_loss: 3.3939e-06 - gender_loss: 0.4528 - case_loss: 0.4595 - degree_loss: 0.1555 - strength_loss: 0.1268 - inflection_loss: 0.0939 - pos_acc: 0.8965 - person_acc: 0.9496 - number_acc: 0.8796 - tense_acc: 0.9647 - mood_acc: 0.9541 - voice_acc: 1.0000 - gender_acc: 0.8347 - case_acc: 0.8285 - degree_acc: 0.9467 - strength_acc: 0.9576 - inflection_acc: 0.9658 - val_loss: 4.0318 - val_pos_loss: 0.5997 - val_person_loss: 0.1136 - val_number_loss: 0.4809 - val_tense_loss: 0.0902 - val_mood_loss: 0.1541 - val_voice_loss: 2.2531e-06 - val_gender_loss: 0.6903 - val_case_loss: 0.5823 - val_degree_loss: 0.3936 - val_strength_loss: 0.2770 - val_inflection_loss: 0.1803 - val_pos_acc: 0.8372 - val_person_acc: 0.9645 - val_number_acc: 0.8456 - val_tense_acc: 0.9685 - val_mood_acc: 0.9512 - val_voice_acc: 1.0000 - val_gender_acc: 0.7721 - val_case_acc: 0.7968 - val_degree_acc: 0.8937 - val_strength_acc: 0.9265 - val_inflection_acc: 0.9457\n",
      "Epoch 34/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.8006 - pos_loss: 0.3310 - person_loss: 0.1475 - number_loss: 0.3152 - tense_loss: 0.1051 - mood_loss: 0.1409 - voice_loss: 3.0162e-06 - gender_loss: 0.4521 - case_loss: 0.4576 - degree_loss: 0.1519 - strength_loss: 0.1206 - inflection_loss: 0.0925 - pos_acc: 0.8978 - person_acc: 0.9508 - number_acc: 0.8832 - tense_acc: 0.9629 - mood_acc: 0.9549 - voice_acc: 1.0000 - gender_acc: 0.8366 - case_acc: 0.8291 - degree_acc: 0.9488 - strength_acc: 0.9593 - inflection_acc: 0.9656 - val_loss: 3.9367 - val_pos_loss: 0.5833 - val_person_loss: 0.0917 - val_number_loss: 0.4696 - val_tense_loss: 0.0847 - val_mood_loss: 0.1326 - val_voice_loss: 1.8648e-06 - val_gender_loss: 0.6698 - val_case_loss: 0.5883 - val_degree_loss: 0.3954 - val_strength_loss: 0.2602 - val_inflection_loss: 0.1932 - val_pos_acc: 0.8325 - val_person_acc: 0.9725 - val_number_acc: 0.8485 - val_tense_acc: 0.9720 - val_mood_acc: 0.9520 - val_voice_acc: 1.0000 - val_gender_acc: 0.7675 - val_case_acc: 0.7877 - val_degree_acc: 0.8903 - val_strength_acc: 0.9258 - val_inflection_acc: 0.9414\n",
      "Epoch 35/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.7682 - pos_loss: 0.3291 - person_loss: 0.1475 - number_loss: 0.3069 - tense_loss: 0.1020 - mood_loss: 0.1393 - voice_loss: 2.9902e-06 - gender_loss: 0.4452 - case_loss: 0.4537 - degree_loss: 0.1528 - strength_loss: 0.1240 - inflection_loss: 0.0886 - pos_acc: 0.8964 - person_acc: 0.9502 - number_acc: 0.8856 - tense_acc: 0.9646 - mood_acc: 0.9540 - voice_acc: 1.0000 - gender_acc: 0.8370 - case_acc: 0.8326 - degree_acc: 0.9502 - strength_acc: 0.9589 - inflection_acc: 0.9673 - val_loss: 3.9939 - val_pos_loss: 0.6091 - val_person_loss: 0.1058 - val_number_loss: 0.4720 - val_tense_loss: 0.0938 - val_mood_loss: 0.1441 - val_voice_loss: 1.7758e-06 - val_gender_loss: 0.6705 - val_case_loss: 0.5785 - val_degree_loss: 0.3977 - val_strength_loss: 0.2757 - val_inflection_loss: 0.1820 - val_pos_acc: 0.8367 - val_person_acc: 0.9699 - val_number_acc: 0.8496 - val_tense_acc: 0.9666 - val_mood_acc: 0.9497 - val_voice_acc: 1.0000 - val_gender_acc: 0.7794 - val_case_acc: 0.7894 - val_degree_acc: 0.8905 - val_strength_acc: 0.9252 - val_inflection_acc: 0.9412\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.7011 - pos_loss: 0.3157 - person_loss: 0.1423 - number_loss: 0.2969 - tense_loss: 0.0982 - mood_loss: 0.1332 - voice_loss: 2.7306e-06 - gender_loss: 0.4397 - case_loss: 0.4445 - degree_loss: 0.1485 - strength_loss: 0.1166 - inflection_loss: 0.0860 - pos_acc: 0.9030 - person_acc: 0.9525 - number_acc: 0.8891 - tense_acc: 0.9664 - mood_acc: 0.9574 - voice_acc: 1.0000 - gender_acc: 0.8408 - case_acc: 0.8343 - degree_acc: 0.9509 - strength_acc: 0.9613 - inflection_acc: 0.9671 - val_loss: 4.0869 - val_pos_loss: 0.6288 - val_person_loss: 0.0925 - val_number_loss: 0.4810 - val_tense_loss: 0.0919 - val_mood_loss: 0.1443 - val_voice_loss: 1.2486e-06 - val_gender_loss: 0.6899 - val_case_loss: 0.5912 - val_degree_loss: 0.4189 - val_strength_loss: 0.2890 - val_inflection_loss: 0.1959 - val_pos_acc: 0.8320 - val_person_acc: 0.9729 - val_number_acc: 0.8594 - val_tense_acc: 0.9710 - val_mood_acc: 0.9527 - val_voice_acc: 1.0000 - val_gender_acc: 0.7667 - val_case_acc: 0.7904 - val_degree_acc: 0.8912 - val_strength_acc: 0.9245 - val_inflection_acc: 0.9419\n",
      "Epoch 37/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.7059 - pos_loss: 0.3150 - person_loss: 0.1454 - number_loss: 0.2998 - tense_loss: 0.1012 - mood_loss: 0.1346 - voice_loss: 2.8455e-06 - gender_loss: 0.4357 - case_loss: 0.4400 - degree_loss: 0.1476 - strength_loss: 0.1155 - inflection_loss: 0.0878 - pos_acc: 0.9011 - person_acc: 0.9505 - number_acc: 0.8878 - tense_acc: 0.9641 - mood_acc: 0.9557 - voice_acc: 1.0000 - gender_acc: 0.8417 - case_acc: 0.8354 - degree_acc: 0.9515 - strength_acc: 0.9613 - inflection_acc: 0.9677 - val_loss: 3.8339 - val_pos_loss: 0.5683 - val_person_loss: 0.1006 - val_number_loss: 0.4407 - val_tense_loss: 0.0861 - val_mood_loss: 0.1306 - val_voice_loss: 1.5520e-06 - val_gender_loss: 0.6520 - val_case_loss: 0.5508 - val_degree_loss: 0.3898 - val_strength_loss: 0.2650 - val_inflection_loss: 0.1768 - val_pos_acc: 0.8458 - val_person_acc: 0.9686 - val_number_acc: 0.8559 - val_tense_acc: 0.9714 - val_mood_acc: 0.9572 - val_voice_acc: 1.0000 - val_gender_acc: 0.7802 - val_case_acc: 0.8004 - val_degree_acc: 0.8921 - val_strength_acc: 0.9265 - val_inflection_acc: 0.9454\n",
      "Epoch 38/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.6948 - pos_loss: 0.3133 - person_loss: 0.1377 - number_loss: 0.2982 - tense_loss: 0.0963 - mood_loss: 0.1314 - voice_loss: 2.5977e-06 - gender_loss: 0.4344 - case_loss: 0.4437 - degree_loss: 0.1484 - strength_loss: 0.1171 - inflection_loss: 0.0892 - pos_acc: 0.9009 - person_acc: 0.9526 - number_acc: 0.8883 - tense_acc: 0.9667 - mood_acc: 0.9564 - voice_acc: 1.0000 - gender_acc: 0.8405 - case_acc: 0.8333 - degree_acc: 0.9508 - strength_acc: 0.9618 - inflection_acc: 0.9672 - val_loss: 4.0733 - val_pos_loss: 0.6139 - val_person_loss: 0.1050 - val_number_loss: 0.4820 - val_tense_loss: 0.1014 - val_mood_loss: 0.1522 - val_voice_loss: 1.5295e-06 - val_gender_loss: 0.6842 - val_case_loss: 0.5697 - val_degree_loss: 0.3942 - val_strength_loss: 0.2708 - val_inflection_loss: 0.2117 - val_pos_acc: 0.8350 - val_person_acc: 0.9705 - val_number_acc: 0.8481 - val_tense_acc: 0.9672 - val_mood_acc: 0.9535 - val_voice_acc: 1.0000 - val_gender_acc: 0.7704 - val_case_acc: 0.8041 - val_degree_acc: 0.8975 - val_strength_acc: 0.9321 - val_inflection_acc: 0.9388\n",
      "Epoch 39/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.6639 - pos_loss: 0.3099 - person_loss: 0.1407 - number_loss: 0.2921 - tense_loss: 0.0989 - mood_loss: 0.1293 - voice_loss: 2.5859e-06 - gender_loss: 0.4294 - case_loss: 0.4328 - degree_loss: 0.1448 - strength_loss: 0.1163 - inflection_loss: 0.0861 - pos_acc: 0.9034 - person_acc: 0.9532 - number_acc: 0.8926 - tense_acc: 0.9662 - mood_acc: 0.9581 - voice_acc: 1.0000 - gender_acc: 0.8453 - case_acc: 0.8394 - degree_acc: 0.9532 - strength_acc: 0.9623 - inflection_acc: 0.9677 - val_loss: 3.9325 - val_pos_loss: 0.5779 - val_person_loss: 0.1080 - val_number_loss: 0.4657 - val_tense_loss: 0.0899 - val_mood_loss: 0.1439 - val_voice_loss: 1.6833e-06 - val_gender_loss: 0.6607 - val_case_loss: 0.5642 - val_degree_loss: 0.3962 - val_strength_loss: 0.2740 - val_inflection_loss: 0.1814 - val_pos_acc: 0.8497 - val_person_acc: 0.9645 - val_number_acc: 0.8571 - val_tense_acc: 0.9689 - val_mood_acc: 0.9501 - val_voice_acc: 1.0000 - val_gender_acc: 0.7799 - val_case_acc: 0.8008 - val_degree_acc: 0.8958 - val_strength_acc: 0.9279 - val_inflection_acc: 0.9452\n",
      "Epoch 40/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.6220 - pos_loss: 0.3035 - person_loss: 0.1363 - number_loss: 0.2891 - tense_loss: 0.0927 - mood_loss: 0.1293 - voice_loss: 2.2036e-06 - gender_loss: 0.4225 - case_loss: 0.4280 - degree_loss: 0.1436 - strength_loss: 0.1148 - inflection_loss: 0.0832 - pos_acc: 0.9056 - person_acc: 0.9551 - number_acc: 0.8924 - tense_acc: 0.9669 - mood_acc: 0.9578 - voice_acc: 1.0000 - gender_acc: 0.8468 - case_acc: 0.8397 - degree_acc: 0.9523 - strength_acc: 0.9628 - inflection_acc: 0.9700 - val_loss: 4.0846 - val_pos_loss: 0.5984 - val_person_loss: 0.1020 - val_number_loss: 0.4980 - val_tense_loss: 0.0906 - val_mood_loss: 0.1597 - val_voice_loss: 8.9208e-07 - val_gender_loss: 0.6915 - val_case_loss: 0.5904 - val_degree_loss: 0.4033 - val_strength_loss: 0.2748 - val_inflection_loss: 0.2118 - val_pos_acc: 0.8384 - val_person_acc: 0.9692 - val_number_acc: 0.8471 - val_tense_acc: 0.9676 - val_mood_acc: 0.9464 - val_voice_acc: 1.0000 - val_gender_acc: 0.7776 - val_case_acc: 0.7960 - val_degree_acc: 0.8983 - val_strength_acc: 0.9218 - val_inflection_acc: 0.9346\n",
      "Epoch 41/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.5695 - pos_loss: 0.2925 - person_loss: 0.1355 - number_loss: 0.2817 - tense_loss: 0.0887 - mood_loss: 0.1243 - voice_loss: 1.7227e-06 - gender_loss: 0.4234 - case_loss: 0.4224 - degree_loss: 0.1414 - strength_loss: 0.1091 - inflection_loss: 0.0842 - pos_acc: 0.9075 - person_acc: 0.9538 - number_acc: 0.8960 - tense_acc: 0.9698 - mood_acc: 0.9599 - voice_acc: 1.0000 - gender_acc: 0.8470 - case_acc: 0.8397 - degree_acc: 0.9534 - strength_acc: 0.9637 - inflection_acc: 0.9685 - val_loss: 3.9699 - val_pos_loss: 0.6001 - val_person_loss: 0.1141 - val_number_loss: 0.4566 - val_tense_loss: 0.1015 - val_mood_loss: 0.1529 - val_voice_loss: 1.1462e-06 - val_gender_loss: 0.6523 - val_case_loss: 0.5532 - val_degree_loss: 0.4230 - val_strength_loss: 0.2757 - val_inflection_loss: 0.1838 - val_pos_acc: 0.8373 - val_person_acc: 0.9639 - val_number_acc: 0.8613 - val_tense_acc: 0.9669 - val_mood_acc: 0.9462 - val_voice_acc: 1.0000 - val_gender_acc: 0.7895 - val_case_acc: 0.8098 - val_degree_acc: 0.8912 - val_strength_acc: 0.9222 - val_inflection_acc: 0.9442\n",
      "Epoch 42/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.5689 - pos_loss: 0.2921 - person_loss: 0.1340 - number_loss: 0.2810 - tense_loss: 0.0920 - mood_loss: 0.1251 - voice_loss: 1.6442e-06 - gender_loss: 0.4141 - case_loss: 0.4251 - degree_loss: 0.1407 - strength_loss: 0.1077 - inflection_loss: 0.0829 - pos_acc: 0.9070 - person_acc: 0.9554 - number_acc: 0.8956 - tense_acc: 0.9682 - mood_acc: 0.9582 - voice_acc: 1.0000 - gender_acc: 0.8504 - case_acc: 0.8399 - degree_acc: 0.9535 - strength_acc: 0.9652 - inflection_acc: 0.9699 - val_loss: 4.1938 - val_pos_loss: 0.6289 - val_person_loss: 0.0934 - val_number_loss: 0.4772 - val_tense_loss: 0.0980 - val_mood_loss: 0.1518 - val_voice_loss: 1.0199e-06 - val_gender_loss: 0.6962 - val_case_loss: 0.6135 - val_degree_loss: 0.4451 - val_strength_loss: 0.2969 - val_inflection_loss: 0.2269 - val_pos_acc: 0.8381 - val_person_acc: 0.9697 - val_number_acc: 0.8525 - val_tense_acc: 0.9653 - val_mood_acc: 0.9493 - val_voice_acc: 1.0000 - val_gender_acc: 0.7661 - val_case_acc: 0.7842 - val_degree_acc: 0.8851 - val_strength_acc: 0.9277 - val_inflection_acc: 0.9265\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.5495 - pos_loss: 0.2871 - person_loss: 0.1353 - number_loss: 0.2804 - tense_loss: 0.0923 - mood_loss: 0.1234 - voice_loss: 1.8551e-06 - gender_loss: 0.4075 - case_loss: 0.4135 - degree_loss: 0.1375 - strength_loss: 0.1063 - inflection_loss: 0.0845 - pos_acc: 0.9092 - person_acc: 0.9536 - number_acc: 0.8956 - tense_acc: 0.9683 - mood_acc: 0.9585 - voice_acc: 1.0000 - gender_acc: 0.8517 - case_acc: 0.8443 - degree_acc: 0.9547 - strength_acc: 0.9657 - inflection_acc: 0.9690 - val_loss: 3.8867 - val_pos_loss: 0.5879 - val_person_loss: 0.0969 - val_number_loss: 0.4483 - val_tense_loss: 0.0945 - val_mood_loss: 0.1400 - val_voice_loss: 9.3037e-07 - val_gender_loss: 0.6660 - val_case_loss: 0.5616 - val_degree_loss: 0.3878 - val_strength_loss: 0.2569 - val_inflection_loss: 0.1913 - val_pos_acc: 0.8491 - val_person_acc: 0.9710 - val_number_acc: 0.8695 - val_tense_acc: 0.9697 - val_mood_acc: 0.9500 - val_voice_acc: 1.0000 - val_gender_acc: 0.7830 - val_case_acc: 0.8070 - val_degree_acc: 0.9017 - val_strength_acc: 0.9325 - val_inflection_acc: 0.9414\n",
      "Epoch 44/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.5588 - pos_loss: 0.2933 - person_loss: 0.1272 - number_loss: 0.2788 - tense_loss: 0.0895 - mood_loss: 0.1217 - voice_loss: 1.3797e-06 - gender_loss: 0.4114 - case_loss: 0.4181 - degree_loss: 0.1400 - strength_loss: 0.1091 - inflection_loss: 0.0841 - pos_acc: 0.9103 - person_acc: 0.9572 - number_acc: 0.8963 - tense_acc: 0.9685 - mood_acc: 0.9599 - voice_acc: 1.0000 - gender_acc: 0.8518 - case_acc: 0.8434 - degree_acc: 0.9540 - strength_acc: 0.9652 - inflection_acc: 0.9690 - val_loss: 3.7506 - val_pos_loss: 0.5728 - val_person_loss: 0.0937 - val_number_loss: 0.4357 - val_tense_loss: 0.0914 - val_mood_loss: 0.1448 - val_voice_loss: 1.1447e-06 - val_gender_loss: 0.6252 - val_case_loss: 0.5308 - val_degree_loss: 0.3653 - val_strength_loss: 0.2462 - val_inflection_loss: 0.1825 - val_pos_acc: 0.8527 - val_person_acc: 0.9703 - val_number_acc: 0.8690 - val_tense_acc: 0.9677 - val_mood_acc: 0.9494 - val_voice_acc: 1.0000 - val_gender_acc: 0.7904 - val_case_acc: 0.8103 - val_degree_acc: 0.9066 - val_strength_acc: 0.9376 - val_inflection_acc: 0.9444\n",
      "Epoch 45/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.4798 - pos_loss: 0.2811 - person_loss: 0.1264 - number_loss: 0.2724 - tense_loss: 0.0858 - mood_loss: 0.1166 - voice_loss: 1.3417e-06 - gender_loss: 0.3982 - case_loss: 0.4040 - degree_loss: 0.1368 - strength_loss: 0.1038 - inflection_loss: 0.0803 - pos_acc: 0.9108 - person_acc: 0.9571 - number_acc: 0.8983 - tense_acc: 0.9705 - mood_acc: 0.9609 - voice_acc: 1.0000 - gender_acc: 0.8551 - case_acc: 0.8470 - degree_acc: 0.9541 - strength_acc: 0.9649 - inflection_acc: 0.9695 - val_loss: 3.6421 - val_pos_loss: 0.5336 - val_person_loss: 0.0951 - val_number_loss: 0.4336 - val_tense_loss: 0.0808 - val_mood_loss: 0.1247 - val_voice_loss: 1.2345e-06 - val_gender_loss: 0.6215 - val_case_loss: 0.5319 - val_degree_loss: 0.3526 - val_strength_loss: 0.2322 - val_inflection_loss: 0.1773 - val_pos_acc: 0.8616 - val_person_acc: 0.9694 - val_number_acc: 0.8650 - val_tense_acc: 0.9722 - val_mood_acc: 0.9556 - val_voice_acc: 1.0000 - val_gender_acc: 0.7922 - val_case_acc: 0.8172 - val_degree_acc: 0.9085 - val_strength_acc: 0.9391 - val_inflection_acc: 0.9470\n",
      "Epoch 46/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.4411 - pos_loss: 0.2702 - person_loss: 0.1276 - number_loss: 0.2714 - tense_loss: 0.0849 - mood_loss: 0.1142 - voice_loss: 1.4274e-06 - gender_loss: 0.3927 - case_loss: 0.3993 - degree_loss: 0.1323 - strength_loss: 0.1003 - inflection_loss: 0.0799 - pos_acc: 0.9140 - person_acc: 0.9576 - number_acc: 0.9019 - tense_acc: 0.9704 - mood_acc: 0.9625 - voice_acc: 1.0000 - gender_acc: 0.8590 - case_acc: 0.8492 - degree_acc: 0.9567 - strength_acc: 0.9676 - inflection_acc: 0.9709 - val_loss: 3.7388 - val_pos_loss: 0.5569 - val_person_loss: 0.0869 - val_number_loss: 0.4506 - val_tense_loss: 0.0821 - val_mood_loss: 0.1243 - val_voice_loss: 8.8881e-07 - val_gender_loss: 0.6377 - val_case_loss: 0.5438 - val_degree_loss: 0.3619 - val_strength_loss: 0.2365 - val_inflection_loss: 0.1933 - val_pos_acc: 0.8589 - val_person_acc: 0.9744 - val_number_acc: 0.8623 - val_tense_acc: 0.9690 - val_mood_acc: 0.9635 - val_voice_acc: 1.0000 - val_gender_acc: 0.7810 - val_case_acc: 0.8129 - val_degree_acc: 0.9088 - val_strength_acc: 0.9376 - val_inflection_acc: 0.9392\n",
      "Epoch 47/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.4734 - pos_loss: 0.2766 - person_loss: 0.1290 - number_loss: 0.2684 - tense_loss: 0.0884 - mood_loss: 0.1194 - voice_loss: 1.4319e-06 - gender_loss: 0.4005 - case_loss: 0.4045 - degree_loss: 0.1323 - strength_loss: 0.1002 - inflection_loss: 0.0786 - pos_acc: 0.9122 - person_acc: 0.9561 - number_acc: 0.9020 - tense_acc: 0.9692 - mood_acc: 0.9609 - voice_acc: 1.0000 - gender_acc: 0.8555 - case_acc: 0.8496 - degree_acc: 0.9559 - strength_acc: 0.9673 - inflection_acc: 0.9716 - val_loss: 3.7645 - val_pos_loss: 0.5541 - val_person_loss: 0.0910 - val_number_loss: 0.4404 - val_tense_loss: 0.0826 - val_mood_loss: 0.1289 - val_voice_loss: 4.8015e-07 - val_gender_loss: 0.6373 - val_case_loss: 0.5456 - val_degree_loss: 0.3772 - val_strength_loss: 0.2503 - val_inflection_loss: 0.1866 - val_pos_acc: 0.8530 - val_person_acc: 0.9739 - val_number_acc: 0.8611 - val_tense_acc: 0.9709 - val_mood_acc: 0.9593 - val_voice_acc: 1.0000 - val_gender_acc: 0.7942 - val_case_acc: 0.8145 - val_degree_acc: 0.9004 - val_strength_acc: 0.9351 - val_inflection_acc: 0.9458\n",
      "Epoch 48/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.4402 - pos_loss: 0.2720 - person_loss: 0.1247 - number_loss: 0.2639 - tense_loss: 0.0825 - mood_loss: 0.1143 - voice_loss: 1.4386e-06 - gender_loss: 0.3963 - case_loss: 0.3977 - degree_loss: 0.1337 - strength_loss: 0.1003 - inflection_loss: 0.0772 - pos_acc: 0.9156 - person_acc: 0.9580 - number_acc: 0.9047 - tense_acc: 0.9704 - mood_acc: 0.9612 - voice_acc: 1.0000 - gender_acc: 0.8594 - case_acc: 0.8517 - degree_acc: 0.9571 - strength_acc: 0.9678 - inflection_acc: 0.9716 - val_loss: 3.7426 - val_pos_loss: 0.5687 - val_person_loss: 0.0911 - val_number_loss: 0.4330 - val_tense_loss: 0.0835 - val_mood_loss: 0.1269 - val_voice_loss: 7.1917e-07 - val_gender_loss: 0.6436 - val_case_loss: 0.5360 - val_degree_loss: 0.3762 - val_strength_loss: 0.2565 - val_inflection_loss: 0.1749 - val_pos_acc: 0.8537 - val_person_acc: 0.9744 - val_number_acc: 0.8780 - val_tense_acc: 0.9738 - val_mood_acc: 0.9581 - val_voice_acc: 1.0000 - val_gender_acc: 0.7809 - val_case_acc: 0.8105 - val_degree_acc: 0.9040 - val_strength_acc: 0.9332 - val_inflection_acc: 0.9510\n",
      "Epoch 49/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3921 - pos_loss: 0.2614 - person_loss: 0.1219 - number_loss: 0.2628 - tense_loss: 0.0786 - mood_loss: 0.1112 - voice_loss: 1.3900e-06 - gender_loss: 0.3917 - case_loss: 0.3918 - degree_loss: 0.1295 - strength_loss: 0.0970 - inflection_loss: 0.0750 - pos_acc: 0.9158 - person_acc: 0.9584 - number_acc: 0.9027 - tense_acc: 0.9725 - mood_acc: 0.9633 - voice_acc: 1.0000 - gender_acc: 0.8566 - case_acc: 0.8506 - degree_acc: 0.9568 - strength_acc: 0.9680 - inflection_acc: 0.9717 - val_loss: 3.6472 - val_pos_loss: 0.5278 - val_person_loss: 0.0969 - val_number_loss: 0.4231 - val_tense_loss: 0.0849 - val_mood_loss: 0.1346 - val_voice_loss: 8.3447e-07 - val_gender_loss: 0.6202 - val_case_loss: 0.5287 - val_degree_loss: 0.3610 - val_strength_loss: 0.2312 - val_inflection_loss: 0.1734 - val_pos_acc: 0.8591 - val_person_acc: 0.9695 - val_number_acc: 0.8699 - val_tense_acc: 0.9704 - val_mood_acc: 0.9595 - val_voice_acc: 1.0000 - val_gender_acc: 0.7851 - val_case_acc: 0.8040 - val_degree_acc: 0.9055 - val_strength_acc: 0.9382 - val_inflection_acc: 0.9490\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3987 - pos_loss: 0.2638 - person_loss: 0.1235 - number_loss: 0.2627 - tense_loss: 0.0797 - mood_loss: 0.1108 - voice_loss: 1.2630e-06 - gender_loss: 0.3889 - case_loss: 0.3926 - degree_loss: 0.1287 - strength_loss: 0.0980 - inflection_loss: 0.0782 - pos_acc: 0.9170 - person_acc: 0.9600 - number_acc: 0.9043 - tense_acc: 0.9716 - mood_acc: 0.9640 - voice_acc: 1.0000 - gender_acc: 0.8602 - case_acc: 0.8535 - degree_acc: 0.9582 - strength_acc: 0.9681 - inflection_acc: 0.9716 - val_loss: 3.7247 - val_pos_loss: 0.5389 - val_person_loss: 0.0921 - val_number_loss: 0.4371 - val_tense_loss: 0.0927 - val_mood_loss: 0.1296 - val_voice_loss: 5.5618e-07 - val_gender_loss: 0.6509 - val_case_loss: 0.5339 - val_degree_loss: 0.3639 - val_strength_loss: 0.2498 - val_inflection_loss: 0.1782 - val_pos_acc: 0.8589 - val_person_acc: 0.9685 - val_number_acc: 0.8660 - val_tense_acc: 0.9680 - val_mood_acc: 0.9527 - val_voice_acc: 1.0000 - val_gender_acc: 0.7905 - val_case_acc: 0.8164 - val_degree_acc: 0.9007 - val_strength_acc: 0.9377 - val_inflection_acc: 0.9470\n",
      "Epoch 51/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3879 - pos_loss: 0.2601 - person_loss: 0.1218 - number_loss: 0.2590 - tense_loss: 0.0786 - mood_loss: 0.1121 - voice_loss: 1.2527e-06 - gender_loss: 0.3885 - case_loss: 0.3878 - degree_loss: 0.1295 - strength_loss: 0.0961 - inflection_loss: 0.0771 - pos_acc: 0.9176 - person_acc: 0.9589 - number_acc: 0.9054 - tense_acc: 0.9727 - mood_acc: 0.9630 - voice_acc: 1.0000 - gender_acc: 0.8602 - case_acc: 0.8542 - degree_acc: 0.9575 - strength_acc: 0.9688 - inflection_acc: 0.9722 - val_loss: 3.7780 - val_pos_loss: 0.5553 - val_person_loss: 0.0898 - val_number_loss: 0.4554 - val_tense_loss: 0.0806 - val_mood_loss: 0.1219 - val_voice_loss: 5.8727e-07 - val_gender_loss: 0.6494 - val_case_loss: 0.5400 - val_degree_loss: 0.3825 - val_strength_loss: 0.2546 - val_inflection_loss: 0.1934 - val_pos_acc: 0.8640 - val_person_acc: 0.9745 - val_number_acc: 0.8694 - val_tense_acc: 0.9718 - val_mood_acc: 0.9622 - val_voice_acc: 1.0000 - val_gender_acc: 0.7850 - val_case_acc: 0.8138 - val_degree_acc: 0.8990 - val_strength_acc: 0.9316 - val_inflection_acc: 0.9355\n",
      "Epoch 52/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3470 - pos_loss: 0.2587 - person_loss: 0.1198 - number_loss: 0.2569 - tense_loss: 0.0800 - mood_loss: 0.1091 - voice_loss: 1.1166e-06 - gender_loss: 0.3820 - case_loss: 0.3853 - degree_loss: 0.1266 - strength_loss: 0.0953 - inflection_loss: 0.0758 - pos_acc: 0.9197 - person_acc: 0.9607 - number_acc: 0.9075 - tense_acc: 0.9726 - mood_acc: 0.9652 - voice_acc: 1.0000 - gender_acc: 0.8628 - case_acc: 0.8567 - degree_acc: 0.9591 - strength_acc: 0.9698 - inflection_acc: 0.9729 - val_loss: 3.8160 - val_pos_loss: 0.5591 - val_person_loss: 0.0956 - val_number_loss: 0.4631 - val_tense_loss: 0.0898 - val_mood_loss: 0.1434 - val_voice_loss: 4.8294e-07 - val_gender_loss: 0.6422 - val_case_loss: 0.5454 - val_degree_loss: 0.3755 - val_strength_loss: 0.2489 - val_inflection_loss: 0.2095 - val_pos_acc: 0.8515 - val_person_acc: 0.9698 - val_number_acc: 0.8618 - val_tense_acc: 0.9670 - val_mood_acc: 0.9482 - val_voice_acc: 1.0000 - val_gender_acc: 0.7998 - val_case_acc: 0.8118 - val_degree_acc: 0.9071 - val_strength_acc: 0.9355 - val_inflection_acc: 0.9380\n",
      "Epoch 53/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3137 - pos_loss: 0.2508 - person_loss: 0.1156 - number_loss: 0.2532 - tense_loss: 0.0772 - mood_loss: 0.1045 - voice_loss: 9.8068e-07 - gender_loss: 0.3774 - case_loss: 0.3784 - degree_loss: 0.1262 - strength_loss: 0.0957 - inflection_loss: 0.0723 - pos_acc: 0.9215 - person_acc: 0.9608 - number_acc: 0.9067 - tense_acc: 0.9737 - mood_acc: 0.9648 - voice_acc: 1.0000 - gender_acc: 0.8647 - case_acc: 0.8593 - degree_acc: 0.9588 - strength_acc: 0.9689 - inflection_acc: 0.9734 - val_loss: 3.6953 - val_pos_loss: 0.5428 - val_person_loss: 0.0785 - val_number_loss: 0.4365 - val_tense_loss: 0.0769 - val_mood_loss: 0.1179 - val_voice_loss: 5.6775e-07 - val_gender_loss: 0.6503 - val_case_loss: 0.5452 - val_degree_loss: 0.3651 - val_strength_loss: 0.2395 - val_inflection_loss: 0.1936 - val_pos_acc: 0.8671 - val_person_acc: 0.9787 - val_number_acc: 0.8785 - val_tense_acc: 0.9747 - val_mood_acc: 0.9632 - val_voice_acc: 1.0000 - val_gender_acc: 0.7944 - val_case_acc: 0.8254 - val_degree_acc: 0.9063 - val_strength_acc: 0.9427 - val_inflection_acc: 0.9493\n",
      "Epoch 54/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.3298 - pos_loss: 0.2556 - person_loss: 0.1198 - number_loss: 0.2569 - tense_loss: 0.0789 - mood_loss: 0.1068 - voice_loss: 1.3070e-06 - gender_loss: 0.3818 - case_loss: 0.3806 - degree_loss: 0.1250 - strength_loss: 0.0934 - inflection_loss: 0.0731 - pos_acc: 0.9200 - person_acc: 0.9602 - number_acc: 0.9067 - tense_acc: 0.9729 - mood_acc: 0.9654 - voice_acc: 1.0000 - gender_acc: 0.8634 - case_acc: 0.8594 - degree_acc: 0.9597 - strength_acc: 0.9689 - inflection_acc: 0.9740 - val_loss: 3.7076 - val_pos_loss: 0.5263 - val_person_loss: 0.0758 - val_number_loss: 0.4463 - val_tense_loss: 0.0750 - val_mood_loss: 0.1182 - val_voice_loss: 9.5082e-07 - val_gender_loss: 0.6560 - val_case_loss: 0.5700 - val_degree_loss: 0.3547 - val_strength_loss: 0.2402 - val_inflection_loss: 0.1891 - val_pos_acc: 0.8752 - val_person_acc: 0.9762 - val_number_acc: 0.8720 - val_tense_acc: 0.9762 - val_mood_acc: 0.9621 - val_voice_acc: 1.0000 - val_gender_acc: 0.7903 - val_case_acc: 0.8146 - val_degree_acc: 0.9078 - val_strength_acc: 0.9363 - val_inflection_acc: 0.9463\n",
      "Epoch 55/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2789 - pos_loss: 0.2479 - person_loss: 0.1155 - number_loss: 0.2472 - tense_loss: 0.0766 - mood_loss: 0.1050 - voice_loss: 1.0452e-06 - gender_loss: 0.3679 - case_loss: 0.3725 - degree_loss: 0.1205 - strength_loss: 0.0889 - inflection_loss: 0.0719 - pos_acc: 0.9205 - person_acc: 0.9622 - number_acc: 0.9093 - tense_acc: 0.9731 - mood_acc: 0.9661 - voice_acc: 1.0000 - gender_acc: 0.8660 - case_acc: 0.8601 - degree_acc: 0.9605 - strength_acc: 0.9705 - inflection_acc: 0.9744 - val_loss: 3.6161 - val_pos_loss: 0.5055 - val_person_loss: 0.0700 - val_number_loss: 0.4399 - val_tense_loss: 0.0675 - val_mood_loss: 0.1085 - val_voice_loss: 3.5048e-07 - val_gender_loss: 0.6445 - val_case_loss: 0.5530 - val_degree_loss: 0.3493 - val_strength_loss: 0.2488 - val_inflection_loss: 0.1866 - val_pos_acc: 0.8748 - val_person_acc: 0.9774 - val_number_acc: 0.8691 - val_tense_acc: 0.9756 - val_mood_acc: 0.9630 - val_voice_acc: 1.0000 - val_gender_acc: 0.7924 - val_case_acc: 0.8145 - val_degree_acc: 0.9033 - val_strength_acc: 0.9306 - val_inflection_acc: 0.9467\n",
      "Epoch 56/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2741 - pos_loss: 0.2440 - person_loss: 0.1144 - number_loss: 0.2483 - tense_loss: 0.0737 - mood_loss: 0.1023 - voice_loss: 8.8240e-07 - gender_loss: 0.3675 - case_loss: 0.3724 - degree_loss: 0.1224 - strength_loss: 0.0941 - inflection_loss: 0.0725 - pos_acc: 0.9237 - person_acc: 0.9622 - number_acc: 0.9087 - tense_acc: 0.9743 - mood_acc: 0.9649 - voice_acc: 1.0000 - gender_acc: 0.8682 - case_acc: 0.8603 - degree_acc: 0.9599 - strength_acc: 0.9693 - inflection_acc: 0.9736 - val_loss: 3.7777 - val_pos_loss: 0.5441 - val_person_loss: 0.1011 - val_number_loss: 0.4478 - val_tense_loss: 0.0985 - val_mood_loss: 0.1473 - val_voice_loss: 4.1002e-07 - val_gender_loss: 0.6490 - val_case_loss: 0.5473 - val_degree_loss: 0.3667 - val_strength_loss: 0.2433 - val_inflection_loss: 0.1756 - val_pos_acc: 0.8632 - val_person_acc: 0.9666 - val_number_acc: 0.8677 - val_tense_acc: 0.9669 - val_mood_acc: 0.9515 - val_voice_acc: 1.0000 - val_gender_acc: 0.7873 - val_case_acc: 0.8102 - val_degree_acc: 0.9097 - val_strength_acc: 0.9372 - val_inflection_acc: 0.9399\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2708 - pos_loss: 0.2458 - person_loss: 0.1153 - number_loss: 0.2449 - tense_loss: 0.0762 - mood_loss: 0.1016 - voice_loss: 8.0430e-07 - gender_loss: 0.3660 - case_loss: 0.3744 - degree_loss: 0.1218 - strength_loss: 0.0922 - inflection_loss: 0.0706 - pos_acc: 0.9229 - person_acc: 0.9622 - number_acc: 0.9113 - tense_acc: 0.9738 - mood_acc: 0.9662 - voice_acc: 1.0000 - gender_acc: 0.8692 - case_acc: 0.8619 - degree_acc: 0.9606 - strength_acc: 0.9698 - inflection_acc: 0.9749 - val_loss: 3.5559 - val_pos_loss: 0.5038 - val_person_loss: 0.0824 - val_number_loss: 0.4291 - val_tense_loss: 0.0676 - val_mood_loss: 0.1125 - val_voice_loss: 4.3207e-07 - val_gender_loss: 0.6353 - val_case_loss: 0.5277 - val_degree_loss: 0.3397 - val_strength_loss: 0.2429 - val_inflection_loss: 0.1748 - val_pos_acc: 0.8697 - val_person_acc: 0.9726 - val_number_acc: 0.8714 - val_tense_acc: 0.9773 - val_mood_acc: 0.9631 - val_voice_acc: 1.0000 - val_gender_acc: 0.7891 - val_case_acc: 0.8202 - val_degree_acc: 0.9093 - val_strength_acc: 0.9378 - val_inflection_acc: 0.9420\n",
      "Epoch 58/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2500 - pos_loss: 0.2363 - person_loss: 0.1129 - number_loss: 0.2465 - tense_loss: 0.0723 - mood_loss: 0.0997 - voice_loss: 5.9651e-07 - gender_loss: 0.3702 - case_loss: 0.3696 - degree_loss: 0.1211 - strength_loss: 0.0911 - inflection_loss: 0.0720 - pos_acc: 0.9265 - person_acc: 0.9619 - number_acc: 0.9090 - tense_acc: 0.9752 - mood_acc: 0.9678 - voice_acc: 1.0000 - gender_acc: 0.8676 - case_acc: 0.8624 - degree_acc: 0.9615 - strength_acc: 0.9707 - inflection_acc: 0.9751 - val_loss: 3.8773 - val_pos_loss: 0.5673 - val_person_loss: 0.0974 - val_number_loss: 0.4661 - val_tense_loss: 0.0846 - val_mood_loss: 0.1475 - val_voice_loss: 3.1316e-07 - val_gender_loss: 0.6608 - val_case_loss: 0.5457 - val_degree_loss: 0.3980 - val_strength_loss: 0.2608 - val_inflection_loss: 0.2022 - val_pos_acc: 0.8567 - val_person_acc: 0.9695 - val_number_acc: 0.8663 - val_tense_acc: 0.9685 - val_mood_acc: 0.9525 - val_voice_acc: 1.0000 - val_gender_acc: 0.7882 - val_case_acc: 0.8223 - val_degree_acc: 0.9001 - val_strength_acc: 0.9345 - val_inflection_acc: 0.9468\n",
      "Epoch 59/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2317 - pos_loss: 0.2391 - person_loss: 0.1102 - number_loss: 0.2391 - tense_loss: 0.0724 - mood_loss: 0.1004 - voice_loss: 6.2733e-07 - gender_loss: 0.3632 - case_loss: 0.3680 - degree_loss: 0.1191 - strength_loss: 0.0882 - inflection_loss: 0.0708 - pos_acc: 0.9240 - person_acc: 0.9632 - number_acc: 0.9128 - tense_acc: 0.9749 - mood_acc: 0.9663 - voice_acc: 1.0000 - gender_acc: 0.8680 - case_acc: 0.8632 - degree_acc: 0.9614 - strength_acc: 0.9707 - inflection_acc: 0.9736 - val_loss: 3.6176 - val_pos_loss: 0.5246 - val_person_loss: 0.0822 - val_number_loss: 0.4296 - val_tense_loss: 0.0769 - val_mood_loss: 0.1252 - val_voice_loss: 3.9022e-07 - val_gender_loss: 0.6238 - val_case_loss: 0.5272 - val_degree_loss: 0.3582 - val_strength_loss: 0.2297 - val_inflection_loss: 0.1873 - val_pos_acc: 0.8663 - val_person_acc: 0.9708 - val_number_acc: 0.8759 - val_tense_acc: 0.9730 - val_mood_acc: 0.9584 - val_voice_acc: 1.0000 - val_gender_acc: 0.7956 - val_case_acc: 0.8250 - val_degree_acc: 0.9060 - val_strength_acc: 0.9388 - val_inflection_acc: 0.9509\n",
      "Epoch 60/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.2041 - pos_loss: 0.2345 - person_loss: 0.1086 - number_loss: 0.2362 - tense_loss: 0.0722 - mood_loss: 0.0994 - voice_loss: 7.2261e-07 - gender_loss: 0.3606 - case_loss: 0.3616 - degree_loss: 0.1175 - strength_loss: 0.0878 - inflection_loss: 0.0680 - pos_acc: 0.9255 - person_acc: 0.9634 - number_acc: 0.9145 - tense_acc: 0.9746 - mood_acc: 0.9657 - voice_acc: 1.0000 - gender_acc: 0.8707 - case_acc: 0.8662 - degree_acc: 0.9618 - strength_acc: 0.9706 - inflection_acc: 0.9756 - val_loss: 3.6295 - val_pos_loss: 0.5185 - val_person_loss: 0.1003 - val_number_loss: 0.4269 - val_tense_loss: 0.0779 - val_mood_loss: 0.1307 - val_voice_loss: 4.4815e-07 - val_gender_loss: 0.6255 - val_case_loss: 0.5244 - val_degree_loss: 0.3697 - val_strength_loss: 0.2418 - val_inflection_loss: 0.1763 - val_pos_acc: 0.8710 - val_person_acc: 0.9726 - val_number_acc: 0.8765 - val_tense_acc: 0.9749 - val_mood_acc: 0.9595 - val_voice_acc: 1.0000 - val_gender_acc: 0.7987 - val_case_acc: 0.8226 - val_degree_acc: 0.9080 - val_strength_acc: 0.9381 - val_inflection_acc: 0.9477\n",
      "Epoch 61/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1826 - pos_loss: 0.2317 - person_loss: 0.1097 - number_loss: 0.2327 - tense_loss: 0.0706 - mood_loss: 0.0975 - voice_loss: 7.0966e-07 - gender_loss: 0.3563 - case_loss: 0.3593 - degree_loss: 0.1170 - strength_loss: 0.0877 - inflection_loss: 0.0684 - pos_acc: 0.9273 - person_acc: 0.9641 - number_acc: 0.9162 - tense_acc: 0.9756 - mood_acc: 0.9671 - voice_acc: 1.0000 - gender_acc: 0.8723 - case_acc: 0.8658 - degree_acc: 0.9623 - strength_acc: 0.9711 - inflection_acc: 0.9759 - val_loss: 3.7779 - val_pos_loss: 0.5756 - val_person_loss: 0.0910 - val_number_loss: 0.4338 - val_tense_loss: 0.0933 - val_mood_loss: 0.1352 - val_voice_loss: 2.8295e-07 - val_gender_loss: 0.6333 - val_case_loss: 0.5310 - val_degree_loss: 0.3946 - val_strength_loss: 0.2668 - val_inflection_loss: 0.1837 - val_pos_acc: 0.8617 - val_person_acc: 0.9675 - val_number_acc: 0.8771 - val_tense_acc: 0.9683 - val_mood_acc: 0.9529 - val_voice_acc: 1.0000 - val_gender_acc: 0.7955 - val_case_acc: 0.8245 - val_degree_acc: 0.9026 - val_strength_acc: 0.9313 - val_inflection_acc: 0.9444\n",
      "Epoch 62/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1970 - pos_loss: 0.2343 - person_loss: 0.1072 - number_loss: 0.2399 - tense_loss: 0.0706 - mood_loss: 0.0984 - voice_loss: 6.3752e-07 - gender_loss: 0.3527 - case_loss: 0.3608 - degree_loss: 0.1186 - strength_loss: 0.0869 - inflection_loss: 0.0707 - pos_acc: 0.9255 - person_acc: 0.9644 - number_acc: 0.9120 - tense_acc: 0.9749 - mood_acc: 0.9679 - voice_acc: 1.0000 - gender_acc: 0.8759 - case_acc: 0.8675 - degree_acc: 0.9618 - strength_acc: 0.9711 - inflection_acc: 0.9744 - val_loss: 3.4537 - val_pos_loss: 0.4998 - val_person_loss: 0.0829 - val_number_loss: 0.3995 - val_tense_loss: 0.0749 - val_mood_loss: 0.1189 - val_voice_loss: 3.0117e-07 - val_gender_loss: 0.5900 - val_case_loss: 0.4990 - val_degree_loss: 0.3490 - val_strength_loss: 0.2306 - val_inflection_loss: 0.1586 - val_pos_acc: 0.8778 - val_person_acc: 0.9756 - val_number_acc: 0.8805 - val_tense_acc: 0.9764 - val_mood_acc: 0.9624 - val_voice_acc: 1.0000 - val_gender_acc: 0.8029 - val_case_acc: 0.8313 - val_degree_acc: 0.9119 - val_strength_acc: 0.9417 - val_inflection_acc: 0.9567\n",
      "Epoch 63/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1668 - pos_loss: 0.2262 - person_loss: 0.1072 - number_loss: 0.2391 - tense_loss: 0.0684 - mood_loss: 0.0967 - voice_loss: 5.5884e-07 - gender_loss: 0.3566 - case_loss: 0.3494 - degree_loss: 0.1120 - strength_loss: 0.0821 - inflection_loss: 0.0701 - pos_acc: 0.9288 - person_acc: 0.9657 - number_acc: 0.9133 - tense_acc: 0.9751 - mood_acc: 0.9680 - voice_acc: 1.0000 - gender_acc: 0.8726 - case_acc: 0.8688 - degree_acc: 0.9622 - strength_acc: 0.9727 - inflection_acc: 0.9749 - val_loss: 3.6977 - val_pos_loss: 0.5577 - val_person_loss: 0.0739 - val_number_loss: 0.4306 - val_tense_loss: 0.0753 - val_mood_loss: 0.1195 - val_voice_loss: 2.1951e-07 - val_gender_loss: 0.6275 - val_case_loss: 0.5419 - val_degree_loss: 0.3824 - val_strength_loss: 0.2664 - val_inflection_loss: 0.1835 - val_pos_acc: 0.8715 - val_person_acc: 0.9786 - val_number_acc: 0.8725 - val_tense_acc: 0.9738 - val_mood_acc: 0.9616 - val_voice_acc: 1.0000 - val_gender_acc: 0.7935 - val_case_acc: 0.8116 - val_degree_acc: 0.9069 - val_strength_acc: 0.9370 - val_inflection_acc: 0.9463\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1572 - pos_loss: 0.2258 - person_loss: 0.1098 - number_loss: 0.2313 - tense_loss: 0.0706 - mood_loss: 0.0978 - voice_loss: 6.1493e-07 - gender_loss: 0.3495 - case_loss: 0.3491 - degree_loss: 0.1151 - strength_loss: 0.0807 - inflection_loss: 0.0675 - pos_acc: 0.9290 - person_acc: 0.9642 - number_acc: 0.9152 - tense_acc: 0.9752 - mood_acc: 0.9677 - voice_acc: 1.0000 - gender_acc: 0.8754 - case_acc: 0.8718 - degree_acc: 0.9638 - strength_acc: 0.9739 - inflection_acc: 0.9750 - val_loss: 3.5680 - val_pos_loss: 0.5122 - val_person_loss: 0.0835 - val_number_loss: 0.4304 - val_tense_loss: 0.0810 - val_mood_loss: 0.1192 - val_voice_loss: 3.3229e-07 - val_gender_loss: 0.6053 - val_case_loss: 0.5196 - val_degree_loss: 0.3476 - val_strength_loss: 0.2311 - val_inflection_loss: 0.1835 - val_pos_acc: 0.8686 - val_person_acc: 0.9797 - val_number_acc: 0.8771 - val_tense_acc: 0.9716 - val_mood_acc: 0.9621 - val_voice_acc: 1.0000 - val_gender_acc: 0.7999 - val_case_acc: 0.8240 - val_degree_acc: 0.9088 - val_strength_acc: 0.9385 - val_inflection_acc: 0.9479\n",
      "Epoch 65/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1461 - pos_loss: 0.2236 - person_loss: 0.1081 - number_loss: 0.2325 - tense_loss: 0.0662 - mood_loss: 0.0930 - voice_loss: 6.6029e-07 - gender_loss: 0.3477 - case_loss: 0.3515 - degree_loss: 0.1146 - strength_loss: 0.0833 - inflection_loss: 0.0674 - pos_acc: 0.9295 - person_acc: 0.9642 - number_acc: 0.9165 - tense_acc: 0.9763 - mood_acc: 0.9689 - voice_acc: 1.0000 - gender_acc: 0.8754 - case_acc: 0.8669 - degree_acc: 0.9632 - strength_acc: 0.9726 - inflection_acc: 0.9763 - val_loss: 3.5798 - val_pos_loss: 0.5271 - val_person_loss: 0.0790 - val_number_loss: 0.4345 - val_tense_loss: 0.0746 - val_mood_loss: 0.1155 - val_voice_loss: 2.8570e-07 - val_gender_loss: 0.6213 - val_case_loss: 0.5269 - val_degree_loss: 0.3457 - val_strength_loss: 0.2288 - val_inflection_loss: 0.1774 - val_pos_acc: 0.8708 - val_person_acc: 0.9806 - val_number_acc: 0.8662 - val_tense_acc: 0.9760 - val_mood_acc: 0.9641 - val_voice_acc: 1.0000 - val_gender_acc: 0.7893 - val_case_acc: 0.8240 - val_degree_acc: 0.9135 - val_strength_acc: 0.9435 - val_inflection_acc: 0.9500\n",
      "Epoch 66/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1260 - pos_loss: 0.2192 - person_loss: 0.1049 - number_loss: 0.2264 - tense_loss: 0.0681 - mood_loss: 0.0913 - voice_loss: 5.4122e-07 - gender_loss: 0.3497 - case_loss: 0.3452 - degree_loss: 0.1139 - strength_loss: 0.0833 - inflection_loss: 0.0657 - pos_acc: 0.9297 - person_acc: 0.9659 - number_acc: 0.9170 - tense_acc: 0.9763 - mood_acc: 0.9700 - voice_acc: 1.0000 - gender_acc: 0.8765 - case_acc: 0.8720 - degree_acc: 0.9641 - strength_acc: 0.9734 - inflection_acc: 0.9763 - val_loss: 3.5331 - val_pos_loss: 0.5126 - val_person_loss: 0.0817 - val_number_loss: 0.4032 - val_tense_loss: 0.0743 - val_mood_loss: 0.1188 - val_voice_loss: 3.0715e-07 - val_gender_loss: 0.6201 - val_case_loss: 0.5122 - val_degree_loss: 0.3522 - val_strength_loss: 0.2352 - val_inflection_loss: 0.1714 - val_pos_acc: 0.8636 - val_person_acc: 0.9783 - val_number_acc: 0.8857 - val_tense_acc: 0.9743 - val_mood_acc: 0.9648 - val_voice_acc: 1.0000 - val_gender_acc: 0.7964 - val_case_acc: 0.8287 - val_degree_acc: 0.9099 - val_strength_acc: 0.9350 - val_inflection_acc: 0.9494\n",
      "Epoch 67/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.1050 - pos_loss: 0.2169 - person_loss: 0.1054 - number_loss: 0.2269 - tense_loss: 0.0659 - mood_loss: 0.0918 - voice_loss: 5.0225e-07 - gender_loss: 0.3434 - case_loss: 0.3436 - degree_loss: 0.1111 - strength_loss: 0.0799 - inflection_loss: 0.0650 - pos_acc: 0.9313 - person_acc: 0.9662 - number_acc: 0.9178 - tense_acc: 0.9777 - mood_acc: 0.9695 - voice_acc: 1.0000 - gender_acc: 0.8783 - case_acc: 0.8703 - degree_acc: 0.9636 - strength_acc: 0.9735 - inflection_acc: 0.9761 - val_loss: 3.4642 - val_pos_loss: 0.5028 - val_person_loss: 0.0860 - val_number_loss: 0.4129 - val_tense_loss: 0.0788 - val_mood_loss: 0.1195 - val_voice_loss: 2.3975e-07 - val_gender_loss: 0.5932 - val_case_loss: 0.4968 - val_degree_loss: 0.3364 - val_strength_loss: 0.2261 - val_inflection_loss: 0.1657 - val_pos_acc: 0.8712 - val_person_acc: 0.9723 - val_number_acc: 0.8824 - val_tense_acc: 0.9719 - val_mood_acc: 0.9602 - val_voice_acc: 1.0000 - val_gender_acc: 0.8067 - val_case_acc: 0.8253 - val_degree_acc: 0.9112 - val_strength_acc: 0.9442 - val_inflection_acc: 0.9499\n",
      "Epoch 68/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0985 - pos_loss: 0.2158 - person_loss: 0.1033 - number_loss: 0.2264 - tense_loss: 0.0655 - mood_loss: 0.0886 - voice_loss: 5.2092e-07 - gender_loss: 0.3451 - case_loss: 0.3445 - degree_loss: 0.1109 - strength_loss: 0.0805 - inflection_loss: 0.0662 - pos_acc: 0.9322 - person_acc: 0.9663 - number_acc: 0.9182 - tense_acc: 0.9770 - mood_acc: 0.9708 - voice_acc: 1.0000 - gender_acc: 0.8768 - case_acc: 0.8734 - degree_acc: 0.9653 - strength_acc: 0.9750 - inflection_acc: 0.9767 - val_loss: 3.5703 - val_pos_loss: 0.5156 - val_person_loss: 0.0817 - val_number_loss: 0.4281 - val_tense_loss: 0.0804 - val_mood_loss: 0.1266 - val_voice_loss: 2.6005e-07 - val_gender_loss: 0.6347 - val_case_loss: 0.5200 - val_degree_loss: 0.3424 - val_strength_loss: 0.2309 - val_inflection_loss: 0.1735 - val_pos_acc: 0.8788 - val_person_acc: 0.9768 - val_number_acc: 0.8785 - val_tense_acc: 0.9748 - val_mood_acc: 0.9628 - val_voice_acc: 1.0000 - val_gender_acc: 0.7915 - val_case_acc: 0.8328 - val_degree_acc: 0.9094 - val_strength_acc: 0.9485 - val_inflection_acc: 0.9498\n",
      "Epoch 69/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0763 - pos_loss: 0.2105 - person_loss: 0.1002 - number_loss: 0.2226 - tense_loss: 0.0650 - mood_loss: 0.0882 - voice_loss: 4.4929e-07 - gender_loss: 0.3414 - case_loss: 0.3443 - degree_loss: 0.1096 - strength_loss: 0.0771 - inflection_loss: 0.0660 - pos_acc: 0.9324 - person_acc: 0.9665 - number_acc: 0.9183 - tense_acc: 0.9774 - mood_acc: 0.9707 - voice_acc: 1.0000 - gender_acc: 0.8787 - case_acc: 0.8726 - degree_acc: 0.9646 - strength_acc: 0.9759 - inflection_acc: 0.9757 - val_loss: 3.7801 - val_pos_loss: 0.5544 - val_person_loss: 0.0976 - val_number_loss: 0.4486 - val_tense_loss: 0.0889 - val_mood_loss: 0.1283 - val_voice_loss: 2.7634e-07 - val_gender_loss: 0.6420 - val_case_loss: 0.5571 - val_degree_loss: 0.3707 - val_strength_loss: 0.2489 - val_inflection_loss: 0.2092 - val_pos_acc: 0.8734 - val_person_acc: 0.9709 - val_number_acc: 0.8753 - val_tense_acc: 0.9704 - val_mood_acc: 0.9600 - val_voice_acc: 1.0000 - val_gender_acc: 0.7942 - val_case_acc: 0.8324 - val_degree_acc: 0.9095 - val_strength_acc: 0.9410 - val_inflection_acc: 0.9459\n",
      "Epoch 70/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0916 - pos_loss: 0.2147 - person_loss: 0.1052 - number_loss: 0.2251 - tense_loss: 0.0687 - mood_loss: 0.0911 - voice_loss: 4.6505e-07 - gender_loss: 0.3391 - case_loss: 0.3440 - degree_loss: 0.1086 - strength_loss: 0.0781 - inflection_loss: 0.0664 - pos_acc: 0.9315 - person_acc: 0.9655 - number_acc: 0.9183 - tense_acc: 0.9768 - mood_acc: 0.9698 - voice_acc: 1.0000 - gender_acc: 0.8797 - case_acc: 0.8730 - degree_acc: 0.9643 - strength_acc: 0.9742 - inflection_acc: 0.9766 - val_loss: 3.6113 - val_pos_loss: 0.5497 - val_person_loss: 0.0851 - val_number_loss: 0.4148 - val_tense_loss: 0.0834 - val_mood_loss: 0.1188 - val_voice_loss: 2.0203e-07 - val_gender_loss: 0.6181 - val_case_loss: 0.5235 - val_degree_loss: 0.3491 - val_strength_loss: 0.2473 - val_inflection_loss: 0.1796 - val_pos_acc: 0.8723 - val_person_acc: 0.9763 - val_number_acc: 0.8792 - val_tense_acc: 0.9754 - val_mood_acc: 0.9659 - val_voice_acc: 1.0000 - val_gender_acc: 0.8015 - val_case_acc: 0.8296 - val_degree_acc: 0.9093 - val_strength_acc: 0.9370 - val_inflection_acc: 0.9476\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0531 - pos_loss: 0.2104 - person_loss: 0.0980 - number_loss: 0.2189 - tense_loss: 0.0637 - mood_loss: 0.0893 - voice_loss: 5.0940e-07 - gender_loss: 0.3366 - case_loss: 0.3347 - degree_loss: 0.1084 - strength_loss: 0.0779 - inflection_loss: 0.0639 - pos_acc: 0.9329 - person_acc: 0.9674 - number_acc: 0.9205 - tense_acc: 0.9776 - mood_acc: 0.9706 - voice_acc: 1.0000 - gender_acc: 0.8797 - case_acc: 0.8767 - degree_acc: 0.9650 - strength_acc: 0.9755 - inflection_acc: 0.9770 - val_loss: 3.6232 - val_pos_loss: 0.5220 - val_person_loss: 0.0799 - val_number_loss: 0.4296 - val_tense_loss: 0.0813 - val_mood_loss: 0.1206 - val_voice_loss: 1.7536e-07 - val_gender_loss: 0.6388 - val_case_loss: 0.5330 - val_degree_loss: 0.3453 - val_strength_loss: 0.2319 - val_inflection_loss: 0.1919 - val_pos_acc: 0.8685 - val_person_acc: 0.9727 - val_number_acc: 0.8750 - val_tense_acc: 0.9731 - val_mood_acc: 0.9579 - val_voice_acc: 1.0000 - val_gender_acc: 0.7897 - val_case_acc: 0.8234 - val_degree_acc: 0.9122 - val_strength_acc: 0.9436 - val_inflection_acc: 0.9443\n",
      "Epoch 72/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0608 - pos_loss: 0.2078 - person_loss: 0.1028 - number_loss: 0.2195 - tense_loss: 0.0631 - mood_loss: 0.0900 - voice_loss: 3.9337e-07 - gender_loss: 0.3350 - case_loss: 0.3351 - degree_loss: 0.1088 - strength_loss: 0.0785 - inflection_loss: 0.0639 - pos_acc: 0.9351 - person_acc: 0.9649 - number_acc: 0.9196 - tense_acc: 0.9781 - mood_acc: 0.9700 - voice_acc: 1.0000 - gender_acc: 0.8807 - case_acc: 0.8766 - degree_acc: 0.9651 - strength_acc: 0.9748 - inflection_acc: 0.9766 - val_loss: 3.5928 - val_pos_loss: 0.5334 - val_person_loss: 0.0749 - val_number_loss: 0.4206 - val_tense_loss: 0.0747 - val_mood_loss: 0.1159 - val_voice_loss: 1.6478e-07 - val_gender_loss: 0.6402 - val_case_loss: 0.5299 - val_degree_loss: 0.3404 - val_strength_loss: 0.2474 - val_inflection_loss: 0.1793 - val_pos_acc: 0.8708 - val_person_acc: 0.9764 - val_number_acc: 0.8748 - val_tense_acc: 0.9755 - val_mood_acc: 0.9616 - val_voice_acc: 1.0000 - val_gender_acc: 0.7932 - val_case_acc: 0.8206 - val_degree_acc: 0.9105 - val_strength_acc: 0.9393 - val_inflection_acc: 0.9474\n",
      "Epoch 73/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0366 - pos_loss: 0.2034 - person_loss: 0.0986 - number_loss: 0.2203 - tense_loss: 0.0630 - mood_loss: 0.0883 - voice_loss: 4.2403e-07 - gender_loss: 0.3366 - case_loss: 0.3340 - degree_loss: 0.1057 - strength_loss: 0.0748 - inflection_loss: 0.0635 - pos_acc: 0.9350 - person_acc: 0.9684 - number_acc: 0.9204 - tense_acc: 0.9795 - mood_acc: 0.9706 - voice_acc: 1.0000 - gender_acc: 0.8800 - case_acc: 0.8769 - degree_acc: 0.9658 - strength_acc: 0.9753 - inflection_acc: 0.9777 - val_loss: 3.5546 - val_pos_loss: 0.5079 - val_person_loss: 0.0864 - val_number_loss: 0.4195 - val_tense_loss: 0.0732 - val_mood_loss: 0.1186 - val_voice_loss: 2.5711e-07 - val_gender_loss: 0.6127 - val_case_loss: 0.5217 - val_degree_loss: 0.3439 - val_strength_loss: 0.2446 - val_inflection_loss: 0.1795 - val_pos_acc: 0.8672 - val_person_acc: 0.9734 - val_number_acc: 0.8750 - val_tense_acc: 0.9745 - val_mood_acc: 0.9593 - val_voice_acc: 1.0000 - val_gender_acc: 0.7968 - val_case_acc: 0.8276 - val_degree_acc: 0.9157 - val_strength_acc: 0.9393 - val_inflection_acc: 0.9426\n",
      "Epoch 74/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0611 - pos_loss: 0.2081 - person_loss: 0.1017 - number_loss: 0.2198 - tense_loss: 0.0631 - mood_loss: 0.0877 - voice_loss: 4.4359e-07 - gender_loss: 0.3370 - case_loss: 0.3369 - degree_loss: 0.1070 - strength_loss: 0.0772 - inflection_loss: 0.0656 - pos_acc: 0.9348 - person_acc: 0.9671 - number_acc: 0.9209 - tense_acc: 0.9778 - mood_acc: 0.9705 - voice_acc: 1.0000 - gender_acc: 0.8786 - case_acc: 0.8744 - degree_acc: 0.9653 - strength_acc: 0.9758 - inflection_acc: 0.9765 - val_loss: 3.4930 - val_pos_loss: 0.5070 - val_person_loss: 0.0889 - val_number_loss: 0.3954 - val_tense_loss: 0.0813 - val_mood_loss: 0.1318 - val_voice_loss: 2.0629e-07 - val_gender_loss: 0.5990 - val_case_loss: 0.4998 - val_degree_loss: 0.3583 - val_strength_loss: 0.2287 - val_inflection_loss: 0.1627 - val_pos_acc: 0.8783 - val_person_acc: 0.9732 - val_number_acc: 0.8827 - val_tense_acc: 0.9690 - val_mood_acc: 0.9572 - val_voice_acc: 1.0000 - val_gender_acc: 0.8073 - val_case_acc: 0.8312 - val_degree_acc: 0.9126 - val_strength_acc: 0.9482 - val_inflection_acc: 0.9528\n",
      "Epoch 75/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0397 - pos_loss: 0.2054 - person_loss: 0.1004 - number_loss: 0.2213 - tense_loss: 0.0632 - mood_loss: 0.0846 - voice_loss: 3.5610e-07 - gender_loss: 0.3310 - case_loss: 0.3361 - degree_loss: 0.1069 - strength_loss: 0.0756 - inflection_loss: 0.0650 - pos_acc: 0.9353 - person_acc: 0.9672 - number_acc: 0.9212 - tense_acc: 0.9773 - mood_acc: 0.9723 - voice_acc: 1.0000 - gender_acc: 0.8830 - case_acc: 0.8748 - degree_acc: 0.9646 - strength_acc: 0.9759 - inflection_acc: 0.9768 - val_loss: 3.3555 - val_pos_loss: 0.4976 - val_person_loss: 0.0693 - val_number_loss: 0.3919 - val_tense_loss: 0.0644 - val_mood_loss: 0.1093 - val_voice_loss: 2.2343e-07 - val_gender_loss: 0.5765 - val_case_loss: 0.4763 - val_degree_loss: 0.3385 - val_strength_loss: 0.2277 - val_inflection_loss: 0.1635 - val_pos_acc: 0.8808 - val_person_acc: 0.9779 - val_number_acc: 0.8879 - val_tense_acc: 0.9782 - val_mood_acc: 0.9662 - val_voice_acc: 1.0000 - val_gender_acc: 0.8060 - val_case_acc: 0.8397 - val_degree_acc: 0.9090 - val_strength_acc: 0.9406 - val_inflection_acc: 0.9525\n",
      "Epoch 76/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9790 - pos_loss: 0.1955 - person_loss: 0.0951 - number_loss: 0.2163 - tense_loss: 0.0586 - mood_loss: 0.0836 - voice_loss: 4.7787e-07 - gender_loss: 0.3263 - case_loss: 0.3221 - degree_loss: 0.1052 - strength_loss: 0.0764 - inflection_loss: 0.0622 - pos_acc: 0.9381 - person_acc: 0.9692 - number_acc: 0.9241 - tense_acc: 0.9797 - mood_acc: 0.9717 - voice_acc: 1.0000 - gender_acc: 0.8844 - case_acc: 0.8789 - degree_acc: 0.9658 - strength_acc: 0.9758 - inflection_acc: 0.9777 - val_loss: 3.4614 - val_pos_loss: 0.5010 - val_person_loss: 0.0806 - val_number_loss: 0.4138 - val_tense_loss: 0.0726 - val_mood_loss: 0.1116 - val_voice_loss: 1.7874e-07 - val_gender_loss: 0.6065 - val_case_loss: 0.5200 - val_degree_loss: 0.3302 - val_strength_loss: 0.2227 - val_inflection_loss: 0.1798 - val_pos_acc: 0.8774 - val_person_acc: 0.9748 - val_number_acc: 0.8824 - val_tense_acc: 0.9785 - val_mood_acc: 0.9663 - val_voice_acc: 1.0000 - val_gender_acc: 0.8012 - val_case_acc: 0.8292 - val_degree_acc: 0.9139 - val_strength_acc: 0.9442 - val_inflection_acc: 0.9495\n",
      "Epoch 77/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 2.0487 - pos_loss: 0.2079 - person_loss: 0.1029 - number_loss: 0.2208 - tense_loss: 0.0646 - mood_loss: 0.0925 - voice_loss: 4.9388e-07 - gender_loss: 0.3298 - case_loss: 0.3337 - degree_loss: 0.1078 - strength_loss: 0.0777 - inflection_loss: 0.0654 - pos_acc: 0.9345 - person_acc: 0.9666 - number_acc: 0.9210 - tense_acc: 0.9773 - mood_acc: 0.9698 - voice_acc: 1.0000 - gender_acc: 0.8845 - case_acc: 0.8749 - degree_acc: 0.9659 - strength_acc: 0.9756 - inflection_acc: 0.9766 - val_loss: 3.5099 - val_pos_loss: 0.4961 - val_person_loss: 0.0720 - val_number_loss: 0.4135 - val_tense_loss: 0.0690 - val_mood_loss: 0.1107 - val_voice_loss: 2.3390e-07 - val_gender_loss: 0.6036 - val_case_loss: 0.5222 - val_degree_loss: 0.3496 - val_strength_loss: 0.2387 - val_inflection_loss: 0.1896 - val_pos_acc: 0.8750 - val_person_acc: 0.9745 - val_number_acc: 0.8787 - val_tense_acc: 0.9755 - val_mood_acc: 0.9631 - val_voice_acc: 1.0000 - val_gender_acc: 0.8023 - val_case_acc: 0.8231 - val_degree_acc: 0.9175 - val_strength_acc: 0.9389 - val_inflection_acc: 0.9466\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9879 - pos_loss: 0.1942 - person_loss: 0.0966 - number_loss: 0.2154 - tense_loss: 0.0596 - mood_loss: 0.0820 - voice_loss: 3.6679e-07 - gender_loss: 0.3289 - case_loss: 0.3263 - degree_loss: 0.1019 - strength_loss: 0.0739 - inflection_loss: 0.0623 - pos_acc: 0.9361 - person_acc: 0.9690 - number_acc: 0.9212 - tense_acc: 0.9793 - mood_acc: 0.9724 - voice_acc: 1.0000 - gender_acc: 0.8822 - case_acc: 0.8799 - degree_acc: 0.9670 - strength_acc: 0.9757 - inflection_acc: 0.9782 - val_loss: 3.3915 - val_pos_loss: 0.4931 - val_person_loss: 0.0641 - val_number_loss: 0.4102 - val_tense_loss: 0.0592 - val_mood_loss: 0.0978 - val_voice_loss: 2.0105e-07 - val_gender_loss: 0.5924 - val_case_loss: 0.5099 - val_degree_loss: 0.3237 - val_strength_loss: 0.2140 - val_inflection_loss: 0.1797 - val_pos_acc: 0.8818 - val_person_acc: 0.9785 - val_number_acc: 0.8880 - val_tense_acc: 0.9783 - val_mood_acc: 0.9631 - val_voice_acc: 1.0000 - val_gender_acc: 0.8057 - val_case_acc: 0.8295 - val_degree_acc: 0.9170 - val_strength_acc: 0.9464 - val_inflection_acc: 0.9553\n",
      "Epoch 79/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9595 - pos_loss: 0.1936 - person_loss: 0.0951 - number_loss: 0.2093 - tense_loss: 0.0587 - mood_loss: 0.0824 - voice_loss: 4.3115e-07 - gender_loss: 0.3216 - case_loss: 0.3210 - degree_loss: 0.1024 - strength_loss: 0.0730 - inflection_loss: 0.0594 - pos_acc: 0.9382 - person_acc: 0.9692 - number_acc: 0.9253 - tense_acc: 0.9793 - mood_acc: 0.9726 - voice_acc: 1.0000 - gender_acc: 0.8858 - case_acc: 0.8822 - degree_acc: 0.9678 - strength_acc: 0.9768 - inflection_acc: 0.9795 - val_loss: 3.5517 - val_pos_loss: 0.5104 - val_person_loss: 0.0714 - val_number_loss: 0.4425 - val_tense_loss: 0.0705 - val_mood_loss: 0.1045 - val_voice_loss: 1.6037e-07 - val_gender_loss: 0.6230 - val_case_loss: 0.5219 - val_degree_loss: 0.3469 - val_strength_loss: 0.2405 - val_inflection_loss: 0.1935 - val_pos_acc: 0.8792 - val_person_acc: 0.9744 - val_number_acc: 0.8708 - val_tense_acc: 0.9768 - val_mood_acc: 0.9637 - val_voice_acc: 1.0000 - val_gender_acc: 0.7938 - val_case_acc: 0.8318 - val_degree_acc: 0.9169 - val_strength_acc: 0.9418 - val_inflection_acc: 0.9540\n",
      "Epoch 80/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9572 - pos_loss: 0.1952 - person_loss: 0.0951 - number_loss: 0.2106 - tense_loss: 0.0608 - mood_loss: 0.0830 - voice_loss: 3.1158e-07 - gender_loss: 0.3177 - case_loss: 0.3163 - degree_loss: 0.1040 - strength_loss: 0.0743 - inflection_loss: 0.0619 - pos_acc: 0.9373 - person_acc: 0.9690 - number_acc: 0.9243 - tense_acc: 0.9788 - mood_acc: 0.9724 - voice_acc: 1.0000 - gender_acc: 0.8882 - case_acc: 0.8825 - degree_acc: 0.9661 - strength_acc: 0.9765 - inflection_acc: 0.9780 - val_loss: 3.5683 - val_pos_loss: 0.5397 - val_person_loss: 0.0832 - val_number_loss: 0.4231 - val_tense_loss: 0.0837 - val_mood_loss: 0.1325 - val_voice_loss: 1.8739e-07 - val_gender_loss: 0.6054 - val_case_loss: 0.5120 - val_degree_loss: 0.3406 - val_strength_loss: 0.2363 - val_inflection_loss: 0.1833 - val_pos_acc: 0.8674 - val_person_acc: 0.9721 - val_number_acc: 0.8799 - val_tense_acc: 0.9722 - val_mood_acc: 0.9594 - val_voice_acc: 1.0000 - val_gender_acc: 0.8054 - val_case_acc: 0.8312 - val_degree_acc: 0.9125 - val_strength_acc: 0.9398 - val_inflection_acc: 0.9507\n",
      "Epoch 81/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9950 - pos_loss: 0.2042 - person_loss: 0.0958 - number_loss: 0.2094 - tense_loss: 0.0615 - mood_loss: 0.0811 - voice_loss: 3.2496e-07 - gender_loss: 0.3224 - case_loss: 0.3253 - degree_loss: 0.1062 - strength_loss: 0.0773 - inflection_loss: 0.0612 - pos_acc: 0.9367 - person_acc: 0.9695 - number_acc: 0.9237 - tense_acc: 0.9791 - mood_acc: 0.9732 - voice_acc: 1.0000 - gender_acc: 0.8856 - case_acc: 0.8783 - degree_acc: 0.9662 - strength_acc: 0.9756 - inflection_acc: 0.9785 - val_loss: 3.4457 - val_pos_loss: 0.4946 - val_person_loss: 0.0675 - val_number_loss: 0.4170 - val_tense_loss: 0.0628 - val_mood_loss: 0.1028 - val_voice_loss: 1.7697e-07 - val_gender_loss: 0.5953 - val_case_loss: 0.5147 - val_degree_loss: 0.3341 - val_strength_loss: 0.2279 - val_inflection_loss: 0.1767 - val_pos_acc: 0.8795 - val_person_acc: 0.9798 - val_number_acc: 0.8738 - val_tense_acc: 0.9808 - val_mood_acc: 0.9669 - val_voice_acc: 1.0000 - val_gender_acc: 0.8047 - val_case_acc: 0.8307 - val_degree_acc: 0.9169 - val_strength_acc: 0.9465 - val_inflection_acc: 0.9429\n",
      "Epoch 82/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9736 - pos_loss: 0.1920 - person_loss: 0.0963 - number_loss: 0.2152 - tense_loss: 0.0591 - mood_loss: 0.0821 - voice_loss: 2.5362e-07 - gender_loss: 0.3186 - case_loss: 0.3201 - degree_loss: 0.1018 - strength_loss: 0.0722 - inflection_loss: 0.0612 - pos_acc: 0.9388 - person_acc: 0.9681 - number_acc: 0.9214 - tense_acc: 0.9791 - mood_acc: 0.9727 - voice_acc: 1.0000 - gender_acc: 0.8872 - case_acc: 0.8816 - degree_acc: 0.9673 - strength_acc: 0.9762 - inflection_acc: 0.9775 - val_loss: 3.5577 - val_pos_loss: 0.4885 - val_person_loss: 0.0767 - val_number_loss: 0.4330 - val_tense_loss: 0.0726 - val_mood_loss: 0.1098 - val_voice_loss: 1.5133e-07 - val_gender_loss: 0.6227 - val_case_loss: 0.5407 - val_degree_loss: 0.3449 - val_strength_loss: 0.2316 - val_inflection_loss: 0.1884 - val_pos_acc: 0.8745 - val_person_acc: 0.9792 - val_number_acc: 0.8831 - val_tense_acc: 0.9790 - val_mood_acc: 0.9682 - val_voice_acc: 1.0000 - val_gender_acc: 0.7990 - val_case_acc: 0.8296 - val_degree_acc: 0.9166 - val_strength_acc: 0.9470 - val_inflection_acc: 0.9442\n",
      "Epoch 83/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9739 - pos_loss: 0.1932 - person_loss: 0.0942 - number_loss: 0.2133 - tense_loss: 0.0603 - mood_loss: 0.0831 - voice_loss: 2.8855e-07 - gender_loss: 0.3214 - case_loss: 0.3163 - degree_loss: 0.1030 - strength_loss: 0.0731 - inflection_loss: 0.0640 - pos_acc: 0.9387 - person_acc: 0.9680 - number_acc: 0.9227 - tense_acc: 0.9785 - mood_acc: 0.9716 - voice_acc: 1.0000 - gender_acc: 0.8853 - case_acc: 0.8860 - degree_acc: 0.9668 - strength_acc: 0.9758 - inflection_acc: 0.9773 - val_loss: 3.4861 - val_pos_loss: 0.4956 - val_person_loss: 0.0737 - val_number_loss: 0.4262 - val_tense_loss: 0.0770 - val_mood_loss: 0.1153 - val_voice_loss: 1.5644e-07 - val_gender_loss: 0.6025 - val_case_loss: 0.5038 - val_degree_loss: 0.3373 - val_strength_loss: 0.2266 - val_inflection_loss: 0.1840 - val_pos_acc: 0.8701 - val_person_acc: 0.9761 - val_number_acc: 0.8777 - val_tense_acc: 0.9746 - val_mood_acc: 0.9628 - val_voice_acc: 1.0000 - val_gender_acc: 0.8025 - val_case_acc: 0.8317 - val_degree_acc: 0.9181 - val_strength_acc: 0.9445 - val_inflection_acc: 0.9426\n",
      "Epoch 84/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9514 - pos_loss: 0.1900 - person_loss: 0.0932 - number_loss: 0.2103 - tense_loss: 0.0581 - mood_loss: 0.0792 - voice_loss: 2.7004e-07 - gender_loss: 0.3186 - case_loss: 0.3186 - degree_loss: 0.1017 - strength_loss: 0.0717 - inflection_loss: 0.0608 - pos_acc: 0.9396 - person_acc: 0.9690 - number_acc: 0.9252 - tense_acc: 0.9804 - mood_acc: 0.9734 - voice_acc: 1.0000 - gender_acc: 0.8867 - case_acc: 0.8828 - degree_acc: 0.9679 - strength_acc: 0.9763 - inflection_acc: 0.9789 - val_loss: 3.4757 - val_pos_loss: 0.5031 - val_person_loss: 0.0717 - val_number_loss: 0.4182 - val_tense_loss: 0.0701 - val_mood_loss: 0.1082 - val_voice_loss: 1.6294e-07 - val_gender_loss: 0.6054 - val_case_loss: 0.5163 - val_degree_loss: 0.3402 - val_strength_loss: 0.2286 - val_inflection_loss: 0.1839 - val_pos_acc: 0.8697 - val_person_acc: 0.9755 - val_number_acc: 0.8781 - val_tense_acc: 0.9766 - val_mood_acc: 0.9641 - val_voice_acc: 1.0000 - val_gender_acc: 0.7998 - val_case_acc: 0.8295 - val_degree_acc: 0.9109 - val_strength_acc: 0.9432 - val_inflection_acc: 0.9454\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9106 - pos_loss: 0.1901 - person_loss: 0.0923 - number_loss: 0.2019 - tense_loss: 0.0598 - mood_loss: 0.0801 - voice_loss: 2.8426e-07 - gender_loss: 0.3183 - case_loss: 0.3157 - degree_loss: 0.1008 - strength_loss: 0.0716 - inflection_loss: 0.0577 - pos_acc: 0.9416 - person_acc: 0.9701 - number_acc: 0.9276 - tense_acc: 0.9795 - mood_acc: 0.9737 - voice_acc: 1.0000 - gender_acc: 0.8895 - case_acc: 0.8831 - degree_acc: 0.9675 - strength_acc: 0.9776 - inflection_acc: 0.9801 - val_loss: 3.3668 - val_pos_loss: 0.4865 - val_person_loss: 0.0710 - val_number_loss: 0.4066 - val_tense_loss: 0.0686 - val_mood_loss: 0.1063 - val_voice_loss: 1.7769e-07 - val_gender_loss: 0.5840 - val_case_loss: 0.5026 - val_degree_loss: 0.3279 - val_strength_loss: 0.2204 - val_inflection_loss: 0.1722 - val_pos_acc: 0.8784 - val_person_acc: 0.9810 - val_number_acc: 0.8893 - val_tense_acc: 0.9775 - val_mood_acc: 0.9669 - val_voice_acc: 1.0000 - val_gender_acc: 0.8049 - val_case_acc: 0.8379 - val_degree_acc: 0.9171 - val_strength_acc: 0.9421 - val_inflection_acc: 0.9528\n",
      "Epoch 86/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8705 - pos_loss: 0.1852 - person_loss: 0.0920 - number_loss: 0.1996 - tense_loss: 0.0550 - mood_loss: 0.0778 - voice_loss: 2.9899e-07 - gender_loss: 0.3038 - case_loss: 0.3049 - degree_loss: 0.0971 - strength_loss: 0.0681 - inflection_loss: 0.0583 - pos_acc: 0.9406 - person_acc: 0.9701 - number_acc: 0.9289 - tense_acc: 0.9817 - mood_acc: 0.9748 - voice_acc: 1.0000 - gender_acc: 0.8918 - case_acc: 0.8870 - degree_acc: 0.9690 - strength_acc: 0.9782 - inflection_acc: 0.9788 - val_loss: 3.7174 - val_pos_loss: 0.5412 - val_person_loss: 0.0931 - val_number_loss: 0.4695 - val_tense_loss: 0.0838 - val_mood_loss: 0.1216 - val_voice_loss: 1.7758e-07 - val_gender_loss: 0.6349 - val_case_loss: 0.5476 - val_degree_loss: 0.3596 - val_strength_loss: 0.2337 - val_inflection_loss: 0.2059 - val_pos_acc: 0.8681 - val_person_acc: 0.9723 - val_number_acc: 0.8643 - val_tense_acc: 0.9738 - val_mood_acc: 0.9627 - val_voice_acc: 1.0000 - val_gender_acc: 0.7983 - val_case_acc: 0.8156 - val_degree_acc: 0.9129 - val_strength_acc: 0.9457 - val_inflection_acc: 0.9349\n",
      "Epoch 87/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9385 - pos_loss: 0.1906 - person_loss: 0.0962 - number_loss: 0.2047 - tense_loss: 0.0589 - mood_loss: 0.0799 - voice_loss: 2.8119e-07 - gender_loss: 0.3156 - case_loss: 0.3182 - degree_loss: 0.1014 - strength_loss: 0.0710 - inflection_loss: 0.0592 - pos_acc: 0.9407 - person_acc: 0.9686 - number_acc: 0.9260 - tense_acc: 0.9796 - mood_acc: 0.9742 - voice_acc: 1.0000 - gender_acc: 0.8890 - case_acc: 0.8831 - degree_acc: 0.9683 - strength_acc: 0.9761 - inflection_acc: 0.9789 - val_loss: 3.5418 - val_pos_loss: 0.5128 - val_person_loss: 0.0854 - val_number_loss: 0.4249 - val_tense_loss: 0.0831 - val_mood_loss: 0.1276 - val_voice_loss: 1.8815e-07 - val_gender_loss: 0.5967 - val_case_loss: 0.4973 - val_degree_loss: 0.3643 - val_strength_loss: 0.2383 - val_inflection_loss: 0.1725 - val_pos_acc: 0.8836 - val_person_acc: 0.9730 - val_number_acc: 0.8749 - val_tense_acc: 0.9739 - val_mood_acc: 0.9635 - val_voice_acc: 1.0000 - val_gender_acc: 0.8016 - val_case_acc: 0.8363 - val_degree_acc: 0.9118 - val_strength_acc: 0.9433 - val_inflection_acc: 0.9552\n",
      "Epoch 88/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9353 - pos_loss: 0.1868 - person_loss: 0.0925 - number_loss: 0.2093 - tense_loss: 0.0593 - mood_loss: 0.0803 - voice_loss: 2.7959e-07 - gender_loss: 0.3138 - case_loss: 0.3150 - degree_loss: 0.0993 - strength_loss: 0.0701 - inflection_loss: 0.0600 - pos_acc: 0.9402 - person_acc: 0.9710 - number_acc: 0.9247 - tense_acc: 0.9795 - mood_acc: 0.9734 - voice_acc: 1.0000 - gender_acc: 0.8902 - case_acc: 0.8849 - degree_acc: 0.9678 - strength_acc: 0.9772 - inflection_acc: 0.9783 - val_loss: 3.6691 - val_pos_loss: 0.5388 - val_person_loss: 0.0766 - val_number_loss: 0.4702 - val_tense_loss: 0.0789 - val_mood_loss: 0.1220 - val_voice_loss: 1.5727e-07 - val_gender_loss: 0.6226 - val_case_loss: 0.5391 - val_degree_loss: 0.3493 - val_strength_loss: 0.2274 - val_inflection_loss: 0.2016 - val_pos_acc: 0.8744 - val_person_acc: 0.9795 - val_number_acc: 0.8629 - val_tense_acc: 0.9725 - val_mood_acc: 0.9639 - val_voice_acc: 1.0000 - val_gender_acc: 0.8014 - val_case_acc: 0.8272 - val_degree_acc: 0.9095 - val_strength_acc: 0.9414 - val_inflection_acc: 0.9438\n",
      "Epoch 89/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9016 - pos_loss: 0.1861 - person_loss: 0.0949 - number_loss: 0.2012 - tense_loss: 0.0566 - mood_loss: 0.0797 - voice_loss: 2.5897e-07 - gender_loss: 0.3091 - case_loss: 0.3076 - degree_loss: 0.1024 - strength_loss: 0.0699 - inflection_loss: 0.0581 - pos_acc: 0.9413 - person_acc: 0.9689 - number_acc: 0.9282 - tense_acc: 0.9800 - mood_acc: 0.9730 - voice_acc: 1.0000 - gender_acc: 0.8900 - case_acc: 0.8868 - degree_acc: 0.9678 - strength_acc: 0.9774 - inflection_acc: 0.9802 - val_loss: 3.5129 - val_pos_loss: 0.5010 - val_person_loss: 0.0885 - val_number_loss: 0.4223 - val_tense_loss: 0.0800 - val_mood_loss: 0.1266 - val_voice_loss: 1.6260e-07 - val_gender_loss: 0.6144 - val_case_loss: 0.5228 - val_degree_loss: 0.3323 - val_strength_loss: 0.2213 - val_inflection_loss: 0.1886 - val_pos_acc: 0.8719 - val_person_acc: 0.9688 - val_number_acc: 0.8818 - val_tense_acc: 0.9732 - val_mood_acc: 0.9601 - val_voice_acc: 1.0000 - val_gender_acc: 0.8008 - val_case_acc: 0.8289 - val_degree_acc: 0.9157 - val_strength_acc: 0.9448 - val_inflection_acc: 0.9431\n",
      "Epoch 90/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.9172 - pos_loss: 0.1881 - person_loss: 0.0950 - number_loss: 0.2062 - tense_loss: 0.0606 - mood_loss: 0.0817 - voice_loss: 2.1202e-07 - gender_loss: 0.3075 - case_loss: 0.3133 - degree_loss: 0.0993 - strength_loss: 0.0685 - inflection_loss: 0.0578 - pos_acc: 0.9396 - person_acc: 0.9688 - number_acc: 0.9261 - tense_acc: 0.9797 - mood_acc: 0.9740 - voice_acc: 1.0000 - gender_acc: 0.8901 - case_acc: 0.8835 - degree_acc: 0.9682 - strength_acc: 0.9773 - inflection_acc: 0.9793 - val_loss: 3.3919 - val_pos_loss: 0.4835 - val_person_loss: 0.0787 - val_number_loss: 0.3937 - val_tense_loss: 0.0688 - val_mood_loss: 0.1071 - val_voice_loss: 1.3913e-07 - val_gender_loss: 0.6000 - val_case_loss: 0.5131 - val_degree_loss: 0.3254 - val_strength_loss: 0.2260 - val_inflection_loss: 0.1686 - val_pos_acc: 0.8781 - val_person_acc: 0.9736 - val_number_acc: 0.8914 - val_tense_acc: 0.9750 - val_mood_acc: 0.9623 - val_voice_acc: 1.0000 - val_gender_acc: 0.8052 - val_case_acc: 0.8369 - val_degree_acc: 0.9158 - val_strength_acc: 0.9390 - val_inflection_acc: 0.9541\n",
      "Epoch 91/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8996 - pos_loss: 0.1866 - person_loss: 0.0910 - number_loss: 0.2033 - tense_loss: 0.0563 - mood_loss: 0.0806 - voice_loss: 2.0123e-07 - gender_loss: 0.3137 - case_loss: 0.3051 - degree_loss: 0.0978 - strength_loss: 0.0676 - inflection_loss: 0.0574 - pos_acc: 0.9402 - person_acc: 0.9714 - number_acc: 0.9284 - tense_acc: 0.9804 - mood_acc: 0.9733 - voice_acc: 1.0000 - gender_acc: 0.8894 - case_acc: 0.8877 - degree_acc: 0.9686 - strength_acc: 0.9784 - inflection_acc: 0.9800 - val_loss: 3.5949 - val_pos_loss: 0.5242 - val_person_loss: 0.0760 - val_number_loss: 0.4356 - val_tense_loss: 0.0667 - val_mood_loss: 0.1113 - val_voice_loss: 1.5208e-07 - val_gender_loss: 0.6360 - val_case_loss: 0.5232 - val_degree_loss: 0.3423 - val_strength_loss: 0.2445 - val_inflection_loss: 0.1975 - val_pos_acc: 0.8701 - val_person_acc: 0.9774 - val_number_acc: 0.8744 - val_tense_acc: 0.9744 - val_mood_acc: 0.9658 - val_voice_acc: 1.0000 - val_gender_acc: 0.7955 - val_case_acc: 0.8233 - val_degree_acc: 0.9132 - val_strength_acc: 0.9371 - val_inflection_acc: 0.9421\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8853 - pos_loss: 0.1841 - person_loss: 0.0923 - number_loss: 0.2012 - tense_loss: 0.0560 - mood_loss: 0.0792 - voice_loss: 2.5214e-07 - gender_loss: 0.3099 - case_loss: 0.3037 - degree_loss: 0.0975 - strength_loss: 0.0678 - inflection_loss: 0.0585 - pos_acc: 0.9415 - person_acc: 0.9695 - number_acc: 0.9271 - tense_acc: 0.9822 - mood_acc: 0.9743 - voice_acc: 1.0000 - gender_acc: 0.8912 - case_acc: 0.8875 - degree_acc: 0.9690 - strength_acc: 0.9783 - inflection_acc: 0.9791 - val_loss: 3.2840 - val_pos_loss: 0.4593 - val_person_loss: 0.0702 - val_number_loss: 0.4026 - val_tense_loss: 0.0600 - val_mood_loss: 0.1042 - val_voice_loss: 1.3890e-07 - val_gender_loss: 0.5744 - val_case_loss: 0.4945 - val_degree_loss: 0.3090 - val_strength_loss: 0.2010 - val_inflection_loss: 0.1758 - val_pos_acc: 0.8839 - val_person_acc: 0.9739 - val_number_acc: 0.8813 - val_tense_acc: 0.9771 - val_mood_acc: 0.9632 - val_voice_acc: 1.0000 - val_gender_acc: 0.8095 - val_case_acc: 0.8301 - val_degree_acc: 0.9149 - val_strength_acc: 0.9449 - val_inflection_acc: 0.9504\n",
      "Epoch 93/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8800 - pos_loss: 0.1826 - person_loss: 0.0873 - number_loss: 0.2001 - tense_loss: 0.0565 - mood_loss: 0.0778 - voice_loss: 1.9398e-07 - gender_loss: 0.3075 - case_loss: 0.3017 - degree_loss: 0.1005 - strength_loss: 0.0676 - inflection_loss: 0.0576 - pos_acc: 0.9410 - person_acc: 0.9714 - number_acc: 0.9271 - tense_acc: 0.9807 - mood_acc: 0.9748 - voice_acc: 1.0000 - gender_acc: 0.8910 - case_acc: 0.8886 - degree_acc: 0.9673 - strength_acc: 0.9778 - inflection_acc: 0.9799 - val_loss: 3.4320 - val_pos_loss: 0.4939 - val_person_loss: 0.0831 - val_number_loss: 0.4162 - val_tense_loss: 0.0629 - val_mood_loss: 0.1129 - val_voice_loss: 1.3850e-07 - val_gender_loss: 0.6071 - val_case_loss: 0.5015 - val_degree_loss: 0.3322 - val_strength_loss: 0.2240 - val_inflection_loss: 0.1638 - val_pos_acc: 0.8712 - val_person_acc: 0.9757 - val_number_acc: 0.8739 - val_tense_acc: 0.9794 - val_mood_acc: 0.9664 - val_voice_acc: 1.0000 - val_gender_acc: 0.8058 - val_case_acc: 0.8344 - val_degree_acc: 0.9107 - val_strength_acc: 0.9396 - val_inflection_acc: 0.9496\n",
      "Epoch 94/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8669 - pos_loss: 0.1832 - person_loss: 0.0865 - number_loss: 0.2004 - tense_loss: 0.0532 - mood_loss: 0.0754 - voice_loss: 2.0405e-07 - gender_loss: 0.3038 - case_loss: 0.3052 - degree_loss: 0.0952 - strength_loss: 0.0663 - inflection_loss: 0.0580 - pos_acc: 0.9425 - person_acc: 0.9720 - number_acc: 0.9273 - tense_acc: 0.9810 - mood_acc: 0.9752 - voice_acc: 1.0000 - gender_acc: 0.8925 - case_acc: 0.8882 - degree_acc: 0.9686 - strength_acc: 0.9771 - inflection_acc: 0.9800 - val_loss: 3.4339 - val_pos_loss: 0.5074 - val_person_loss: 0.0769 - val_number_loss: 0.3986 - val_tense_loss: 0.0688 - val_mood_loss: 0.1085 - val_voice_loss: 1.3712e-07 - val_gender_loss: 0.5831 - val_case_loss: 0.5089 - val_degree_loss: 0.3380 - val_strength_loss: 0.2297 - val_inflection_loss: 0.1808 - val_pos_acc: 0.8794 - val_person_acc: 0.9801 - val_number_acc: 0.8955 - val_tense_acc: 0.9801 - val_mood_acc: 0.9709 - val_voice_acc: 1.0000 - val_gender_acc: 0.8193 - val_case_acc: 0.8362 - val_degree_acc: 0.9207 - val_strength_acc: 0.9390 - val_inflection_acc: 0.9543\n",
      "Epoch 95/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8548 - pos_loss: 0.1825 - person_loss: 0.0890 - number_loss: 0.1995 - tense_loss: 0.0528 - mood_loss: 0.0744 - voice_loss: 2.0034e-07 - gender_loss: 0.3020 - case_loss: 0.3007 - degree_loss: 0.0972 - strength_loss: 0.0667 - inflection_loss: 0.0550 - pos_acc: 0.9421 - person_acc: 0.9714 - number_acc: 0.9281 - tense_acc: 0.9810 - mood_acc: 0.9758 - voice_acc: 1.0000 - gender_acc: 0.8926 - case_acc: 0.8893 - degree_acc: 0.9677 - strength_acc: 0.9785 - inflection_acc: 0.9799 - val_loss: 3.4891 - val_pos_loss: 0.5259 - val_person_loss: 0.0745 - val_number_loss: 0.4083 - val_tense_loss: 0.0673 - val_mood_loss: 0.1114 - val_voice_loss: 1.2842e-07 - val_gender_loss: 0.5856 - val_case_loss: 0.4992 - val_degree_loss: 0.3489 - val_strength_loss: 0.2473 - val_inflection_loss: 0.1787 - val_pos_acc: 0.8788 - val_person_acc: 0.9800 - val_number_acc: 0.8937 - val_tense_acc: 0.9780 - val_mood_acc: 0.9627 - val_voice_acc: 1.0000 - val_gender_acc: 0.8103 - val_case_acc: 0.8424 - val_degree_acc: 0.9152 - val_strength_acc: 0.9394 - val_inflection_acc: 0.9536\n",
      "Epoch 96/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8644 - pos_loss: 0.1794 - person_loss: 0.0905 - number_loss: 0.1996 - tense_loss: 0.0549 - mood_loss: 0.0778 - voice_loss: 1.9740e-07 - gender_loss: 0.2989 - case_loss: 0.2999 - degree_loss: 0.0969 - strength_loss: 0.0673 - inflection_loss: 0.0566 - pos_acc: 0.9434 - person_acc: 0.9710 - number_acc: 0.9288 - tense_acc: 0.9816 - mood_acc: 0.9739 - voice_acc: 1.0000 - gender_acc: 0.8952 - case_acc: 0.8896 - degree_acc: 0.9695 - strength_acc: 0.9782 - inflection_acc: 0.9802 - val_loss: 3.4332 - val_pos_loss: 0.4902 - val_person_loss: 0.0749 - val_number_loss: 0.4088 - val_tense_loss: 0.0684 - val_mood_loss: 0.1145 - val_voice_loss: 1.4623e-07 - val_gender_loss: 0.5888 - val_case_loss: 0.5183 - val_degree_loss: 0.3281 - val_strength_loss: 0.2224 - val_inflection_loss: 0.1826 - val_pos_acc: 0.8781 - val_person_acc: 0.9744 - val_number_acc: 0.8873 - val_tense_acc: 0.9774 - val_mood_acc: 0.9643 - val_voice_acc: 1.0000 - val_gender_acc: 0.8172 - val_case_acc: 0.8360 - val_degree_acc: 0.9162 - val_strength_acc: 0.9470 - val_inflection_acc: 0.9526\n",
      "Epoch 97/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8308 - pos_loss: 0.1745 - person_loss: 0.0863 - number_loss: 0.1943 - tense_loss: 0.0550 - mood_loss: 0.0743 - voice_loss: 2.0788e-07 - gender_loss: 0.2970 - case_loss: 0.2992 - degree_loss: 0.0947 - strength_loss: 0.0638 - inflection_loss: 0.0566 - pos_acc: 0.9458 - person_acc: 0.9716 - number_acc: 0.9290 - tense_acc: 0.9804 - mood_acc: 0.9747 - voice_acc: 1.0000 - gender_acc: 0.8959 - case_acc: 0.8902 - degree_acc: 0.9688 - strength_acc: 0.9787 - inflection_acc: 0.9799 - val_loss: 3.4061 - val_pos_loss: 0.4929 - val_person_loss: 0.0736 - val_number_loss: 0.4141 - val_tense_loss: 0.0676 - val_mood_loss: 0.1041 - val_voice_loss: 1.5056e-07 - val_gender_loss: 0.5955 - val_case_loss: 0.4973 - val_degree_loss: 0.3249 - val_strength_loss: 0.2313 - val_inflection_loss: 0.1791 - val_pos_acc: 0.8772 - val_person_acc: 0.9763 - val_number_acc: 0.8850 - val_tense_acc: 0.9772 - val_mood_acc: 0.9671 - val_voice_acc: 1.0000 - val_gender_acc: 0.8097 - val_case_acc: 0.8407 - val_degree_acc: 0.9165 - val_strength_acc: 0.9414 - val_inflection_acc: 0.9470\n",
      "Epoch 98/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8413 - pos_loss: 0.1771 - person_loss: 0.0864 - number_loss: 0.1966 - tense_loss: 0.0547 - mood_loss: 0.0752 - voice_loss: 2.0052e-07 - gender_loss: 0.3027 - case_loss: 0.2992 - degree_loss: 0.0942 - strength_loss: 0.0656 - inflection_loss: 0.0559 - pos_acc: 0.9437 - person_acc: 0.9716 - number_acc: 0.9298 - tense_acc: 0.9812 - mood_acc: 0.9748 - voice_acc: 1.0000 - gender_acc: 0.8922 - case_acc: 0.8896 - degree_acc: 0.9693 - strength_acc: 0.9774 - inflection_acc: 0.9802 - val_loss: 3.2895 - val_pos_loss: 0.4798 - val_person_loss: 0.0614 - val_number_loss: 0.3907 - val_tense_loss: 0.0592 - val_mood_loss: 0.0912 - val_voice_loss: 1.4710e-07 - val_gender_loss: 0.5860 - val_case_loss: 0.4963 - val_degree_loss: 0.3132 - val_strength_loss: 0.2237 - val_inflection_loss: 0.1728 - val_pos_acc: 0.8864 - val_person_acc: 0.9844 - val_number_acc: 0.9003 - val_tense_acc: 0.9813 - val_mood_acc: 0.9753 - val_voice_acc: 1.0000 - val_gender_acc: 0.8139 - val_case_acc: 0.8464 - val_degree_acc: 0.9165 - val_strength_acc: 0.9415 - val_inflection_acc: 0.9558\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8118 - pos_loss: 0.1712 - person_loss: 0.0867 - number_loss: 0.1957 - tense_loss: 0.0512 - mood_loss: 0.0735 - voice_loss: 1.6532e-07 - gender_loss: 0.2934 - case_loss: 0.2947 - degree_loss: 0.0919 - strength_loss: 0.0648 - inflection_loss: 0.0541 - pos_acc: 0.9442 - person_acc: 0.9717 - number_acc: 0.9290 - tense_acc: 0.9818 - mood_acc: 0.9751 - voice_acc: 1.0000 - gender_acc: 0.8962 - case_acc: 0.8925 - degree_acc: 0.9709 - strength_acc: 0.9787 - inflection_acc: 0.9805 - val_loss: 3.3589 - val_pos_loss: 0.4926 - val_person_loss: 0.0673 - val_number_loss: 0.4164 - val_tense_loss: 0.0693 - val_mood_loss: 0.1020 - val_voice_loss: 1.3149e-07 - val_gender_loss: 0.5765 - val_case_loss: 0.4954 - val_degree_loss: 0.3177 - val_strength_loss: 0.2227 - val_inflection_loss: 0.1638 - val_pos_acc: 0.8711 - val_person_acc: 0.9795 - val_number_acc: 0.8857 - val_tense_acc: 0.9794 - val_mood_acc: 0.9687 - val_voice_acc: 1.0000 - val_gender_acc: 0.8084 - val_case_acc: 0.8352 - val_degree_acc: 0.9160 - val_strength_acc: 0.9409 - val_inflection_acc: 0.9474\n",
      "Epoch 100/100\n",
      "2426/2426 [==============================] - 20s 8ms/step - loss: 1.8000 - pos_loss: 0.1720 - person_loss: 0.0837 - number_loss: 0.1941 - tense_loss: 0.0486 - mood_loss: 0.0697 - voice_loss: 1.8159e-07 - gender_loss: 0.2933 - case_loss: 0.2941 - degree_loss: 0.0944 - strength_loss: 0.0655 - inflection_loss: 0.0528 - pos_acc: 0.9453 - person_acc: 0.9731 - number_acc: 0.9306 - tense_acc: 0.9827 - mood_acc: 0.9767 - voice_acc: 1.0000 - gender_acc: 0.8967 - case_acc: 0.8923 - degree_acc: 0.9703 - strength_acc: 0.9791 - inflection_acc: 0.9813 - val_loss: 3.5374 - val_pos_loss: 0.5185 - val_person_loss: 0.0701 - val_number_loss: 0.4414 - val_tense_loss: 0.0636 - val_mood_loss: 0.1114 - val_voice_loss: 1.2261e-07 - val_gender_loss: 0.6051 - val_case_loss: 0.5031 - val_degree_loss: 0.3669 - val_strength_loss: 0.2533 - val_inflection_loss: 0.1803 - val_pos_acc: 0.8804 - val_person_acc: 0.9787 - val_number_acc: 0.8767 - val_tense_acc: 0.9785 - val_mood_acc: 0.9637 - val_voice_acc: 1.0000 - val_gender_acc: 0.8088 - val_case_acc: 0.8364 - val_degree_acc: 0.9143 - val_strength_acc: 0.9413 - val_inflection_acc: 0.9448\n"
     ]
    }
   ],
   "source": [
    "idx = list(range(len(X)))\n",
    "np.random.shuffle(idx)\n",
    "history = m1.fit([X[idx], X2_train[idx]], [y[idx] for y in Y],\n",
    "                 batch_size=16, epochs=100,\n",
    "                 #callbacks=[plot_losses],\n",
    "                 validation_data=([X_test, X2_test], [y for y in Y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let's evaluate the model against the test set, the same that was used in evaluating the Perceptron tagger above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "110/110 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'case': 0.87,\n",
       " 'degree': 0.941,\n",
       " 'gender': 0.852,\n",
       " 'inflection': 0.958,\n",
       " 'mood': 0.972,\n",
       " 'number': 0.907,\n",
       " 'person': 0.986,\n",
       " 'pos': 0.914,\n",
       " 'strength': 0.955,\n",
       " 'tense': 0.983,\n",
       " 'voice': 1.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(feature_names, np.round(m1.evaluate([X_test, X2_test], [y for y in  Y_test], batch_size=256)[-11:], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this result to the Perceptron tagger above, we see that the Word+Char model improves on NLTK's AveragedPerceptron across all features, and especially so for the POS tagging task.  \n",
    "\n",
    "The POS accuracy of 0.918 puts the Word+Char in the range of the Germanic taggers tested by Horsmann and Zesch, though somewhat on the lower end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Perceptron Tagger | Word+Char LSTM | Diff |\n",
    "----------|-------------------|----------------|------|\n",
    "|  POS | 0.840 | 0.914 | + 7.4% |\n",
    "| Person | 0.956 | 0.979 | + 2.3% |\n",
    "| Number | 0.827 | 0.91 | + 9.3% |\n",
    "| Tense | 0.960 | 0.979 | + 3.9% |\n",
    "| Mood | 0.952 | 0.967 | + 1.5% |\n",
    "| Gender | 0.798 | 0.858 | + 6.0% |\n",
    "| Case | 0.814 | 0.874 | + 6.0 % |\n",
    "| Degree | 0.907 | 0.935 | + 2.8% |\n",
    "| Strength | 0.927 | 0.954 | + 2.7% |\n",
    "| Inflection | 0.931 | 0.965 | + 3.4% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_labels(labelsXML):\n",
    "    root = ET.fromstring(labelsXML)\n",
    "\n",
    "    labels = {}\n",
    "\n",
    "    for parts_of_speech in root.iter('parts-of-speech'):\n",
    "        labels['pos'] = {}\n",
    "\n",
    "        for value in parts_of_speech.iter('value'):\n",
    "            tag = value.attrib['tag']\n",
    "            label = value.attrib['summary']\n",
    "            labels['pos'][tag] = label\n",
    "\n",
    "    for field in root.iter('field'):\n",
    "        feature = field.attrib['tag']\n",
    "        labels[feature] = {}\n",
    "        labels[feature]['-'] = 'none'\n",
    "\n",
    "        for value in field.iter('value'):\n",
    "            tag = value.attrib['tag']\n",
    "            label = value.attrib['summary']\n",
    "            labels[feature][tag] = label\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsXML = '''\n",
    "<tags>\n",
    "<parts-of-speech>\n",
    "      <value tag=\"A-\" summary=\"adjective\"/>\n",
    "      <value tag=\"Df\" summary=\"adverb\"/>\n",
    "      <value tag=\"S-\" summary=\"article\"/>\n",
    "      <value tag=\"Ma\" summary=\"cardinal numeral\"/>\n",
    "      <value tag=\"Nb\" summary=\"common noun\"/>\n",
    "      <value tag=\"C-\" summary=\"conjunction\"/>\n",
    "      <value tag=\"Pd\" summary=\"demonstrative pronoun\"/>\n",
    "      <value tag=\"F-\" summary=\"foreign word\"/>\n",
    "      <value tag=\"Px\" summary=\"indefinite pronoun\"/>\n",
    "      <value tag=\"N-\" summary=\"infinitive marker\"/>\n",
    "      <value tag=\"I-\" summary=\"interjection\"/>\n",
    "      <value tag=\"Du\" summary=\"interrogative adverb\"/>\n",
    "      <value tag=\"Pi\" summary=\"interrogative pronoun\"/>\n",
    "      <value tag=\"Mo\" summary=\"ordinal numeral\"/>\n",
    "      <value tag=\"Pp\" summary=\"personal pronoun\"/>\n",
    "      <value tag=\"Pk\" summary=\"personal reflexive pronoun\"/>\n",
    "      <value tag=\"Ps\" summary=\"possessive pronoun\"/>\n",
    "      <value tag=\"Pt\" summary=\"possessive reflexive pronoun\"/>\n",
    "      <value tag=\"R-\" summary=\"preposition\"/>\n",
    "      <value tag=\"Ne\" summary=\"proper noun\"/>\n",
    "      <value tag=\"Py\" summary=\"quantifier\"/>\n",
    "      <value tag=\"Pc\" summary=\"reciprocal pronoun\"/>\n",
    "      <value tag=\"Dq\" summary=\"relative adverb\"/>\n",
    "      <value tag=\"Pr\" summary=\"relative pronoun\"/>\n",
    "      <value tag=\"G-\" summary=\"subjunction\"/>\n",
    "      <value tag=\"V-\" summary=\"verb\"/>\n",
    "      <value tag=\"X-\" summary=\"unassigned\"/>\n",
    "    </parts-of-speech>\n",
    "    <morphology>\n",
    "      <field tag=\"person\">\n",
    "        <value tag=\"1\" summary=\"first person\"/>\n",
    "        <value tag=\"2\" summary=\"second person\"/>\n",
    "        <value tag=\"3\" summary=\"third person\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain person\"/>\n",
    "      </field>\n",
    "      <field tag=\"number\">\n",
    "        <value tag=\"s\" summary=\"singular\"/>\n",
    "        <value tag=\"d\" summary=\"dual\"/>\n",
    "        <value tag=\"p\" summary=\"plural\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain number\"/>\n",
    "      </field>\n",
    "      <field tag=\"tense\">\n",
    "        <value tag=\"p\" summary=\"present\"/>\n",
    "        <value tag=\"i\" summary=\"imperfect\"/>\n",
    "        <value tag=\"r\" summary=\"perfect\"/>\n",
    "        <value tag=\"s\" summary=\"resultative\"/>\n",
    "        <value tag=\"a\" summary=\"aorist\"/>\n",
    "        <value tag=\"u\" summary=\"past\"/>\n",
    "        <value tag=\"l\" summary=\"pluperfect\"/>\n",
    "        <value tag=\"f\" summary=\"future\"/>\n",
    "        <value tag=\"t\" summary=\"future perfect\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain tense\"/>\n",
    "      </field>\n",
    "      <field tag=\"mood\">\n",
    "        <value tag=\"i\" summary=\"indicative\"/>\n",
    "        <value tag=\"s\" summary=\"subjunctive\"/>\n",
    "        <value tag=\"m\" summary=\"imperative\"/>\n",
    "        <value tag=\"o\" summary=\"optative\"/>\n",
    "        <value tag=\"n\" summary=\"infinitive\"/>\n",
    "        <value tag=\"p\" summary=\"participle\"/>\n",
    "        <value tag=\"d\" summary=\"gerund\"/>\n",
    "        <value tag=\"g\" summary=\"gerundive\"/>\n",
    "        <value tag=\"u\" summary=\"supine\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain mood\"/>\n",
    "        <value tag=\"y\" summary=\"finiteness unspecified\"/>\n",
    "        <value tag=\"e\" summary=\"indicative or subjunctive\"/>\n",
    "        <value tag=\"f\" summary=\"indicative or imperative\"/>\n",
    "        <value tag=\"h\" summary=\"subjunctive or imperative\"/>\n",
    "        <value tag=\"t\" summary=\"finite\"/>\n",
    "      </field>\n",
    "      <field tag=\"voice\">\n",
    "        <value tag=\"a\" summary=\"active\"/>\n",
    "        <value tag=\"m\" summary=\"middle\"/>\n",
    "        <value tag=\"p\" summary=\"passive\"/>\n",
    "        <value tag=\"e\" summary=\"middle or passive\"/>\n",
    "        <value tag=\"x\" summary=\"unspecified\"/>\n",
    "      </field>\n",
    "      <field tag=\"gender\">\n",
    "        <value tag=\"m\" summary=\"masculine\"/>\n",
    "        <value tag=\"f\" summary=\"feminine\"/>\n",
    "        <value tag=\"n\" summary=\"neuter\"/>\n",
    "        <value tag=\"p\" summary=\"masculine or feminine\"/>\n",
    "        <value tag=\"o\" summary=\"masculine or neuter\"/>\n",
    "        <value tag=\"r\" summary=\"feminine or neuter\"/>\n",
    "        <value tag=\"q\" summary=\"masculine, feminine or neuter\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain gender\"/>\n",
    "      </field>\n",
    "      <field tag=\"case\">\n",
    "        <value tag=\"n\" summary=\"nominative\"/>\n",
    "        <value tag=\"a\" summary=\"accusative\"/>\n",
    "        <value tag=\"o\" summary=\"oblique\"/>\n",
    "        <value tag=\"g\" summary=\"genitive\"/>\n",
    "        <value tag=\"c\" summary=\"genitive or dative\"/>\n",
    "        <value tag=\"e\" summary=\"accusative or dative\"/>\n",
    "        <value tag=\"d\" summary=\"dative\"/>\n",
    "        <value tag=\"b\" summary=\"ablative\"/>\n",
    "        <value tag=\"i\" summary=\"instrumental\"/>\n",
    "        <value tag=\"l\" summary=\"locative\"/>\n",
    "        <value tag=\"v\" summary=\"vocative\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain case\"/>\n",
    "        <value tag=\"z\" summary=\"no case\"/>\n",
    "      </field>\n",
    "      <field tag=\"degree\">\n",
    "        <value tag=\"p\" summary=\"positive\"/>\n",
    "        <value tag=\"c\" summary=\"comparative\"/>\n",
    "        <value tag=\"s\" summary=\"superlative\"/>\n",
    "        <value tag=\"x\" summary=\"uncertain degree\"/>\n",
    "        <value tag=\"z\" summary=\"no degree\"/>\n",
    "      </field>\n",
    "      <field tag=\"strength\">\n",
    "        <value tag=\"w\" summary=\"weak\"/>\n",
    "        <value tag=\"s\" summary=\"strong\"/>\n",
    "        <value tag=\"t\" summary=\"weak or strong\"/>\n",
    "      </field>\n",
    "      <field tag=\"inflection\">\n",
    "        <value tag=\"n\" summary=\"non-inflecting\"/>\n",
    "        <value tag=\"i\" summary=\"inflecting\"/>\n",
    "      </field>\n",
    "    </morphology>\n",
    "</tags>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case': {'-': 'none',\n",
       "  'a': 'accusative',\n",
       "  'b': 'ablative',\n",
       "  'c': 'genitive or dative',\n",
       "  'd': 'dative',\n",
       "  'e': 'accusative or dative',\n",
       "  'g': 'genitive',\n",
       "  'i': 'instrumental',\n",
       "  'l': 'locative',\n",
       "  'n': 'nominative',\n",
       "  'o': 'oblique',\n",
       "  'v': 'vocative',\n",
       "  'x': 'uncertain case',\n",
       "  'z': 'no case'},\n",
       " 'degree': {'-': 'none',\n",
       "  'c': 'comparative',\n",
       "  'p': 'positive',\n",
       "  's': 'superlative',\n",
       "  'x': 'uncertain degree',\n",
       "  'z': 'no degree'},\n",
       " 'gender': {'-': 'none',\n",
       "  'f': 'feminine',\n",
       "  'm': 'masculine',\n",
       "  'n': 'neuter',\n",
       "  'o': 'masculine or neuter',\n",
       "  'p': 'masculine or feminine',\n",
       "  'q': 'masculine, feminine or neuter',\n",
       "  'r': 'feminine or neuter',\n",
       "  'x': 'uncertain gender'},\n",
       " 'inflection': {'-': 'none', 'i': 'inflecting', 'n': 'non-inflecting'},\n",
       " 'mood': {'-': 'none',\n",
       "  'd': 'gerund',\n",
       "  'e': 'indicative or subjunctive',\n",
       "  'f': 'indicative or imperative',\n",
       "  'g': 'gerundive',\n",
       "  'h': 'subjunctive or imperative',\n",
       "  'i': 'indicative',\n",
       "  'm': 'imperative',\n",
       "  'n': 'infinitive',\n",
       "  'o': 'optative',\n",
       "  'p': 'participle',\n",
       "  's': 'subjunctive',\n",
       "  't': 'finite',\n",
       "  'u': 'supine',\n",
       "  'x': 'uncertain mood',\n",
       "  'y': 'finiteness unspecified'},\n",
       " 'number': {'-': 'none',\n",
       "  'd': 'dual',\n",
       "  'p': 'plural',\n",
       "  's': 'singular',\n",
       "  'x': 'uncertain number'},\n",
       " 'person': {'-': 'none',\n",
       "  '1': 'first person',\n",
       "  '2': 'second person',\n",
       "  '3': 'third person',\n",
       "  'x': 'uncertain person'},\n",
       " 'pos': {'A-': 'adjective',\n",
       "  'C-': 'conjunction',\n",
       "  'Df': 'adverb',\n",
       "  'Dq': 'relative adverb',\n",
       "  'Du': 'interrogative adverb',\n",
       "  'F-': 'foreign word',\n",
       "  'G-': 'subjunction',\n",
       "  'I-': 'interjection',\n",
       "  'Ma': 'cardinal numeral',\n",
       "  'Mo': 'ordinal numeral',\n",
       "  'N-': 'infinitive marker',\n",
       "  'Nb': 'common noun',\n",
       "  'Ne': 'proper noun',\n",
       "  'Pc': 'reciprocal pronoun',\n",
       "  'Pd': 'demonstrative pronoun',\n",
       "  'Pi': 'interrogative pronoun',\n",
       "  'Pk': 'personal reflexive pronoun',\n",
       "  'Pp': 'personal pronoun',\n",
       "  'Pr': 'relative pronoun',\n",
       "  'Ps': 'possessive pronoun',\n",
       "  'Pt': 'possessive reflexive pronoun',\n",
       "  'Px': 'indefinite pronoun',\n",
       "  'Py': 'quantifier',\n",
       "  'R-': 'preposition',\n",
       "  'S-': 'article',\n",
       "  'V-': 'verb',\n",
       "  'X-': 'unassigned'},\n",
       " 'strength': {'-': 'none', 's': 'strong', 't': 'weak or strong', 'w': 'weak'},\n",
       " 'tense': {'-': 'none',\n",
       "  'a': 'aorist',\n",
       "  'f': 'future',\n",
       "  'i': 'imperfect',\n",
       "  'l': 'pluperfect',\n",
       "  'p': 'present',\n",
       "  'r': 'perfect',\n",
       "  's': 'resultative',\n",
       "  't': 'future perfect',\n",
       "  'u': 'past',\n",
       "  'x': 'uncertain tense'},\n",
       " 'voice': {'-': 'none',\n",
       "  'a': 'active',\n",
       "  'e': 'middle or passive',\n",
       "  'm': 'middle',\n",
       "  'p': 'passive',\n",
       "  'x': 'unspecified'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full = extract_labels(labelsXML)\n",
    "labels_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cd1308fc4047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "predicted = m2.predict([X_test, X2_test])\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print('---------------- ', feature, ' ----------------')\n",
    "    gold = np.ndarray.flatten(np.argmax(Y_test[i], axis=2))\n",
    "    mask = gold > 0\n",
    "    pred = np.ndarray.flatten(np.argmax(predicted[i], axis=2))\n",
    "    \n",
    "    gold_labels = [labels_full[feature][labels[i][idx-1]] for idx in gold[mask]]\n",
    "    pred_labels = [labels_full[feature][labels[i][idx-1]] for idx in pred[mask]]\n",
    "    print(ConfusionMatrix(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few considerations:\n",
    "\n",
    "* POS tagging, there is some equivocation between adjectives and nouns, perhaps explained by the observation that in OE adjectives are inflected like nouns for case, gender, number.  Also, because of its relatively free word order, adjectives are not predictable pre- or post-nominal or in predicate position.\n",
    "* Another issue for POS tagging is the \"subjunction\" annotation, which the tagger frequently mis-tags as adverbial.  These \"subjuction\" elements appear to be complementizers, such as *þy* \"therefore, because\", which might rightly be tagged as adverbial, i.e. merged in A-bar positions.\n",
    "* Forms annotated as quantifiers are tagged as adjectives by the classifier.  This again is linguistically defensible, as words like *micel* \"much, big, a lot\" and *manig* \"many\" are sometimes classed as adjectives (e.g. by [Wiktionary](https://en.wiktionary.org/wiki/manig))\n",
    "* As for case, there is a predictable equivocation between nominative and accusative case assignment.  This is due to syncretism in OE's case system: nominative and accusative forms overlap in cells of the nominal and adjectival paradigms -- principally in masculine and neuter lemmas --, and since word order is only weakly driven by syntax, disambiguation is difficult.\n",
    "* The last point might also explain the system's lack of accuracy in distinguishing masculine from neuter forms. Its apparent difficulty in distinguishing feminine from masculine forms likely lies in the phonological heterogeneity of the categories: on encountering a new word, it is difficult to guess at its gender from its form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productification\n",
    "\n",
    "To be useful in practice, the model parameters must be exported, and the preprocessing routines wrappend in a library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_morpho_model(path, model, alphabet, feature_names, labels, labels_full):\n",
    "    model.save(path + \"/\" + 'morpho_model.h5')\n",
    "        \n",
    "    with open(path + '/' + 'morpho_labels.pickle', 'wb') as out:\n",
    "        pickle.dump(alphabet, out)\n",
    "        pickle.dump(feature_names, out)\n",
    "        pickle.dump(labels, out)\n",
    "        pickle.dump(labels_full, out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_morpho_model('../models/oe', m1, alpha, feature_names, labels, labels_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading of the model, the preprocessing and the tagging are best wrapped into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jds/tensorflow-gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class MorphologicalAnalyzer:\n",
    "    def __init__(self, path):\n",
    "        \"\"\"Loads the model from a saved HDF5 file,\n",
    "        along with related data structures for taglibs and vocabularies.\"\"\"\n",
    "        \n",
    "        self.model = keras.models.load_model(path + '/' + 'morpho_model.h5')\n",
    "\n",
    "        with open(path + '/' + 'morpho_labels.pickle', 'rb') as inp:\n",
    "            self.alphabet = pickle.load(inp)\n",
    "            self.feature_names = pickle.load(inp)\n",
    "            self.labels = pickle.load(inp)\n",
    "            self.labels_full = pickle.load(inp)\n",
    "\n",
    "        input_layer = self.model.get_layer(index=0)\n",
    "\n",
    "        self.max_sent_len = input_layer.input_shape[1]\n",
    "        self.max_word_len = input_layer.input_shape[2]\n",
    "        \n",
    "        self.vocab = {}\n",
    "        with open(path + '/' + 'oe_types.txt', 'r') as f:\n",
    "            for pair in f:\n",
    "                idx, word = pair.split()\n",
    "                self.vocab[word] = int(idx)\n",
    "                \n",
    "        \n",
    "    def _select_letter_indices(self, word):\n",
    "        \"\"\"For an input words, returns a vector of indices into the alphabet.\"\"\"\n",
    "        v = np.zeros((self.max_word_len))\n",
    "    \n",
    "        for i in range(min(len(word), self.max_word_len)):\n",
    "            v[i] = self.alphabet.index(word[i]) + 1 if word[i] in self.alphabet else 0\n",
    "\n",
    "        return v\n",
    "        \n",
    "    def _characterize(self, sentences):\n",
    "        \"\"\"For a list of sentences, returns a tensor of dimension \n",
    "        (num sentences, words_per_sentence, letters_per_word)\n",
    "        of indices into the alphabet.\"\"\"\n",
    "        \n",
    "        X = np.zeros((len(sentences), self.max_sent_len, self.max_word_len), dtype='int32')\n",
    "    \n",
    "        for i, sent in enumerate(sentences):\n",
    "            for j, word in enumerate(sent):\n",
    "                if j >= self.max_sent_len:\n",
    "                    break\n",
    "                X[i, j, :] = self._select_letter_indices(word)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _wordize(self, sentences):\n",
    "        \"\"\"For a list of sentences, returns a tensor of dimension\n",
    "        (num_sentences, words_per_sentence)\n",
    "        of indices into the vocabulary.\"\"\"\n",
    "        \n",
    "        X = np.zeros((len(sentences), self.max_sent_len))\n",
    "        for i, sent in enumerate(sentences):\n",
    "            for j, word in enumerate(sent):\n",
    "                if j == self.max_sent_len:\n",
    "                    break\n",
    "                X[i,j] = self.vocab[word] + 1 if word in self. vocab else 0\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenizes a text into sentences and words (list of list of strings) if necessary,\n",
    "        and maps the tokens to indices into the vocabulary and the alphabet,\n",
    "        returning a 3D tensor for the characters and 2D tensor for the words.\"\"\"\n",
    "        \n",
    "        if type(text) != list:\n",
    "            text = text.replace('!','.') \n",
    "            text = text.replace(',', ' ')\n",
    "            sents = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        else:\n",
    "            sents = text\n",
    "        \n",
    "        X1 = self._characterize(sents)\n",
    "        X2 = self._wordize(sents)\n",
    "        \n",
    "        return sents, X1, X2\n",
    "        \n",
    "    def tag(self, text, batch_size=2049):\n",
    "        \"\"\"Takes a text as a string or a list of list of string tokens,\n",
    "        returning a list of list of tuples (word, feature_bundle),\n",
    "        where feature_bundle is a dict of feature to feature_value.\"\"\"\n",
    "        \n",
    "        sentences, X1, X2 = self.preprocess(text)\n",
    "        pred = self.model.predict([X1, X2], verbose=1, batch_size=batch_size)\n",
    "        tagged_sents = []\n",
    "\n",
    "        for i, sent in enumerate(sentences):\n",
    "            tagged_sent = []\n",
    "            for j, word in enumerate(sent):\n",
    "                if j == self.max_sent_len:\n",
    "                    break\n",
    "                feature_bundle = {}\n",
    "                for k, feature_name in enumerate(self.feature_names):\n",
    "                    idx = np.argmax(pred[k][i,j])-1\n",
    "                    feature_value = self.labels_full[feature_name][self.labels[k][idx]]\n",
    "                    if feature_value != 'none':\n",
    "                        feature_bundle[feature_name] = feature_value\n",
    "                tagged_sent.append((word, feature_bundle))\n",
    "            tagged_sents.append(tagged_sent)\n",
    "        \n",
    "        return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 39.10462212562561 seconds ----\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "morph_anal = MorphologicalAnalyzer('../models/oe')\n",
    "print(\"---- {0} seconds ----\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,everything = load_corpus('../texts/oe/oe_all.txt', tagged=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a laptop with a low-end i5 CPU (Intel© Core™ i5-8250U CPU @ 1.60GHz) with 4 independent cores and 16GB of RAM, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110656/110656 [==============================] - 590s 5ms/step\n",
      "---- 687.4790625572205 seconds ----\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tags = morph_anal.tag(everything, batch_size=256)\n",
    "print(\"---- {0} seconds ----\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "364 seconds for about 1.5MM words translates to a rate of ~ 4,100 words/sec.  This compares with the ~10,000 words/sec for just one feaature (POS) turned in by the Perceptron tagger.  So we lose 2.5x the speed of the individual tagger, but get all features output in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, beowulf = load_corpus('../texts/oe/beowulf.txt', tagged = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181/3181 [==============================] - 37s 12ms/step\n",
      "---- 39.57464933395386 seconds ----\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tags = morph_anal.tag(beowulf, batch_size=512)\n",
    "print(\"---- {0} seconds ----\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hwæt!',\n",
       "  {'inflection': 'inflecting',\n",
       "   'mood': 'imperative',\n",
       "   'number': 'singular',\n",
       "   'person': 'second person',\n",
       "   'pos': 'verb'}),\n",
       " ('We',\n",
       "  {'case': 'nominative',\n",
       "   'inflection': 'inflecting',\n",
       "   'number': 'plural',\n",
       "   'person': 'first person',\n",
       "   'pos': 'personal pronoun'}),\n",
       " ('Gardena', {'inflection': 'inflecting', 'pos': 'verb'}),\n",
       " ('in',\n",
       "  {'degree': 'no degree',\n",
       "   'inflection': 'non-inflecting',\n",
       "   'pos': 'preposition'}),\n",
       " ('geardagum,',\n",
       "  {'case': 'dative',\n",
       "   'gender': 'masculine',\n",
       "   'inflection': 'inflecting',\n",
       "   'number': 'plural',\n",
       "   'pos': 'common noun'})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
